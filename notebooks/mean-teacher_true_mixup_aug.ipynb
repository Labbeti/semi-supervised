{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1703.01780.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/lcances/.miniconda3/envs/ssl/bin/python'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import time\n",
    "import pprint\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from SSL.util.loaders import (\n",
    "    load_dataset,\n",
    "    load_optimizer,\n",
    "    load_callbacks,\n",
    "    load_preprocesser,\n",
    ")\n",
    "from SSL.util.model_loader import load_model\n",
    "from SSL.util.checkpoint import CheckPoint, mSummaryWriter\n",
    "from SSL.util.mixup import MixUpBatchShuffle\n",
    "from SSL.util.utils import reset_seed, get_datetime, track_maximum, DotDict\n",
    "from SSL.ramps import Warmup, sigmoid_rampup\n",
    "from SSL.losses import JensenShanon\n",
    "\n",
    "from metric_utils.metrics import CategoricalAccuracy, FScore, ContinueAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from augmentation_utils.signal_augmentations import Occlusion, SignalAugmentation\n",
    "from augmentation_utils.spec_augmentations import SpecAugmentation\n",
    "from augmentation_utils.augmentations import ComposeAugmentation\n",
    "from mlu.transforms.waveform import StretchPadCrop\n",
    "from mlu.transforms.base import WaveformTransform, SpectrogramTransform\n",
    "from mlu.transforms.spectrogram import CutOutSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--from_config\", default=\"\", type=str)\n",
    "parser.add_argument(\"-d\", \"--dataset_root\", default=\"../datasets\", type=str)\n",
    "parser.add_argument(\"-D\", \"--dataset\", default=\"speechcommand\", type=str)\n",
    "\n",
    "group_t = parser.add_argument_group(\"Commun parameters\")\n",
    "group_t.add_argument(\"-m\", \"--model\", default=\"wideresnet28_2\", type=str)\n",
    "group_t.add_argument(\"--supervised_ratio\", default=0.1, type=float)\n",
    "group_t.add_argument(\"--batch_size\", default=64, type=int)\n",
    "group_t.add_argument(\"--nb_epoch\", default=200, type=int)\n",
    "group_t.add_argument(\"--learning_rate\", default=0.001, type=float)\n",
    "group_t.add_argument(\"--resume\", action=\"store_true\", default=False)\n",
    "group_t.add_argument(\"--seed\", default=1234, type=int)\n",
    "group_t.add_argument(\"--num_classes\", default=35, type=int)\n",
    "\n",
    "group_u = parser.add_argument_group(\"Datasets parameters\")\n",
    "group_u.add_argument(\n",
    "    \"-t\", \"--train_folds\", nargs=\"+\", default=[1, 2, 3, 4, 5, 6, 7, 8, 9], type=int\n",
    ")\n",
    "group_u.add_argument(\"-v\", \"--val_folds\", nargs=\"+\", default=[10], type=int)\n",
    "\n",
    "group_s = parser.add_argument_group(\"Student teacher parameters\")\n",
    "group_s.add_argument(\"--ema_alpha\", default=0.999, type=float)\n",
    "group_s.add_argument(\"--warmup_length\", default=50, type=int)\n",
    "group_s.add_argument(\"--lambda_cost_max\", default=1, type=float)\n",
    "group_s.add_argument(\"--teacher_noise\", default=0, type=float)\n",
    "group_s.add_argument(\"--ccost_softmax\", action=\"store_true\", default=False)\n",
    "group_s.add_argument(\"--ccost_method\", type=str, default=\"js\")\n",
    "\n",
    "group_mixup = parser.add_argument_group(\"Mixup parameters\")\n",
    "group_mixup.add_argument(\"--mixup\", action=\"store_true\", default=False)\n",
    "group_mixup.add_argument(\"--mixup_alpha\", type=float, default=0.4)\n",
    "group_mixup.add_argument(\"--mixup_max\", action=\"store_true\", default=False)\n",
    "group_mixup.add_argument(\"--mixup_label\", action=\"store_true\", default=False)\n",
    "\n",
    "group_a = parser.add_argument_group(\"augmentations\")\n",
    "group_a.add_argument(\"--use_augmentation\", action=\"store_true\", default=False)\n",
    "group_a.add_argument(\"--augmentation_method\", type=str, default=\"pick-one\")\n",
    "\n",
    "group_l = parser.add_argument_group(\"Logs\")\n",
    "group_l.add_argument(\"--checkpoint_root\", default=\"../model_save/\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_root\", default=\"../tensorboard/\", type=str)\n",
    "group_l.add_argument(\"--checkpoint_path\", default=\"mean-teacher_mixup\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_path\", default=\"mean-teacher_mixup\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_sufix\", default=\"\", type=str)\n",
    "\n",
    "args = parser.parse_args(\n",
    "    [\"--ccost_softmax\", \"--use_augmentation\", \"--augmentation_method\", \"pick-one\"]\n",
    ")\n",
    "\n",
    "tensorboard_path = os.path.join(\n",
    "    args.tensorboard_root, args.dataset, args.tensorboard_path\n",
    ")\n",
    "checkpoint_path = os.path.join(args.checkpoint_root, args.dataset, args.checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'augmentation_method': 'pick-one',\n",
      " 'batch_size': 64,\n",
      " 'ccost_method': 'js',\n",
      " 'ccost_softmax': True,\n",
      " 'checkpoint_path': 'mean-teacher_mixup',\n",
      " 'checkpoint_root': '../model_save/',\n",
      " 'dataset': 'speechcommand',\n",
      " 'dataset_root': '../datasets',\n",
      " 'ema_alpha': 0.999,\n",
      " 'from_config': '',\n",
      " 'lambda_cost_max': 1,\n",
      " 'learning_rate': 0.001,\n",
      " 'mixup': False,\n",
      " 'mixup_alpha': 0.4,\n",
      " 'mixup_label': False,\n",
      " 'mixup_max': False,\n",
      " 'model': 'wideresnet28_2',\n",
      " 'nb_epoch': 200,\n",
      " 'num_classes': 35,\n",
      " 'resume': False,\n",
      " 'seed': 1234,\n",
      " 'supervised_ratio': 0.1,\n",
      " 'teacher_noise': 0,\n",
      " 'tensorboard_path': 'mean-teacher_mixup',\n",
      " 'tensorboard_root': '../tensorboard/',\n",
      " 'tensorboard_sufix': '',\n",
      " 'train_folds': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      " 'use_augmentation': True,\n",
      " 'val_folds': [10],\n",
      " 'warmup_length': 50}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(vars(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "reset_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the pre-processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform, val_transform = load_preprocesser(args.dataset, \"mean-teacher\")\n",
    "\n",
    "# decompose transform for more control\n",
    "pad_transform = train_transform[0]\n",
    "mel_transform = nn.Sequential(*train_transform[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "composer = None\n",
    "\n",
    "# add transformation for apply augmentation\n",
    "if args.use_augmentation:\n",
    "    ratio = 0.5\n",
    "    weak_pool = [\n",
    "        Occlusion(ratio, sampling_rate=16000, max_size=0.25),\n",
    "        CutOutSpec(\n",
    "            width_scales=(0.1, 0.5), height_scales=(0.1, 0.5), fill_value=-80.0, p=ratio\n",
    "        ),\n",
    "        StretchPadCrop(rates=(0.5, 1.5), align=\"random\", p=ratio),\n",
    "    ]\n",
    "\n",
    "    # Create the augmentation composer\n",
    "    composer = ComposeAugmentation(\n",
    "        pre_process_rule=lambda x: isinstance(\n",
    "            x, (SignalAugmentation, WaveformTransform)\n",
    "        ),\n",
    "        post_process_rule=lambda x: isinstance(\n",
    "            x, (SpecAugmentation, SpectrogramTransform)\n",
    "        ),\n",
    "    )\n",
    "    composer.set_augmentation_pool(weak_pool)\n",
    "    composer.set_process(train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../datasets/SpeechCommands/speech_commands_v0.02\n",
      "Dataset already download and verified\n",
      "../datasets/SpeechCommands/speech_commands_v0.02\n",
      "Dataset already download and verified\n",
      "../datasets/SpeechCommands/speech_commands_v0.02\n",
      "Dataset already download and verified\n",
      "../datasets/SpeechCommands/speech_commands_v0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84843/84843 [00:02<00:00, 38298.41it/s]\n",
      "100%|██████████| 35/35 [00:00<00:00, 798.03it/s]\n"
     ]
    }
   ],
   "source": [
    "manager, train_loader, val_loader = load_dataset(\n",
    "    args.dataset,\n",
    "    \"mean-teacher\",\n",
    "    dataset_root=args.dataset_root,\n",
    "    supervised_ratio=args.supervised_ratio,\n",
    "    batch_size=args.batch_size,\n",
    "    train_folds=args.train_folds,\n",
    "    val_folds=args.val_folds,\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    "    augmentation=composer,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    enable_cache=False,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape =  torch.Size([64, 32])\n"
     ]
    }
   ],
   "source": [
    "input_shape = train_loader._iterables[0].dataset.datasets[0][0][0].shape\n",
    "print(\"input_shape = \", input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_func = load_model(args.dataset, args.model)\n",
    "\n",
    "student = model_func(input_shape=input_shape, num_classes=args.num_classes)\n",
    "teacher = model_func(input_shape=input_shape, num_classes=args.num_classes)\n",
    "\n",
    "student = student.cuda()\n",
    "teacher = teacher.cuda()\n",
    "\n",
    "# We do not need gradient for the teacher model\n",
    "for p in teacher.parameters():\n",
    "    p.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 32]             288\n",
      "       BatchNorm2d-2           [-1, 32, 64, 32]              64\n",
      "              ReLU-3           [-1, 32, 64, 32]               0\n",
      "         MaxPool2d-4           [-1, 32, 32, 16]               0\n",
      "            Conv2d-5           [-1, 32, 32, 16]           9,216\n",
      "       BatchNorm2d-6           [-1, 32, 32, 16]              64\n",
      "              ReLU-7           [-1, 32, 32, 16]               0\n",
      "            Conv2d-8           [-1, 32, 32, 16]           9,216\n",
      "       BatchNorm2d-9           [-1, 32, 32, 16]              64\n",
      "             ReLU-10           [-1, 32, 32, 16]               0\n",
      "       BasicBlock-11           [-1, 32, 32, 16]               0\n",
      "           Conv2d-12           [-1, 32, 32, 16]           9,216\n",
      "      BatchNorm2d-13           [-1, 32, 32, 16]              64\n",
      "             ReLU-14           [-1, 32, 32, 16]               0\n",
      "           Conv2d-15           [-1, 32, 32, 16]           9,216\n",
      "      BatchNorm2d-16           [-1, 32, 32, 16]              64\n",
      "             ReLU-17           [-1, 32, 32, 16]               0\n",
      "       BasicBlock-18           [-1, 32, 32, 16]               0\n",
      "           Conv2d-19           [-1, 32, 32, 16]           9,216\n",
      "      BatchNorm2d-20           [-1, 32, 32, 16]              64\n",
      "             ReLU-21           [-1, 32, 32, 16]               0\n",
      "           Conv2d-22           [-1, 32, 32, 16]           9,216\n",
      "      BatchNorm2d-23           [-1, 32, 32, 16]              64\n",
      "             ReLU-24           [-1, 32, 32, 16]               0\n",
      "       BasicBlock-25           [-1, 32, 32, 16]               0\n",
      "           Conv2d-26           [-1, 32, 32, 16]           9,216\n",
      "      BatchNorm2d-27           [-1, 32, 32, 16]              64\n",
      "             ReLU-28           [-1, 32, 32, 16]               0\n",
      "           Conv2d-29           [-1, 32, 32, 16]           9,216\n",
      "      BatchNorm2d-30           [-1, 32, 32, 16]              64\n",
      "             ReLU-31           [-1, 32, 32, 16]               0\n",
      "       BasicBlock-32           [-1, 32, 32, 16]               0\n",
      "           Conv2d-33            [-1, 64, 16, 8]          18,432\n",
      "      BatchNorm2d-34            [-1, 64, 16, 8]             128\n",
      "             ReLU-35            [-1, 64, 16, 8]               0\n",
      "           Conv2d-36            [-1, 64, 16, 8]          36,864\n",
      "      BatchNorm2d-37            [-1, 64, 16, 8]             128\n",
      "           Conv2d-38            [-1, 64, 16, 8]           2,048\n",
      "      BatchNorm2d-39            [-1, 64, 16, 8]             128\n",
      "             ReLU-40            [-1, 64, 16, 8]               0\n",
      "       BasicBlock-41            [-1, 64, 16, 8]               0\n",
      "           Conv2d-42            [-1, 64, 16, 8]          36,864\n",
      "      BatchNorm2d-43            [-1, 64, 16, 8]             128\n",
      "             ReLU-44            [-1, 64, 16, 8]               0\n",
      "           Conv2d-45            [-1, 64, 16, 8]          36,864\n",
      "      BatchNorm2d-46            [-1, 64, 16, 8]             128\n",
      "             ReLU-47            [-1, 64, 16, 8]               0\n",
      "       BasicBlock-48            [-1, 64, 16, 8]               0\n",
      "           Conv2d-49            [-1, 64, 16, 8]          36,864\n",
      "      BatchNorm2d-50            [-1, 64, 16, 8]             128\n",
      "             ReLU-51            [-1, 64, 16, 8]               0\n",
      "           Conv2d-52            [-1, 64, 16, 8]          36,864\n",
      "      BatchNorm2d-53            [-1, 64, 16, 8]             128\n",
      "             ReLU-54            [-1, 64, 16, 8]               0\n",
      "       BasicBlock-55            [-1, 64, 16, 8]               0\n",
      "           Conv2d-56            [-1, 64, 16, 8]          36,864\n",
      "      BatchNorm2d-57            [-1, 64, 16, 8]             128\n",
      "             ReLU-58            [-1, 64, 16, 8]               0\n",
      "           Conv2d-59            [-1, 64, 16, 8]          36,864\n",
      "      BatchNorm2d-60            [-1, 64, 16, 8]             128\n",
      "             ReLU-61            [-1, 64, 16, 8]               0\n",
      "       BasicBlock-62            [-1, 64, 16, 8]               0\n",
      "           Conv2d-63            [-1, 128, 8, 4]          73,728\n",
      "      BatchNorm2d-64            [-1, 128, 8, 4]             256\n",
      "             ReLU-65            [-1, 128, 8, 4]               0\n",
      "           Conv2d-66            [-1, 128, 8, 4]         147,456\n",
      "      BatchNorm2d-67            [-1, 128, 8, 4]             256\n",
      "           Conv2d-68            [-1, 128, 8, 4]           8,192\n",
      "      BatchNorm2d-69            [-1, 128, 8, 4]             256\n",
      "             ReLU-70            [-1, 128, 8, 4]               0\n",
      "       BasicBlock-71            [-1, 128, 8, 4]               0\n",
      "           Conv2d-72            [-1, 128, 8, 4]         147,456\n",
      "      BatchNorm2d-73            [-1, 128, 8, 4]             256\n",
      "             ReLU-74            [-1, 128, 8, 4]               0\n",
      "           Conv2d-75            [-1, 128, 8, 4]         147,456\n",
      "      BatchNorm2d-76            [-1, 128, 8, 4]             256\n",
      "             ReLU-77            [-1, 128, 8, 4]               0\n",
      "       BasicBlock-78            [-1, 128, 8, 4]               0\n",
      "           Conv2d-79            [-1, 128, 8, 4]         147,456\n",
      "      BatchNorm2d-80            [-1, 128, 8, 4]             256\n",
      "             ReLU-81            [-1, 128, 8, 4]               0\n",
      "           Conv2d-82            [-1, 128, 8, 4]         147,456\n",
      "      BatchNorm2d-83            [-1, 128, 8, 4]             256\n",
      "             ReLU-84            [-1, 128, 8, 4]               0\n",
      "       BasicBlock-85            [-1, 128, 8, 4]               0\n",
      "           Conv2d-86            [-1, 128, 8, 4]         147,456\n",
      "      BatchNorm2d-87            [-1, 128, 8, 4]             256\n",
      "             ReLU-88            [-1, 128, 8, 4]               0\n",
      "           Conv2d-89            [-1, 128, 8, 4]         147,456\n",
      "      BatchNorm2d-90            [-1, 128, 8, 4]             256\n",
      "             ReLU-91            [-1, 128, 8, 4]               0\n",
      "       BasicBlock-92            [-1, 128, 8, 4]               0\n",
      "AdaptiveAvgPool2d-93            [-1, 128, 1, 1]               0\n",
      "           Linear-94                   [-1, 35]           4,515\n",
      "================================================================\n",
      "Total params: 1,475,203\n",
      "Trainable params: 1,475,203\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 7.94\n",
      "Params size (MB): 5.63\n",
      "Estimated Total Size (MB): 13.57\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "s = summary(student, input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_root = (\n",
    "    f\"{args.model}/{args.supervised_ratio}/{get_datetime()}_{model_func.__name__}\"\n",
    ")\n",
    "checkpoint_root = f\"{args.model}/{args.supervised_ratio}/{model_func.__name__}\"\n",
    "\n",
    "# mea teacher parameters\n",
    "sufix_title = f\"_{args.ema_alpha}-emaa\"\n",
    "sufix_title += f\"_{args.warmup_length}-wl\"\n",
    "sufix_title += f\"_{args.lambda_cost_max}-lccm\"\n",
    "\n",
    "# mixup parameters\n",
    "if args.mixup:\n",
    "    sufix_title += \"_mixup\"\n",
    "    if args.mixup_max:\n",
    "        sufix_title += \"-max\"\n",
    "    if args.mixup_label:\n",
    "        sufix_title += \"-label\"\n",
    "    sufix_title += f\"-{args.mixup_alpha}-a\"\n",
    "\n",
    "# ccost function and method\n",
    "if args.ccost_method:\n",
    "    sufix_title += f\"_cc-{args.ccost_method}\"\n",
    "if args.ccost_softmax:\n",
    "    sufix_title += \"-SOFTMAX\"\n",
    "\n",
    "# use augmentation\n",
    "if args.use_augmentation:\n",
    "    sufix_title += f\"_aug\"\n",
    "if args.use_augmentation:\n",
    "    sufix_title += f\"-{args.augmentation_method}\"\n",
    "\n",
    "# normale training parameters\n",
    "sufix_title += f\"_{args.learning_rate}-lr\"\n",
    "sufix_title += f\"_{args.supervised_ratio}-sr\"\n",
    "sufix_title += f\"_{args.nb_epoch}-e\"\n",
    "sufix_title += f\"_{args.batch_size}-bs\"\n",
    "sufix_title += f\"_{args.seed}-seed\"\n",
    "\n",
    "tensorboard_title = tensorboard_root + sufix_title\n",
    "checkpoint_title = checkpoint_root + sufix_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../tensorboard/speechcommand/mean-teacher_mixup/wideresnet28_2/0.1/2021-02-26_09:39:10_wideresnet28_2_0.999-emaa_50-wl_1-lccm_cc-js-SOFTMAX_aug-pick-one_0.001-lr_0.1-sr_200-e_64-bs_1234-seed\n"
     ]
    }
   ],
   "source": [
    "tensorboard = mSummaryWriter(\n",
    "    log_dir=\"%s/%s\" % (tensorboard_path, tensorboard_title), comment=model_func.__name__\n",
    ")\n",
    "print(os.path.join(tensorboard_path, tensorboard_title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer & callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = load_optimizer(\n",
    "    args.dataset, \"mean-teacher\", student=student, learning_rate=args.learning_rate\n",
    ")\n",
    "callbacks = load_callbacks(\n",
    "    args.dataset, \"mean-teacher\", optimizer=optimizer, nb_epoch=args.nb_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint initialise at:  /home/lcances/sync/Documents_sync/Projet/semi-supervised/model_save/speechcommand/mean-teacher_mixup/wideresnet28_2/0.1/wideresnet28_2_0.999-emaa_50-wl_1-lccm_cc-js-SOFTMAX_aug-pick-one_0.001-lr_0.1-sr_200-e_64-bs_1234-seed.torch\n",
      "name:  wideresnet28_2_0.999-emaa_50-wl_1-lccm_cc-js-SOFTMAX_aug-pick-one_0.001-lr_0.1-sr_200-e_64-bs_1234-seed.torch\n",
      "mode:  max\n"
     ]
    }
   ],
   "source": [
    "# losses\n",
    "loss_ce = nn.CrossEntropyLoss(reduction=\"mean\")  # Supervised loss\n",
    "\n",
    "if args.ccost_method == \"mse\":\n",
    "    consistency_cost = nn.MSELoss(reduction=\"mean\")  # Unsupervised loss\n",
    "elif args.ccost_method == \"js\":\n",
    "    consistency_cost = JensenShanon\n",
    "\n",
    "lambda_cost = Warmup(args.lambda_cost_max, args.warmup_length, sigmoid_rampup)\n",
    "callbacks += [lambda_cost]\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint = CheckPoint(\n",
    "    student,\n",
    "    optimizer,\n",
    "    mode=\"max\",\n",
    "    name=\"%s/%s.torch\" % (checkpoint_path, checkpoint_title),\n",
    ")\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function SSL.losses.JensenShanon(logits_1, logits_2)>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consistency_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_calculator():\n",
    "    def c(logits, y):\n",
    "        with torch.no_grad():\n",
    "            y_one_hot = F.one_hot(y, num_classes=args.num_classes)\n",
    "\n",
    "            pred = torch.softmax(logits, dim=1)\n",
    "            arg = torch.argmax(logits, dim=1)\n",
    "\n",
    "            acc = c.fn.acc(arg, y).mean\n",
    "            f1 = c.fn.f1(pred, y_one_hot).mean\n",
    "\n",
    "            return (\n",
    "                acc,\n",
    "                f1,\n",
    "            )\n",
    "\n",
    "    c.fn = DotDict(\n",
    "        acc=CategoricalAccuracy(),\n",
    "        f1=FScore(),\n",
    "    )\n",
    "\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_student_s_metrics = metrics_calculator()\n",
    "calc_student_u_metrics = metrics_calculator()\n",
    "calc_teacher_s_metrics = metrics_calculator()\n",
    "calc_teacher_u_metrics = metrics_calculator()\n",
    "\n",
    "avg_Sce = ContinueAverage()\n",
    "avg_Tce = ContinueAverage()\n",
    "avg_ccost = ContinueAverage()\n",
    "\n",
    "softmax_fn = lambda x: x\n",
    "if args.ccost_softmax:\n",
    "    softmax_fn = nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "def reset_metrics():\n",
    "    for d in [\n",
    "        calc_student_s_metrics.fn,\n",
    "        calc_student_u_metrics.fn,\n",
    "        calc_teacher_s_metrics.fn,\n",
    "        calc_teacher_u_metrics.fn,\n",
    "    ]:\n",
    "        for fn in d.values():\n",
    "            fn.reset()\n",
    "\n",
    "\n",
    "maximum_tracker = track_maximum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Can resume previous training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if args.resume:\n",
    "    checkpoint.load_last()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".        Epoch  - %      - Student:   ce       ccost    acc_s    f1_s     acc_u    f1_u     | Teacher:   ce       acc_s    f1_s     acc_u    f1_u     - Time    \n"
     ]
    }
   ],
   "source": [
    "UNDERLINE_SEQ = \"\\033[1;4m\"\n",
    "RESET_SEQ = \"\\033[0m\"\n",
    "\n",
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<10.8} {:<8.6} {:<8.6} {:<8.6} {:<8.6} {:<8.6} {:<8.6} | {:<10.8} {:<8.6} {:<8.6} {:<8.6} {:<8.6} {:<8.6} - {:<8.6}\"\n",
    "value_form = \"{:<8.8} {:<6d} - {:<6d} - {:<10.8} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} | {:<10.8} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} - {:<8.4f}\"\n",
    "header = header_form.format(\n",
    "    \".               \",\n",
    "    \"Epoch\",\n",
    "    \"%\",\n",
    "    \"Student:\",\n",
    "    \"ce\",\n",
    "    \"ccost\",\n",
    "    \"acc_s\",\n",
    "    \"f1_s\",\n",
    "    \"acc_u\",\n",
    "    \"f1_u\",\n",
    "    \"Teacher:\",\n",
    "    \"ce\",\n",
    "    \"acc_s\",\n",
    "    \"f1_s\",\n",
    "    \"acc_u\",\n",
    "    \"f1_u\",\n",
    "    \"Time\",\n",
    ")\n",
    "\n",
    "train_form = value_form\n",
    "val_form = UNDERLINE_SEQ + value_form + RESET_SEQ\n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_teacher_model(student_model, teacher_model, alpha, epoch):\n",
    "\n",
    "    # Use the true average until the exponential average is more correct\n",
    "    alpha = min(1 - 1 / (epoch + 1), alpha)\n",
    "\n",
    "    for param, ema_param in zip(student_model.parameters(), teacher_model.parameters()):\n",
    "        ema_param.data.mul_(alpha).add_(param.data, alpha=1 - alpha)\n",
    "\n",
    "\n",
    "noise_fn = lambda x: x\n",
    "if args.teacher_noise != 0:\n",
    "    n_db = args.teacher_noise\n",
    "    noise_fn = transforms.Lambda(\n",
    "        lambda x: x + (torch.rand(x.shape).cuda() * n_db + n_db)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup_fn = MixUpBatchShuffle(\n",
    "    alpha=args.mixup_alpha, apply_max=args.mixup_max, mix_labels=args.mixup_label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 64, 32])\n",
      "torch.Size([58, 64, 32])\n"
     ]
    }
   ],
   "source": [
    "for S, U in train_loader:\n",
    "    x_s, y_s, t_x_s, _ = *S[0], *S[1]\n",
    "    x_u, y_u, t_x_u, _ = *U[0], *U[1]\n",
    "\n",
    "    print(t_x_s.shape)\n",
    "    print(t_x_u.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "\n",
    "    nb_batch = len(train_loader)\n",
    "\n",
    "    reset_metrics()\n",
    "    student.train()\n",
    "\n",
    "    for i, (S, U) in enumerate(train_loader):\n",
    "        # Apply augmentation for the teacher model\n",
    "        #         x_s, y_s, t_x_s, _ = *S[0], *S[1]\n",
    "        #         x_u, y_u, t_x_u, _ = *U[0], *U[1]\n",
    "\n",
    "        # Apply augmentation for the student model\n",
    "        t_x_s, y_s, x_s, _ = *S[0], *S[1]\n",
    "        t_x_u, y_u, x_u, _ = *U[0], *U[1]\n",
    "\n",
    "        # Apply mixup if needed, otherwise no mixup.\n",
    "        if args.mixup:\n",
    "            t_x_s, t_y_s = mixup_fn(t_x_s, y_s)\n",
    "            t_x_u, t_y_u = mixup_fn(t_x_u, y_u)\n",
    "\n",
    "        t_x_s, t_x_u = t_x_s.cuda(), t_x_u.cuda()\n",
    "        x_s, x_u = x_s.cuda(), x_u.cuda()\n",
    "        y_s, y_u = y_s.cuda(), y_u.cuda()\n",
    "\n",
    "        # Predictions\n",
    "        student_s_logits = student(x_s)\n",
    "        student_u_logits = student(x_u)\n",
    "        teacher_s_logits = teacher(t_x_s)\n",
    "        teacher_u_logits = teacher(t_x_u)\n",
    "\n",
    "        # Calculate supervised loss (only student on S)\n",
    "        loss = loss_ce(student_s_logits, y_s)\n",
    "\n",
    "        # Calculate consistency cost (mse(student(x), teacher(x))) x is S + U\n",
    "        student_logits = torch.cat((student_s_logits, student_u_logits), dim=0)\n",
    "        teacher_logits = torch.cat((teacher_s_logits, teacher_u_logits), dim=0)\n",
    "        ccost = consistency_cost(softmax_fn(student_logits), softmax_fn(teacher_logits))\n",
    "        #         ccost = consistency_cost(softmax_fn(student_u_logits), softmax_fn(teacher_u_logits))\n",
    "\n",
    "        total_loss = loss + lambda_cost() * ccost\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            # Teacher prediction (for metrics purpose)\n",
    "            _teacher_loss = loss_ce(teacher_s_logits, y_s)\n",
    "\n",
    "            # Update teacher\n",
    "            update_teacher_model(student, teacher, args.ema_alpha, epoch * nb_batch + i)\n",
    "\n",
    "            # Compute the metrics for the student\n",
    "            student_s_metrics = calc_student_s_metrics(student_s_logits, y_s)\n",
    "            student_u_metrics = calc_student_u_metrics(student_u_logits, y_u)\n",
    "            student_s_acc, student_s_f1, student_u_acc, student_u_f1 = (\n",
    "                *student_s_metrics,\n",
    "                *student_u_metrics,\n",
    "            )\n",
    "\n",
    "            # Compute the metrics for the teacher\n",
    "            teacher_s_metrics = calc_teacher_s_metrics(teacher_s_logits, y_s)\n",
    "            teacher_u_metrics = calc_teacher_u_metrics(teacher_u_logits, y_u)\n",
    "            teacher_s_acc, teacher_s_f1, teacher_u_acc, teacher_u_f1 = (\n",
    "                *teacher_s_metrics,\n",
    "                *teacher_u_metrics,\n",
    "            )\n",
    "\n",
    "            # Running average of the two losses\n",
    "            student_running_loss = avg_Sce(loss.item()).mean\n",
    "            teacher_running_loss = avg_Tce(_teacher_loss.item()).mean\n",
    "            running_ccost = avg_ccost(ccost.item()).mean\n",
    "\n",
    "            # logs\n",
    "            print(\n",
    "                train_form.format(\n",
    "                    \"Training: \",\n",
    "                    epoch + 1,\n",
    "                    int(100 * (i + 1) / nb_batch),\n",
    "                    \"\",\n",
    "                    student_running_loss,\n",
    "                    running_ccost,\n",
    "                    *student_s_metrics,\n",
    "                    *student_u_metrics,\n",
    "                    \"\",\n",
    "                    teacher_running_loss,\n",
    "                    *teacher_s_metrics,\n",
    "                    *teacher_u_metrics,\n",
    "                    time.time() - start_time\n",
    "                ),\n",
    "                end=\"\\r\",\n",
    "            )\n",
    "\n",
    "    tensorboard.add_scalar(\"train/student_acc_s\", student_s_acc, epoch)\n",
    "    tensorboard.add_scalar(\"train/student_acc_u\", student_u_acc, epoch)\n",
    "    tensorboard.add_scalar(\"train/student_f1_s\", student_s_f1, epoch)\n",
    "    tensorboard.add_scalar(\"train/student_f1_u\", student_u_f1, epoch)\n",
    "\n",
    "    tensorboard.add_scalar(\"train/teacher_acc_s\", teacher_s_acc, epoch)\n",
    "    tensorboard.add_scalar(\"train/teacher_acc_u\", teacher_u_acc, epoch)\n",
    "    tensorboard.add_scalar(\"train/teacher_f1_s\", teacher_s_f1, epoch)\n",
    "    tensorboard.add_scalar(\"train/teacher_f1_u\", teacher_u_f1, epoch)\n",
    "\n",
    "    tensorboard.add_scalar(\"train/student_loss\", student_running_loss, epoch)\n",
    "    tensorboard.add_scalar(\"train/teacher_loss\", teacher_running_loss, epoch)\n",
    "    tensorboard.add_scalar(\"train/consistency_cost\", running_ccost, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "    reset_metrics()\n",
    "    student.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (X, y) in enumerate(val_loader):\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            # Predictions\n",
    "            student_logits = student(X)\n",
    "            teacher_logits = teacher(X)\n",
    "\n",
    "            # Calculate supervised loss (only student on S)\n",
    "            loss = loss_ce(student_logits, y)\n",
    "            _teacher_loss = loss_ce(teacher_logits, y)  # for metrics only\n",
    "            ccost = consistency_cost(\n",
    "                softmax_fn(student_logits), softmax_fn(teacher_logits)\n",
    "            )\n",
    "\n",
    "            # Compute the metrics\n",
    "            y_one_hot = F.one_hot(y, num_classes=args.num_classes)\n",
    "\n",
    "            # ---- student ----\n",
    "            student_metrics = calc_student_s_metrics(student_logits, y)\n",
    "            student_acc, student_f1 = student_metrics\n",
    "\n",
    "            # ---- teacher ----\n",
    "            teacher_metrics = calc_teacher_s_metrics(teacher_logits, y)\n",
    "            teacher_acc, teacher_f1 = teacher_metrics\n",
    "\n",
    "            # Running average of the two losses\n",
    "            student_running_loss = avg_Sce(loss.item()).mean\n",
    "            teacher_running_loss = avg_Tce(_teacher_loss.item()).mean\n",
    "            running_ccost = avg_ccost(ccost.item()).mean\n",
    "\n",
    "            # logs\n",
    "            print(\n",
    "                val_form.format(\n",
    "                    \"Validation: \",\n",
    "                    epoch + 1,\n",
    "                    int(100 * (i + 1) / len(val_loader)),\n",
    "                    \"\",\n",
    "                    student_running_loss,\n",
    "                    running_ccost,\n",
    "                    *student_metrics,\n",
    "                    0.0,\n",
    "                    0.0,\n",
    "                    \"\",\n",
    "                    teacher_running_loss,\n",
    "                    *teacher_metrics,\n",
    "                    0.0,\n",
    "                    0.0,\n",
    "                    time.time() - start_time\n",
    "                ),\n",
    "                end=\"\\r\",\n",
    "            )\n",
    "\n",
    "    tensorboard.add_scalar(\"val/student_acc\", student_acc, epoch)\n",
    "    tensorboard.add_scalar(\"val/student_f1\", student_f1, epoch)\n",
    "    tensorboard.add_scalar(\"val/teacher_acc\", teacher_acc, epoch)\n",
    "    tensorboard.add_scalar(\"val/teacher_f1\", teacher_f1, epoch)\n",
    "    tensorboard.add_scalar(\"val/student_loss\", student_running_loss, epoch)\n",
    "    tensorboard.add_scalar(\"val/teacher_loss\", teacher_running_loss, epoch)\n",
    "    tensorboard.add_scalar(\"val/consistency_cost\", running_ccost, epoch)\n",
    "\n",
    "    tensorboard.add_scalar(\"hyperparameters/learning_rate\", get_lr(optimizer), epoch)\n",
    "    tensorboard.add_scalar(\"hyperparameters/lambda_cost_max\", lambda_cost(), epoch)\n",
    "\n",
    "    tensorboard.add_scalar(\n",
    "        \"max/student_acc\", maximum_tracker(\"student_acc\", student_acc), epoch\n",
    "    )\n",
    "    tensorboard.add_scalar(\n",
    "        \"max/teacher_acc\", maximum_tracker(\"teacher_acc\", teacher_acc), epoch\n",
    "    )\n",
    "    tensorboard.add_scalar(\n",
    "        \"max/student_f1\", maximum_tracker(\"student_f1\", student_f1), epoch\n",
    "    )\n",
    "    tensorboard.add_scalar(\n",
    "        \"max/teacher_f1\", maximum_tracker(\"teacher_f1\", teacher_f1), epoch\n",
    "    )\n",
    "\n",
    "    checkpoint.step(teacher_acc)\n",
    "    for c in callbacks:\n",
    "        c.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".        Epoch  - %      - Student:   ce       ccost    acc_s    f1_s     acc_u    f1_u     | Teacher:   ce       acc_s    f1_s     acc_u    f1_u     - Time    \n",
      "\n",
      "Training 1      - 100    -            2.9660   0.0002   0.1759   0.0325   0.2017   0.0687   |            3.0581   0.1757   0.0024   0.1882   0.0114   - 469.5872\n",
      "\u001b[1;4mValidati 1      - 100    -            2.8437   0.0003   0.4788   0.2910   0.0000   0.0000   |            2.9687   0.4448   0.0618   0.0000   0.0000   - 15.7468 \u001b[0m\n",
      "Training 2      - 100    -            2.3572   0.0004   0.4726   0.3451   0.5329   0.4514   |            2.3817   0.5530   0.2332   0.6092   0.3575   - 417.0120\n",
      "\u001b[1;4mValidati 2      - 100    -            2.2902   0.0004   0.7005   0.6752   0.0000   0.0000   |            2.3139   0.7384   0.6328   0.0000   0.0000   - 14.0636 \u001b[0m\n",
      "Training 3      - 100    -            2.0113   0.0005   0.6013   0.5535   0.6330   0.6130   |            1.9123   0.7297   0.6225   0.7652   0.7081   - 449.1495\n",
      "\u001b[1;4mValidati 3      - 100    -            1.9704   0.0005   0.7693   0.7602   0.0000   0.0000   |            1.8716   0.8167   0.7922   0.0000   0.0000   - 14.7930 \u001b[0m\n",
      "Training 4      - 100    -            1.7949   0.0006   0.6493   0.6263   0.6782   0.6747   |            1.6073   0.8019   0.7693   0.8161   0.8034   - 460.6821\n",
      "\u001b[1;4mValidati 4      - 100    -            1.7651   0.0006   0.8250   0.8296   0.0000   0.0000   |            1.5809   0.8496   0.8439   0.0000   0.0000   - 14.0988 \u001b[0m\n",
      "Training 5      - 100    -            1.6403   0.0006   0.6822   0.6663   0.6954   0.6984   |            1.3975   0.8396   0.8225   0.8405   0.8390   - 455.0639\n",
      "\u001b[1;4mValidati 5      - 100    -            1.6192   0.0006   0.8277   0.8240   0.0000   0.0000   |            1.3791   0.8682   0.8623   0.0000   0.0000   - 13.6661 \u001b[0m\n",
      "Training 6      - 100    -            1.5232   0.0007   0.7138   0.7062   0.7091   0.7174   |            1.2421   0.8669   0.8593   0.8572   0.8590   - 415.2455\n",
      "\u001b[1;4mValidati 6      - 100    -            1.5064   0.0007   0.8461   0.8548   0.0000   0.0000   |            1.2286   0.8750   0.8763   0.0000   0.0000   - 13.0218 \u001b[0m\n",
      "Training 7      - 100    -            1.4318   0.0007   0.7264   0.7217   0.7142   0.7255   |            1.1221   0.8857   0.8808   0.8653   0.8697   - 417.1071\n",
      "\u001b[1;4mValidati 7      - 100    -            1.4194   0.0007   0.8305   0.8362   0.0000   0.0000   |            1.1119   0.8808   0.8829   0.0000   0.0000   - 13.1660 \u001b[0m\n",
      "Training 8      - 100    -            1.3587   0.0007   0.7401   0.7348   0.7178   0.7296   |            1.0260   0.9007   0.8961   0.8733   0.8781   - 424.3529\n",
      "\u001b[1;4mValidati 8      - 100    -            1.3476   0.0007   0.8580   0.8643   0.0000   0.0000   |            1.0180   0.8909   0.8924   0.0000   0.0000   - 13.0952 \u001b[0m\n",
      "Training 9      - 100    -            1.2978   0.0008   0.7461   0.7462   0.7168   0.7316   |            0.9483   0.9124   0.9054   0.8779   0.8835   - 426.8403\n",
      "\u001b[1;4mValidati 9      - 100    -            1.2883   0.0008   0.8638   0.8715   0.0000   0.0000   |            0.9418   0.8960   0.8965   0.0000   0.0000   - 13.1734 \u001b[0m\n",
      "Training 10     - 100    -            1.2467   0.0008   0.7508   0.7495   0.7118   0.7269   |            0.8838   0.9122   0.9112   0.8811   0.8873   - 422.3511\n",
      "\u001b[1;4mValidati 10     - 100    -            1.2387   0.0008   0.8658   0.8731   0.0000   0.0000   |            0.8785   0.8967   0.8991   0.0000   0.0000   - 13.4953 \u001b[0m\n",
      "Training 11     - 100    -            1.2029   0.0008   0.7612   0.7632   0.7130   0.7287   |            0.8291   0.9233   0.9204   0.8822   0.8893   - 417.6521\n",
      "\u001b[1;4mValidati 11     - 100    -            1.1956   0.0008   0.8798   0.8883   0.0000   0.0000   |            0.8247   0.8972   0.9004   0.0000   0.0000   - 12.8109 \u001b[0m\n",
      "Training 12     - 100    -            1.1634   0.0008   0.7682   0.7727   0.7075   0.7250   |            0.7816   0.9325   0.9300   0.8838   0.8918   - 409.0858\n",
      "\u001b[1;4mValidati 12     - 100    -            1.1571   0.0008   0.8803   0.8891   0.0000   0.0000   |            0.7780   0.8977   0.9023   0.0000   0.0000   - 12.7453 \u001b[0m\n",
      "Training 13     - 100    -            1.1294   0.0009   0.7778   0.7784   0.7004   0.7182   |            0.7405   0.9354   0.9337   0.8851   0.8926   - 396.8881\n",
      "\u001b[1;4mValidati 13     - 100    -            1.1242   0.0009   0.8676   0.8762   0.0000   0.0000   |            0.7374   0.8998   0.9041   0.0000   0.0000   - 12.4167 \u001b[0m\n",
      "Training 14     - 100    -            1.0993   0.0009   0.7792   0.7842   0.7017   0.7196   |            0.7043   0.9386   0.9368   0.8857   0.8938   - 406.1977\n",
      "\u001b[1;4mValidati 14     - 100    -            1.0944   0.0009   0.8755   0.8843   0.0000   0.0000   |            0.7017   0.8985   0.9036   0.0000   0.0000   - 12.8362 \u001b[0m\n",
      "Training 15     - 100    -            1.0717   0.0009   0.7886   0.7933   0.6893   0.7108   |            0.6725   0.9405   0.9382   0.8846   0.8929   - 408.3861\n",
      "\u001b[1;4mValidati 15     - 100    -            1.0671   0.0009   0.8863   0.8939   0.0000   0.0000   |            0.6703   0.8963   0.9016   0.0000   0.0000   - 12.6890 \u001b[0m\n",
      "Training 16     - 100    -            1.0473   0.0010   0.7918   0.7965   0.6843   0.7038   |            0.6440   0.9433   0.9434   0.8845   0.8929   - 410.3622\n",
      "\u001b[1;4mValidati 16     - 100    -            1.0435   0.0009   0.8707   0.8813   0.0000   0.0000   |            0.6422   0.8943   0.9027   0.0000   0.0000   - 12.6305 \u001b[0m\n",
      "Training 17     - 100    -            1.0261   0.0010   0.7923   0.7921   0.6774   0.6991   |            0.6186   0.9464   0.9457   0.8833   0.8920   - 401.2378\n",
      "\u001b[1;4mValidati 17     - 100    -            1.0227   0.0010   0.8705   0.8808   0.0000   0.0000   |            0.6170   0.8930   0.9009   0.0000   0.0000   - 12.7122 \u001b[0m\n",
      "Training 18     - 100    -            1.0069   0.0010   0.7933   0.7951   0.6717   0.6933   |            0.5958   0.9452   0.9437   0.8827   0.8912   - 407.3759\n",
      "\u001b[1;4mValidati 18     - 100    -            1.0034   0.0010   0.8862   0.8956   0.0000   0.0000   |            0.5945   0.8915   0.9008   0.0000   0.0000   - 12.8543 \u001b[0m\n",
      "Training 19     - 100    -            0.9870   0.0010   0.8110   0.8118   0.6625   0.6858   |            0.5755   0.9490   0.9461   0.8820   0.8909   - 409.9002\n",
      "\u001b[1;4mValidati 19     - 100    -            0.9846   0.0010   0.8542   0.8646   0.0000   0.0000   |            0.5743   0.8925   0.9004   0.0000   0.0000   - 12.7799 \u001b[0m\n",
      "Training 20     - 100    -            0.9704   0.0011   0.8049   0.8086   0.6556   0.6789   |            0.5566   0.9494   0.9487   0.8812   0.8902   - 409.7466\n",
      "\u001b[1;4mValidati 20     - 100    -            0.9675   0.0010   0.8803   0.8874   0.0000   0.0000   |            0.5555   0.8959   0.9017   0.0000   0.0000   - 14.3299 \u001b[0m\n",
      "Training 21     - 100    -            0.9539   0.0011   0.8147   0.8164   0.6466   0.6718   |            0.5392   0.9515   0.9498   0.8818   0.8915   - 406.4808\n",
      "\u001b[1;4mValidati 21     - 100    -            0.9515   0.0011   0.8745   0.8818   0.0000   0.0000   |            0.5384   0.8882   0.8965   0.0000   0.0000   - 12.2601 \u001b[0m\n",
      "Training 22     - 100    -            0.9394   0.0011   0.8103   0.8133   0.6411   0.6661   |            0.5234   0.9500   0.9487   0.8800   0.8896   - 400.5267\n",
      "\u001b[1;4mValidati 22     - 100    -            0.9375   0.0011   0.8662   0.8731   0.0000   0.0000   |            0.5227   0.8927   0.9007   0.0000   0.0000   - 12.9904 \u001b[0m\n",
      "Training 23     - 100    -            0.9262   0.0011   0.8137   0.8145   0.6302   0.6549   |            0.5092   0.9513   0.9498   0.8785   0.8883   - 405.9388\n",
      "\u001b[1;4mValidati 23     - 100    -            0.9242   0.0011   0.8749   0.8832   0.0000   0.0000   |            0.5086   0.8914   0.8985   0.0000   0.0000   - 13.0278 \u001b[0m\n",
      "Training 24     - 100    -            0.9141   0.0012   0.8098   0.8161   0.6233   0.6490   |            0.4964   0.9502   0.9481   0.8788   0.8885   - 400.7042\n",
      "\u001b[1;4mValidati 24     - 100    -            0.9125   0.0012   0.8567   0.8662   0.0000   0.0000   |            0.4958   0.8903   0.8976   0.0000   0.0000   - 12.9595 \u001b[0m\n",
      "Training 25     - 100    -            0.9026   0.0012   0.8190   0.8201   0.6146   0.6407   |            0.4843   0.9518   0.9496   0.8765   0.8857   - 406.1421\n",
      "\u001b[1;4mValidati 25     - 100    -            0.9008   0.0012   0.8783   0.8879   0.0000   0.0000   |            0.4838   0.8909   0.8976   0.0000   0.0000   - 12.5613 \u001b[0m\n",
      "Training 26     - 100    -            0.8917   0.0012   0.8152   0.8197   0.6050   0.6316   |            0.4733   0.9497   0.9476   0.8755   0.8850   - 409.9469\n",
      "\u001b[1;4mValidati 26     - 100    -            0.8900   0.0012   0.8760   0.8830   0.0000   0.0000   |            0.4729   0.8915   0.8968   0.0000   0.0000   - 13.0775 \u001b[0m\n",
      "Training 27     - 100    -            0.8814   0.0012   0.8243   0.8273   0.5965   0.6232   |            0.4628   0.9546   0.9520   0.8736   0.8840   - 411.2567\n",
      "\u001b[1;4mValidati 27     - 100    -            0.8801   0.0012   0.8654   0.8743   0.0000   0.0000   |            0.4625   0.8870   0.8950   0.0000   0.0000   - 13.0197 \u001b[0m\n",
      "Training 28     - 100    -            0.8718   0.0013   0.8269   0.8309   0.5893   0.6164   |            0.4531   0.9501   0.9498   0.8705   0.8809   - 413.0189\n",
      "\u001b[1;4mValidati 28     - 100    -            0.8704   0.0013   0.8741   0.8832   0.0000   0.0000   |            0.4529   0.8847   0.8942   0.0000   0.0000   - 13.3109 \u001b[0m\n",
      "Training 29     - 100    -            0.8624   0.0013   0.8255   0.8284   0.5818   0.6093   |            0.4444   0.9489   0.9471   0.8689   0.8798   - 427.9742\n",
      "\u001b[1;4mValidati 29     - 100    -            0.8610   0.0013   0.8794   0.8871   0.0000   0.0000   |            0.4443   0.8857   0.8922   0.0000   0.0000   - 13.3458 \u001b[0m\n",
      "Training 30     - 100    -            0.8541   0.0013   0.8220   0.8263   0.5736   0.6006   |            0.4364   0.9472   0.9450   0.8682   0.8790   - 429.9405\n",
      "\u001b[1;4mValidati 30     - 100    -            0.8530   0.0013   0.8701   0.8782   0.0000   0.0000   |            0.4362   0.8838   0.8920   0.0000   0.0000   - 13.9086 \u001b[0m\n",
      "Training 31     - 100    -            0.8464   0.0013   0.8234   0.8247   0.5650   0.5941   |            0.4289   0.9484   0.9462   0.8688   0.8785   - 416.3746\n",
      "\u001b[1;4mValidati 31     - 100    -            0.8453   0.0013   0.8701   0.8779   0.0000   0.0000   |            0.4288   0.8844   0.8902   0.0000   0.0000   - 12.8418 \u001b[0m\n",
      "Training 32     - 100    -            0.8393   0.0014   0.8185   0.8229   0.5564   0.5845   |            0.4221   0.9435   0.9438   0.8663   0.8760   - 414.3213\n",
      "\u001b[1;4mValidati 32     - 100    -            0.8382   0.0014   0.8769   0.8844   0.0000   0.0000   |            0.4221   0.8828   0.8881   0.0000   0.0000   - 13.2226 \u001b[0m\n",
      "Training 33     - 100    -            0.8322   0.0014   0.8283   0.8314   0.5455   0.5734   |            0.4160   0.9430   0.9403   0.8631   0.8724   - 421.1683\n",
      "\u001b[1;4mValidati 33     - 100    -            0.8310   0.0014   0.8856   0.8928   0.0000   0.0000   |            0.4160   0.8827   0.8880   0.0000   0.0000   - 13.0991 \u001b[0m\n",
      "Training 34     - 100    -            0.8258   0.0014   0.8267   0.8310   0.5381   0.5665   |            0.4100   0.9465   0.9443   0.8632   0.8729   - 431.8977\n",
      "\u001b[1;4mValidati 34     - 100    -            0.8247   0.0014   0.8811   0.8897   0.0000   0.0000   |            0.4100   0.8811   0.8865   0.0000   0.0000   - 12.5990 \u001b[0m\n",
      "Training 35     - 100    -            0.8195   0.0014   0.8306   0.8343   0.5282   0.5561   |            0.4046   0.9418   0.9388   0.8596   0.8701   - 405.2994\n",
      "\u001b[1;4mValidati 35     - 100    -            0.8185   0.0014   0.8781   0.8841   0.0000   0.0000   |            0.4047   0.8776   0.8819   0.0000   0.0000   - 12.8132 \u001b[0m\n",
      "Training 36     - 100    -            0.8131   0.0015   0.8345   0.8399   0.5207   0.5478   |            0.3995   0.9418   0.9396   0.8577   0.8683   - 415.5840\n",
      "\u001b[1;4mValidati 36     - 100    -            0.8122   0.0015   0.8770   0.8854   0.0000   0.0000   |            0.3996   0.8765   0.8826   0.0000   0.0000   - 13.3777 \u001b[0m\n",
      "Training 37     - 100    -            0.8079   0.0015   0.8248   0.8314   0.5120   0.5408   |            0.3949   0.9394   0.9366   0.8567   0.8668   - 415.6955\n",
      "\u001b[1;4mValidati 37     - 100    -            0.8071   0.0015   0.8706   0.8780   0.0000   0.0000   |            0.3951   0.8735   0.8813   0.0000   0.0000   - 13.1163 \u001b[0m\n",
      "Training 38     - 100    -            0.8034   0.0015   0.8222   0.8274   0.5051   0.5326   |            0.3908   0.9343   0.9342   0.8530   0.8631   - 409.0534\n",
      "\u001b[1;4mValidati 38     - 100    -            0.8025   0.0015   0.8794   0.8869   0.0000   0.0000   |            0.3910   0.8696   0.8763   0.0000   0.0000   - 12.7165 \u001b[0m\n",
      "Training 39     - 100    -            0.7988   0.0016   0.8276   0.8317   0.4971   0.5256   |            0.3870   0.9354   0.9332   0.8511   0.8609   - 410.3792\n",
      "\u001b[1;4mValidati 39     - 100    -            0.7982   0.0015   0.8663   0.8735   0.0000   0.0000   |            0.3871   0.8682   0.8766   0.0000   0.0000   - 13.1942 \u001b[0m\n",
      "Training 40     - 100    -            0.7952   0.0016   0.8163   0.8236   0.4911   0.5192   |            0.3834   0.9343   0.9313   0.8499   0.8604   - 406.3746\n",
      "\u001b[1;4mValidati 40     - 100    -            0.7945   0.0016   0.8664   0.8746   0.0000   0.0000   |            0.3836   0.8673   0.8730   0.0000   0.0000   - 12.6380 \u001b[0m\n",
      "Training 41     - 100    -            0.7917   0.0016   0.8221   0.8238   0.4794   0.5066   |            0.3803   0.9318   0.9289   0.8485   0.8589   - 431.3692\n",
      "\u001b[1;4mValidati 41     - 100    -            0.7910   0.0016   0.8695   0.8761   0.0000   0.0000   |            0.3805   0.8638   0.8692   0.0000   0.0000   - 13.2308 \u001b[0m\n",
      "Training 42     - 100    -            0.7879   0.0016   0.8275   0.8322   0.4714   0.4971   |            0.3775   0.9272   0.9258   0.8460   0.8555   - 397.8295\n",
      "\u001b[1;4mValidati 42     - 100    -            0.7872   0.0016   0.8685   0.8756   0.0000   0.0000   |            0.3777   0.8618   0.8672   0.0000   0.0000   - 12.0937 \u001b[0m\n",
      "Training 43     - 100    -            0.7844   0.0017   0.8225   0.8318   0.4658   0.4942   |            0.3750   0.9243   0.9242   0.8418   0.8521   - 405.2294\n",
      "\u001b[1;4mValidati 43     - 100    -            0.7839   0.0017   0.8640   0.8704   0.0000   0.0000   |            0.3753   0.8612   0.8679   0.0000   0.0000   - 12.9007 \u001b[0m\n",
      "Training 44     - 100    -            0.7813   0.0017   0.8208   0.8252   0.4553   0.4814   |            0.3726   0.9284   0.9266   0.8431   0.8533   - 412.7791\n",
      "\u001b[1;4mValidati 44     - 100    -            0.7808   0.0017   0.8660   0.8715   0.0000   0.0000   |            0.3728   0.8626   0.8677   0.0000   0.0000   - 12.6529 \u001b[0m\n",
      "Training 45     - 100    -            0.7788   0.0017   0.8144   0.8201   0.4481   0.4750   |            0.3704   0.9269   0.9250   0.8431   0.8529   - 392.8005\n",
      "\u001b[1;4mValidati 45     - 100    -            0.7783   0.0017   0.8636   0.8712   0.0000   0.0000   |            0.3707   0.8599   0.8664   0.0000   0.0000   - 12.3299 \u001b[0m\n",
      "Training 46     - 100    -            0.7764   0.0017   0.8198   0.8245   0.4440   0.4701   |            0.3686   0.9191   0.9188   0.8408   0.8514   - 398.5762\n",
      "\u001b[1;4mValidati 46     - 100    -            0.7758   0.0017   0.8760   0.8832   0.0000   0.0000   |            0.3689   0.8565   0.8635   0.0000   0.0000   - 12.6858 \u001b[0m\n",
      "Training 47     - 100    -            0.7741   0.0018   0.8121   0.8207   0.4337   0.4603   |            0.3670   0.9175   0.9163   0.8397   0.8504   - 414.5501\n",
      "\u001b[1;4mValidati 47     - 100    -            0.7735   0.0017   0.8732   0.8810   0.0000   0.0000   |            0.3673   0.8548   0.8620   0.0000   0.0000   - 12.6515 \u001b[0m\n",
      "Training 48     - 100    -            0.7720   0.0018   0.8114   0.8159   0.4265   0.4523   |            0.3655   0.9178   0.9170   0.8357   0.8467   - 419.4685\n",
      "\u001b[1;4mValidati 48     - 100    -            0.7716   0.0018   0.8656   0.8725   0.0000   0.0000   |            0.3658   0.8513   0.8583   0.0000   0.0000   - 12.7733 \u001b[0m\n",
      "Training 49     - 100    -            0.7698   0.0018   0.8236   0.8284   0.4198   0.4460   |            0.3644   0.9144   0.9119   0.8305   0.8409   - 414.4284\n",
      "\u001b[1;4mValidati 49     - 100    -            0.7694   0.0018   0.8606   0.8689   0.0000   0.0000   |            0.3648   0.8517   0.8571   0.0000   0.0000   - 14.6638 \u001b[0m\n",
      "Training 50     - 100    -            0.7682   0.0018   0.8103   0.8196   0.4133   0.4398   |            0.3636   0.9066   0.9083   0.8277   0.8389   - 406.2365\n",
      "\u001b[1;4mValidati 50     - 100    -            0.7678   0.0018   0.8697   0.8781   0.0000   0.0000   |            0.3639   0.8473   0.8524   0.0000   0.0000   - 12.5659 \u001b[0m\n",
      "Training 51     - 100    -            0.7667   0.0018   0.8154   0.8237   0.4075   0.4318   |            0.3628   0.9081   0.9076   0.8275   0.8379   - 399.5760\n",
      "\u001b[1;4mValidati 51     - 100    -            0.7663   0.0018   0.8633   0.8698   0.0000   0.0000   |            0.3631   0.8471   0.8538   0.0000   0.0000   - 12.5987 \u001b[0m\n",
      "Training 52     - 100    -            0.7649   0.0019   0.8179   0.8254   0.3995   0.4249   |            0.3620   0.9090   0.9097   0.8260   0.8368   - 400.3849\n",
      "\u001b[1;4mValidati 52     - 100    -            0.7646   0.0019   0.8560   0.8646   0.0000   0.0000   |            0.3624   0.8451   0.8524   0.0000   0.0000   - 12.9049 \u001b[0m\n",
      "Training 53     - 100    -            0.7636   0.0019   0.8131   0.8202   0.3893   0.4144   |            0.3614   0.9081   0.9083   0.8248   0.8355   - 398.9020\n",
      "\u001b[1;4mValidati 53     - 100    -            0.7633   0.0019   0.8591   0.8665   0.0000   0.0000   |            0.3618   0.8401   0.8452   0.0000   0.0000   - 12.7222 \u001b[0m\n",
      "Training 54     - 100    -            0.7622   0.0019   0.8195   0.8262   0.3832   0.4080   |            0.3610   0.9019   0.9029   0.8206   0.8307   - 403.9374\n",
      "\u001b[1;4mValidati 54     - 100    -            0.7619   0.0019   0.8561   0.8632   0.0000   0.0000   |            0.3614   0.8361   0.8421   0.0000   0.0000   - 12.7496 \u001b[0m\n",
      "Training 55     - 100    -            0.7610   0.0019   0.8105   0.8187   0.3761   0.3991   |            0.3608   0.9017   0.9018   0.8180   0.8281   - 401.5231\n",
      "\u001b[1;4mValidati 55     - 100    -            0.7608   0.0019   0.8569   0.8646   0.0000   0.0000   |            0.3612   0.8346   0.8413   0.0000   0.0000   - 12.6645 \u001b[0m\n",
      "Training 56     - 100    -            0.7596   0.0020   0.8189   0.8282   0.3722   0.3970   |            0.3607   0.9018   0.9010   0.8170   0.8276   - 398.9868\n",
      "\u001b[1;4mValidati 56     - 100    -            0.7593   0.0020   0.8553   0.8634   0.0000   0.0000   |            0.3611   0.8381   0.8432   0.0000   0.0000   - 12.6972 \u001b[0m\n",
      "Training 57     - 100    -            0.7582   0.0020   0.8189   0.8251   0.3649   0.3872   |            0.3608   0.8922   0.8948   0.8148   0.8245   - 395.9415\n",
      "\u001b[1;4mValidati 57     - 100    -            0.7581   0.0020   0.8457   0.8531   0.0000   0.0000   |            0.3612   0.8293   0.8371   0.0000   0.0000   - 12.8389 \u001b[0m\n",
      "Training 58     - 100    -            0.7574   0.0020   0.8132   0.8184   0.3588   0.3820   |            0.3608   0.8980   0.8999   0.8140   0.8251   - 401.5627\n",
      "\u001b[1;4mValidati 58     - 100    -            0.7572   0.0020   0.8445   0.8525   0.0000   0.0000   |            0.3613   0.8301   0.8374   0.0000   0.0000   - 13.1233 \u001b[0m\n",
      "Training 59     - 100    -            0.7565   0.0020   0.8159   0.8220   0.3514   0.3734   |            0.3609   0.8965   0.8951   0.8111   0.8226   - 401.4436\n",
      "\u001b[1;4mValidati 59     - 100    -            0.7563   0.0020   0.8568   0.8637   0.0000   0.0000   |            0.3614   0.8284   0.8372   0.0000   0.0000   - 12.8205 \u001b[0m\n",
      "Training 60     - 100    -            0.7557   0.0020   0.8146   0.8249   0.3460   0.3667   |            0.3611   0.8974   0.8976   0.8117   0.8224   - 400.4433\n",
      "\u001b[1;4mValidati 60     - 100    -            0.7555   0.0020   0.8548   0.8610   0.0000   0.0000   |            0.3615   0.8279   0.8369   0.0000   0.0000   - 12.8419 \u001b[0m\n",
      "Training 61     - 100    -            0.7547   0.0021   0.8192   0.8293   0.3401   0.3616   |            0.3615   0.8908   0.8900   0.8093   0.8207   - 397.4473\n",
      "\u001b[1;4mValidati 61     - 100    -            0.7546   0.0021   0.8410   0.8472   0.0000   0.0000   |            0.3619   0.8258   0.8350   0.0000   0.0000   - 13.1094 \u001b[0m\n",
      "Training 62     - 100    -            0.7543   0.0021   0.8052   0.8176   0.3328   0.3536   |            0.3620   0.8901   0.8896   0.8061   0.8162   - 401.4471\n",
      "\u001b[1;4mValidati 62     - 100    -            0.7542   0.0021   0.8427   0.8501   0.0000   0.0000   |            0.3624   0.8250   0.8327   0.0000   0.0000   - 12.7716 \u001b[0m\n",
      "Training 63     - 100    -            0.7540   0.0021   0.8043   0.8179   0.3267   0.3478   |            0.3626   0.8851   0.8863   0.8052   0.8151   - 398.7814\n",
      "\u001b[1;4mValidati 63     - 100    -            0.7538   0.0021   0.8484   0.8558   0.0000   0.0000   |            0.3631   0.8195   0.8262   0.0000   0.0000   - 12.9605 \u001b[0m\n",
      "Training 64     - 100    -            0.7532   0.0021   0.8171   0.8270   0.3184   0.3380   |            0.3633   0.8832   0.8863   0.7992   0.8093   - 399.8813\n",
      "\u001b[1;4mValidati 64     - 100    -            0.7531   0.0021   0.8482   0.8551   0.0000   0.0000   |            0.3638   0.8191   0.8250   0.0000   0.0000   - 12.7664 \u001b[0m\n",
      "Training 65     - 100    -            0.7525   0.0022   0.8139   0.8259   0.3132   0.3332   |            0.3642   0.8779   0.8789   0.7982   0.8082   - 400.8880\n",
      "\u001b[1;4mValidati 65     - 100    -            0.7524   0.0021   0.8499   0.8575   0.0000   0.0000   |            0.3647   0.8116   0.8204   0.0000   0.0000   - 12.7781 \u001b[0m\n",
      "Training 66     - 100    -            0.7522   0.0022   0.8032   0.8146   0.3044   0.3226   |            0.3652   0.8783   0.8796   0.7960   0.8054   - 389.7380\n",
      "\u001b[1;4mValidati 66     - 100    -            0.7521   0.0022   0.8486   0.8570   0.0000   0.0000   |            0.3657   0.8107   0.8168   0.0000   0.0000   - 12.8416 \u001b[0m\n",
      "Training 67     - 100    -            0.7517   0.0022   0.8137   0.8257   0.3007   0.3189   |            0.3663   0.8813   0.8803   0.7930   0.8030   - 400.3458\n",
      "\u001b[1;4mValidati 67     - 100    -            0.7516   0.0022   0.8490   0.8559   0.0000   0.0000   |            0.3668   0.8093   0.8138   0.0000   0.0000   - 12.7010 \u001b[0m\n",
      "Training 68     - 100    -            0.7511   0.0022   0.8145   0.8273   0.2990   0.3162   |            0.3672   0.8811   0.8812   0.7916   0.8040   - 400.1321\n",
      "\u001b[1;4mValidati 68     - 100    -            0.7510   0.0022   0.8440   0.8518   0.0000   0.0000   |            0.3677   0.8101   0.8151   0.0000   0.0000   - 13.3399 \u001b[0m\n",
      "Training 69     - 100    -            0.7504   0.0022   0.8139   0.8284   0.2885   0.3065   |            0.3684   0.8761   0.8785   0.7903   0.8008   - 399.8196\n",
      "\u001b[1;4mValidati 69     - 100    -            0.7503   0.0022   0.8350   0.8435   0.0000   0.0000   |            0.3689   0.8004   0.8088   0.0000   0.0000   - 12.7618 \u001b[0m\n",
      "Training 70     - 100    -            0.7502   0.0023   0.8111   0.8233   0.2875   0.3048   |            0.3696   0.8767   0.8752   0.7873   0.7977   - 399.0914\n",
      "\u001b[1;4mValidati 70     - 100    -            0.7501   0.0023   0.8444   0.8525   0.0000   0.0000   |            0.3702   0.8061   0.8146   0.0000   0.0000   - 13.0748 \u001b[0m\n",
      "Training 71     - 100    -            0.7500   0.0023   0.8064   0.8225   0.2803   0.2973   |            0.3709   0.8743   0.8752   0.7877   0.7984   - 398.2962\n",
      "\u001b[1;4mValidati 71     - 100    -            0.7500   0.0023   0.8393   0.8459   0.0000   0.0000   |            0.3714   0.8037   0.8126   0.0000   0.0000   - 12.8502 \u001b[0m\n",
      "Training 72     - 100    -            0.7498   0.0023   0.8121   0.8229   0.2768   0.2907   |            0.3725   0.8676   0.8667   0.7833   0.7942   - 392.7044\n",
      "\u001b[1;4mValidati 72     - 100    -            0.7498   0.0023   0.8347   0.8414   0.0000   0.0000   |            0.3730   0.8013   0.8096   0.0000   0.0000   - 12.7739 \u001b[0m\n",
      "Training 73     - 100    -            0.7496   0.0023   0.8113   0.8261   0.2722   0.2872   |            0.3738   0.8664   0.8684   0.7866   0.7970   - 403.1469\n",
      "\u001b[1;4mValidati 73     - 100    -            0.7496   0.0023   0.8334   0.8399   0.0000   0.0000   |            0.3743   0.8033   0.8112   0.0000   0.0000   - 13.5346 \u001b[0m\n",
      "Training 74     - 100    -            0.7496   0.0023   0.8061   0.8203   0.2679   0.2825   |            0.3752   0.8683   0.8689   0.7844   0.7954   - 390.1725\n",
      "\u001b[1;4mValidati 74     - 100    -            0.7495   0.0023   0.8410   0.8490   0.0000   0.0000   |            0.3757   0.7985   0.8077   0.0000   0.0000   - 12.9661 \u001b[0m\n",
      "Training 75     - 100    -            0.7493   0.0024   0.8100   0.8260   0.2608   0.2746   |            0.3768   0.8610   0.8650   0.7788   0.7898   - 401.4148\n",
      "\u001b[1;4mValidati 75     - 100    -            0.7493   0.0023   0.8287   0.8370   0.0000   0.0000   |            0.3773   0.7961   0.8030   0.0000   0.0000   - 12.9404 \u001b[0m\n",
      "Training 76     - 100    -            0.7494   0.0024   0.8040   0.8216   0.2564   0.2697   |            0.3786   0.8581   0.8588   0.7753   0.7865   - 401.4007\n",
      "\u001b[1;4mValidati 76     - 100    -            0.7493   0.0024   0.8394   0.8489   0.0000   0.0000   |            0.3791   0.7941   0.8027   0.0000   0.0000   - 12.9860 \u001b[0m\n",
      "Training 77     - 100    -            0.7491   0.0024   0.8124   0.8235   0.2533   0.2661   |            0.3805   0.8509   0.8527   0.7725   0.7844   - 397.7010\n",
      "\u001b[1;4mValidati 77     - 100    -            0.7492   0.0024   0.8240   0.8320   0.0000   0.0000   |            0.3810   0.7861   0.7936   0.0000   0.0000   - 12.8702 \u001b[0m\n",
      "Training 78     - 100    -            0.7488   0.0024   0.8147   0.8309   0.2462   0.2596   |            0.3824   0.8530   0.8564   0.7668   0.7780   - 400.8109\n",
      "\u001b[1;4mValidati 78     - 100    -            0.7489   0.0024   0.8301   0.8372   0.0000   0.0000   |            0.3830   0.7855   0.7957   0.0000   0.0000   - 13.1215 \u001b[0m\n",
      "Training 79     - 100    -            0.7486   0.0024   0.8170   0.8329   0.2437   0.2567   |            0.3843   0.8513   0.8523   0.7687   0.7812   - 406.3988\n",
      "\u001b[1;4mValidati 79     - 100    -            0.7486   0.0024   0.8287   0.8377   0.0000   0.0000   |            0.3849   0.7850   0.7922   0.0000   0.0000   - 13.0012 \u001b[0m\n",
      "Training 80     - 100    -            0.7486   0.0024   0.8075   0.8248   0.2393   0.2505   |            0.3864   0.8511   0.8545   0.7669   0.7779   - 400.9407\n",
      "\u001b[1;4mValidati 80     - 100    -            0.7487   0.0024   0.8224   0.8320   0.0000   0.0000   |            0.3870   0.7847   0.7929   0.0000   0.0000   - 13.1002 \u001b[0m\n",
      "Training 81     - 100    -            0.7487   0.0025   0.8107   0.8290   0.2352   0.2450   |            0.3884   0.8454   0.8477   0.7661   0.7765   - 404.7313\n",
      "\u001b[1;4mValidati 81     - 100    -            0.7488   0.0025   0.8183   0.8259   0.0000   0.0000   |            0.3890   0.7765   0.7865   0.0000   0.0000   - 12.9127 \u001b[0m\n",
      "Training 82     - 100    -            0.7486   0.0025   0.8148   0.8318   0.2291   0.2388   |            0.3906   0.8396   0.8441   0.7611   0.7734   - 401.8694\n",
      "\u001b[1;4mValidati 82     - 100    -            0.7487   0.0025   0.8194   0.8290   0.0000   0.0000   |            0.3912   0.7778   0.7854   0.0000   0.0000   - 12.8998 \u001b[0m\n",
      "Training 83     - 100    -            0.7484   0.0025   0.8156   0.8315   0.2261   0.2365   |            0.3928   0.8423   0.8460   0.7623   0.7750   - 406.2787\n",
      "\u001b[1;4mValidati 83     - 100    -            0.7485   0.0025   0.8173   0.8262   0.0000   0.0000   |            0.3933   0.7800   0.7877   0.0000   0.0000   - 13.0901 \u001b[0m\n",
      "Training 84     - 100    -            0.7486   0.0025   0.8064   0.8261   0.2239   0.2330   |            0.3949   0.8420   0.8457   0.7625   0.7750   - 404.8814\n",
      "\u001b[1;4mValidati 84     - 100    -            0.7486   0.0025   0.8332   0.8398   0.0000   0.0000   |            0.3954   0.7739   0.7836   0.0000   0.0000   - 13.0666 \u001b[0m\n",
      "Training 85     - 100    -            0.7485   0.0025   0.8123   0.8291   0.2187   0.2282   |            0.3971   0.8394   0.8412   0.7592   0.7717   - 403.2258\n",
      "\u001b[1;4mValidati 85     - 100    -            0.7486   0.0025   0.8138   0.8221   0.0000   0.0000   |            0.3976   0.7715   0.7814   0.0000   0.0000   - 13.1812 \u001b[0m\n",
      "Training 86     - 100    -            0.7483   0.0025   0.8167   0.8332   0.2158   0.2231   |            0.3993   0.8412   0.8416   0.7582   0.7701   - 403.0437\n",
      "\u001b[1;4mValidati 86     - 100    -            0.7484   0.0025   0.8312   0.8387   0.0000   0.0000   |            0.3998   0.7707   0.7782   0.0000   0.0000   - 13.2041 \u001b[0m\n",
      "Training 87     - 100    -            0.7478   0.0026   0.8212   0.8443   0.2127   0.2221   |            0.4017   0.8331   0.8352   0.7559   0.7670   - 404.0862\n",
      "\u001b[1;4mValidati 87     - 100    -            0.7479   0.0026   0.8249   0.8333   0.0000   0.0000   |            0.4023   0.7667   0.7764   0.0000   0.0000   - 13.1942 \u001b[0m\n",
      "Training 88     - 100    -            0.7480   0.0026   0.8024   0.8276   0.2077   0.2159   |            0.4040   0.8325   0.8356   0.7516   0.7631   - 407.4863\n",
      "\u001b[1;4mValidati 88     - 100    -            0.7481   0.0026   0.8258   0.8328   0.0000   0.0000   |            0.4046   0.7671   0.7751   0.0000   0.0000   - 13.1128 \u001b[0m\n",
      "Training 89     - 100    -            0.7483   0.0026   0.7992   0.8235   0.2051   0.2127   |            0.4065   0.8280   0.8343   0.7516   0.7628   - 406.1378\n",
      "\u001b[1;4mValidati 89     - 100    -            0.7484   0.0026   0.8202   0.8269   0.0000   0.0000   |            0.4070   0.7710   0.7778   0.0000   0.0000   - 13.1701 \u001b[0m\n",
      "Training 90     - 100    -            0.7483   0.0026   0.8105   0.8289   0.1976   0.2041   |            0.4090   0.8273   0.8278   0.7483   0.7579   - 402.2582\n",
      "\u001b[1;4mValidati 90     - 100    -            0.7485   0.0026   0.8013   0.8100   0.0000   0.0000   |            0.4096   0.7682   0.7728   0.0000   0.0000   - 13.5743 \u001b[0m\n",
      "Training 91     - 100    -            0.7484   0.0026   0.8131   0.8333   0.1956   0.2020   |            0.4114   0.8254   0.8263   0.7454   0.7563   - 405.3318\n",
      "\u001b[1;4mValidati 91     - 100    -            0.7485   0.0026   0.8196   0.8283   0.0000   0.0000   |            0.4120   0.7630   0.7700   0.0000   0.0000   - 13.7612 \u001b[0m\n",
      "Training 92     - 100    -            0.7486   0.0026   0.8042   0.8256   0.1915   0.1975   |            0.4139   0.8219   0.8269   0.7455   0.7571   - 404.0714\n",
      "\u001b[1;4mValidati 92     - 100    -            0.7487   0.0026   0.8190   0.8276   0.0000   0.0000   |            0.4145   0.7602   0.7704   0.0000   0.0000   - 13.2235 \u001b[0m\n",
      "Training 93     - 100    -            0.7490   0.0027   0.8027   0.8209   0.1923   0.1982   |            0.4166   0.8220   0.8231   0.7386   0.7492   - 407.4659\n",
      "\u001b[1;4mValidati 93     - 100    -            0.7491   0.0027   0.8192   0.8280   0.0000   0.0000   |            0.4171   0.7578   0.7637   0.0000   0.0000   - 13.3499 \u001b[0m\n",
      "Training 94     - 100    -            0.7489   0.0027   0.8127   0.8319   0.1861   0.1917   |            0.4192   0.8163   0.8214   0.7372   0.7480   - 403.4434\n",
      "\u001b[1;4mValidati 94     - 100    -            0.7491   0.0027   0.8016   0.8120   0.0000   0.0000   |            0.4198   0.7535   0.7612   0.0000   0.0000   - 13.3044 \u001b[0m\n",
      "Training 95     - 100    -            0.7492   0.0027   0.8098   0.8310   0.1841   0.1885   |            0.4218   0.8152   0.8189   0.7348   0.7447   - 405.4686\n",
      "\u001b[1;4mValidati 95     - 100    -            0.7493   0.0027   0.8068   0.8157   0.0000   0.0000   |            0.4224   0.7527   0.7595   0.0000   0.0000   - 13.3637 \u001b[0m\n",
      "Training 96     - 100    -            0.7495   0.0027   0.8044   0.8288   0.1790   0.1846   |            0.4245   0.8185   0.8193   0.7318   0.7420   - 403.4180\n",
      "\u001b[1;4mValidati 96     - 100    -            0.7495   0.0027   0.8203   0.8295   0.0000   0.0000   |            0.4251   0.7535   0.7589   0.0000   0.0000   - 13.5568 \u001b[0m\n",
      "Training 97     - 100    -            0.7496   0.0027   0.8097   0.8357   0.1780   0.1813   |            0.4271   0.8142   0.8181   0.7325   0.7432   - 401.8778\n",
      "\u001b[1;4mValidati 97     - 100    -            0.7498   0.0027   0.8041   0.8122   0.0000   0.0000   |            0.4277   0.7524   0.7646   0.0000   0.0000   - 13.4580 \u001b[0m\n",
      "Training 98     - 100    -            0.7497   0.0027   0.8112   0.8360   0.1762   0.1794   |            0.4297   0.8101   0.8154   0.7300   0.7401   - 401.7382\n",
      "\u001b[1;4mValidati 98     - 100    -            0.7499   0.0027   0.8072   0.8159   0.0000   0.0000   |            0.4303   0.7503   0.7591   0.0000   0.0000   - 12.9033 \u001b[0m\n",
      "Training 99     - 100    -            0.7500   0.0027   0.8071   0.8297   0.1731   0.1774   |            0.4324   0.8100   0.8132   0.7292   0.7399   - 399.8704\n",
      "\u001b[1;4mValidati 99     - 100    -            0.7502   0.0027   0.8066   0.8156   0.0000   0.0000   |            0.4330   0.7479   0.7580   0.0000   0.0000   - 13.0738 \u001b[0m\n",
      "Training 100    - 100    -            0.7502   0.0028   0.8093   0.8335   0.1685   0.1720   |            0.4351   0.8088   0.8125   0.7263   0.7376   - 404.4362\n",
      "\u001b[1;4mValidati 100    - 100    -            0.7503   0.0028   0.8021   0.8146   0.0000   0.0000   |            0.4357   0.7465   0.7549   0.0000   0.0000   - 14.1223 \u001b[0m\n",
      "Training 101    - 100    -            0.7502   0.0028   0.8091   0.8342   0.1662   0.1685   |            0.4379   0.8061   0.8119   0.7237   0.7346   - 402.0515\n",
      "\u001b[1;4mValidati 101    - 100    -            0.7504   0.0028   0.8068   0.8142   0.0000   0.0000   |            0.4385   0.7422   0.7539   0.0000   0.0000   - 13.3430 \u001b[0m\n",
      "Training 102    - 100    -            0.7504   0.0028   0.8116   0.8351   0.1625   0.1651   |            0.4408   0.8025   0.8075   0.7216   0.7335   - 399.6816\n",
      "\u001b[1;4mValidati 102    - 100    -            0.7506   0.0028   0.8109   0.8183   0.0000   0.0000   |            0.4413   0.7369   0.7480   0.0000   0.0000   - 13.0264 \u001b[0m\n",
      "Training 103    - 100    -            0.7506   0.0028   0.8117   0.8376   0.1606   0.1625   |            0.4438   0.7992   0.8043   0.7209   0.7327   - 400.1740\n",
      "\u001b[1;4mValidati 103    - 100    -            0.7508   0.0028   0.8011   0.8103   0.0000   0.0000   |            0.4443   0.7333   0.7432   0.0000   0.0000   - 13.3021 \u001b[0m\n",
      "Training 104    - 100    -            0.7510   0.0028   0.8064   0.8311   0.1590   0.1592   |            0.4468   0.7952   0.8001   0.7185   0.7301   - 399.9920\n",
      "\u001b[1;4mValidati 104    - 100    -            0.7512   0.0028   0.7973   0.8053   0.0000   0.0000   |            0.4474   0.7303   0.7385   0.0000   0.0000   - 13.0911 \u001b[0m\n",
      "Training 105    - 100    -            0.7511   0.0028   0.8170   0.8429   0.1562   0.1570   |            0.4498   0.7924   0.7960   0.7157   0.7280   - 402.2782\n",
      "\u001b[1;4mValidati 105    - 100    -            0.7512   0.0028   0.7948   0.8058   0.0000   0.0000   |            0.4504   0.7318   0.7397   0.0000   0.0000   - 13.2758 \u001b[0m\n",
      "Training 106    - 100    -            0.7511   0.0028   0.8164   0.8402   0.1546   0.1541   |            0.4527   0.7987   0.8001   0.7156   0.7266   - 401.3915\n",
      "\u001b[1;4mValidati 106    - 100    -            0.7513   0.0028   0.7990   0.8072   0.0000   0.0000   |            0.4533   0.7326   0.7411   0.0000   0.0000   - 13.3667 \u001b[0m\n",
      "Training 107    - 100    -            0.7513   0.0029   0.8101   0.8380   0.1525   0.1533   |            0.4556   0.7978   0.7993   0.7133   0.7246   - 403.0932\n",
      "\u001b[1;4mValidati 107    - 100    -            0.7516   0.0029   0.7988   0.8084   0.0000   0.0000   |            0.4561   0.7290   0.7388   0.0000   0.0000   - 13.4383 \u001b[0m\n",
      "Training 108    - 100    -            0.7514   0.0029   0.8161   0.8424   0.1479   0.1472   |            0.4585   0.7985   0.7976   0.7102   0.7218   - 403.2477\n",
      "\u001b[1;4mValidati 108    - 100    -            0.7516   0.0029   0.8049   0.8143   0.0000   0.0000   |            0.4591   0.7253   0.7364   0.0000   0.0000   - 13.2344 \u001b[0m\n",
      "Training 109    - 100    -            0.7514   0.0029   0.8159   0.8457   0.1468   0.1460   |            0.4617   0.7886   0.7930   0.7056   0.7175   - 404.4055\n",
      "\u001b[1;4mValidati 109    - 100    -            0.7516   0.0029   0.7990   0.8075   0.0000   0.0000   |            0.4623   0.7221   0.7330   0.0000   0.0000   - 13.4912 \u001b[0m\n",
      "Training 110    - 100    -            0.7513   0.0029   0.8200   0.8459   0.1457   0.1431   |            0.4648   0.7892   0.7973   0.7036   0.7155   - 402.9867\n",
      "\u001b[1;4mValidati 110    - 100    -            0.7515   0.0029   0.8017   0.8094   0.0000   0.0000   |            0.4654   0.7172   0.7294   0.0000   0.0000   - 13.4684 \u001b[0m\n",
      "Training 111    - 100    -            0.7513   0.0029   0.8186   0.8473   0.1417   0.1409   |            0.4681   0.7847   0.7885   0.7023   0.7147   - 404.5719\n",
      "\u001b[1;4mValidati 111    - 100    -            0.7515   0.0029   0.7986   0.8052   0.0000   0.0000   |            0.4687   0.7151   0.7244   0.0000   0.0000   - 13.4949 \u001b[0m\n",
      "Training 112    - 100    -            0.7514   0.0029   0.8150   0.8454   0.1395   0.1383   |            0.4714   0.7819   0.7857   0.7014   0.7140   - 405.5044\n",
      "\u001b[1;4mValidati 112    - 100    -            0.7517   0.0029   0.7867   0.7958   0.0000   0.0000   |            0.4720   0.7158   0.7254   0.0000   0.0000   - 13.5394 \u001b[0m\n",
      "Training 113    - 100    -            0.7516   0.0029   0.8127   0.8433   0.1371   0.1364   |            0.4746   0.7810   0.7851   0.7014   0.7142   - 405.8140\n",
      "\u001b[1;4mValidati 113    - 100    -            0.7519   0.0029   0.7999   0.8065   0.0000   0.0000   |            0.4752   0.7140   0.7234   0.0000   0.0000   - 13.4936 \u001b[0m\n",
      "Training 114    - 100    -            0.7519   0.0029   0.8126   0.8387   0.1358   0.1335   |            0.4779   0.7746   0.7778   0.6976   0.7108   - 409.4885\n",
      "\u001b[1;4mValidati 114    - 100    -            0.7521   0.0029   0.8043   0.8128   0.0000   0.0000   |            0.4784   0.7204   0.7287   0.0000   0.0000   - 13.7021 \u001b[0m\n",
      "Training 115    - 100    -            0.7520   0.0030   0.8170   0.8439   0.1345   0.1325   |            0.4809   0.7782   0.7830   0.6996   0.7120   - 405.0153\n",
      "\u001b[1;4mValidati 115    - 100    -            0.7522   0.0030   0.7922   0.8016   0.0000   0.0000   |            0.4815   0.7154   0.7260   0.0000   0.0000   - 13.5751 \u001b[0m\n",
      "Training 116    - 100    -            0.7521   0.0030   0.8118   0.8395   0.1328   0.1301   |            0.4842   0.7740   0.7806   0.6978   0.7101   - 406.7456\n",
      "\u001b[1;4mValidati 116    - 100    -            0.7524   0.0030   0.7856   0.7952   0.0000   0.0000   |            0.4847   0.7127   0.7246   0.0000   0.0000   - 13.8548 \u001b[0m\n",
      "Training 117    - 100    -            0.7521   0.0030   0.8186   0.8471   0.1289   0.1251   |            0.4873   0.7774   0.7828   0.6980   0.7109   - 408.8254\n",
      "\u001b[1;4mValidati 117    - 100    -            0.7524   0.0030   0.7885   0.7967   0.0000   0.0000   |            0.4879   0.7147   0.7245   0.0000   0.0000   - 13.7288 \u001b[0m\n",
      "Training 118    - 100    -            0.7524   0.0030   0.8154   0.8457   0.1261   0.1233   |            0.4903   0.7769   0.7831   0.6991   0.7111   - 404.0062\n",
      "\u001b[1;4mValidati 118    - 100    -            0.7526   0.0030   0.7902   0.8001   0.0000   0.0000   |            0.4908   0.7165   0.7268   0.0000   0.0000   - 13.9954 \u001b[0m\n",
      "Training 119    - 100    -            0.7528   0.0030   0.8058   0.8396   0.1246   0.1209   |            0.4932   0.7790   0.7817   0.6998   0.7109   - 408.4822\n",
      "\u001b[1;4mValidati 119    - 100    -            0.7530   0.0030   0.7889   0.7990   0.0000   0.0000   |            0.4937   0.7186   0.7273   0.0000   0.0000   - 13.7234 \u001b[0m\n",
      "Training 120    - 100    -            0.7530   0.0030   0.8133   0.8451   0.1241   0.1209   |            0.4961   0.7770   0.7797   0.6974   0.7074   - 410.2776\n",
      "\u001b[1;4mValidati 120    - 100    -            0.7532   0.0030   0.7919   0.8008   0.0000   0.0000   |            0.4966   0.7168   0.7249   0.0000   0.0000   - 13.7404 \u001b[0m\n",
      "Training 121    - 100    -            0.7530   0.0030   0.8198   0.8506   0.1229   0.1201   |            0.4991   0.7739   0.7773   0.6947   0.7052   - 407.8553\n",
      "\u001b[1;4mValidati 121    - 100    -            0.7532   0.0030   0.7799   0.7902   0.0000   0.0000   |            0.4996   0.7056   0.7155   0.0000   0.0000   - 14.1140 \u001b[0m\n",
      "Training 122    - 100    -            0.7532   0.0030   0.8164   0.8474   0.1209   0.1166   |            0.5022   0.7659   0.7694   0.6852   0.6959   - 410.3198\n",
      "\u001b[1;4mValidati 122    - 100    -            0.7534   0.0030   0.7881   0.7946   0.0000   0.0000   |            0.5028   0.6937   0.7037   0.0000   0.0000   - 13.7647 \u001b[0m\n",
      "Training 123    - 100    -            0.7533   0.0031   0.8110   0.8451   0.1186   0.1155   |            0.5052   0.7631   0.7685   0.6847   0.6961   - 410.2668\n",
      "\u001b[1;4mValidati 123    - 100    -            0.7536   0.0030   0.7831   0.7919   0.0000   0.0000   |            0.5058   0.6941   0.7023   0.0000   0.0000   - 13.9239 \u001b[0m\n",
      "Training 124    - 100    -            0.7535   0.0031   0.8159   0.8485   0.1167   0.1133   |            0.5084   0.7672   0.7708   0.6843   0.6944   - 407.7225\n",
      "\u001b[1;4mValidati 124    - 100    -            0.7538   0.0031   0.7764   0.7866   0.0000   0.0000   |            0.5090   0.7019   0.7062   0.0000   0.0000   - 13.8092 \u001b[0m\n",
      "Training 125    - 100    -            0.7538   0.0031   0.8100   0.8449   0.1160   0.1126   |            0.5115   0.7638   0.7679   0.6843   0.6953   - 410.8980\n",
      "\u001b[1;4mValidati 125    - 100    -            0.7541   0.0031   0.7716   0.7787   0.0000   0.0000   |            0.5121   0.7022   0.7095   0.0000   0.0000   - 14.0125 \u001b[0m\n",
      "Training 126    - 100    -            0.7540   0.0031   0.8166   0.8497   0.1133   0.1083   |            0.5146   0.7655   0.7709   0.6824   0.6936   - 411.1913\n",
      "\u001b[1;4mValidati 126    - 100    -            0.7543   0.0031   0.7783   0.7873   0.0000   0.0000   |            0.5152   0.6948   0.7042   0.0000   0.0000   - 14.0383 \u001b[0m\n",
      "Training 127    - 100    -            0.7540   0.0031   0.8216   0.8538   0.1120   0.1070   |            0.5177   0.7629   0.7679   0.6807   0.6923   - 408.6728\n",
      "\u001b[1;4mValidati 127    - 100    -            0.7543   0.0031   0.7798   0.7869   0.0000   0.0000   |            0.5183   0.6966   0.7054   0.0000   0.0000   - 13.9563 \u001b[0m\n",
      "Training 128    - 100    -            0.7541   0.0031   0.8157   0.8480   0.1121   0.1078   |            0.5209   0.7592   0.7643   0.6830   0.6938   - 410.4069\n",
      "\u001b[1;4mValidati 128    - 100    -            0.7544   0.0031   0.7798   0.7873   0.0000   0.0000   |            0.5214   0.7004   0.7099   0.0000   0.0000   - 14.0263 \u001b[0m\n",
      "Training 129    - 100    -            0.7544   0.0031   0.8129   0.8459   0.1089   0.1034   |            0.5239   0.7616   0.7651   0.6822   0.6939   - 412.6375\n",
      "\u001b[1;4mValidati 129    - 100    -            0.7547   0.0031   0.7686   0.7771   0.0000   0.0000   |            0.5245   0.6965   0.7068   0.0000   0.0000   - 13.8783 \u001b[0m\n",
      "Training 130    - 100    -            0.7546   0.0031   0.8141   0.8491   0.1082   0.1016   |            0.5270   0.7566   0.7581   0.6816   0.6927   - 413.4239\n",
      "\u001b[1;4mValidati 130    - 100    -            0.7550   0.0031   0.7619   0.7696   0.0000   0.0000   |            0.5276   0.6936   0.7044   0.0000   0.0000   - 13.6034 \u001b[0m\n",
      "Training 131    - 100    -            0.7548   0.0031   0.8176   0.8501   0.1063   0.1008   |            0.5301   0.7567   0.7617   0.6795   0.6904   - 405.4366\n",
      "\u001b[1;4mValidati 131    - 100    -            0.7552   0.0031   0.7762   0.7833   0.0000   0.0000   |            0.5307   0.6969   0.7068   0.0000   0.0000   - 13.5830 \u001b[0m\n",
      "Training 132    - 100    -            0.7552   0.0031   0.8120   0.8456   0.1055   0.0996   |            0.5332   0.7602   0.7649   0.6817   0.6929   - 408.7754\n",
      "\u001b[1;4mValidati 132    - 100    -            0.7556   0.0031   0.7691   0.7777   0.0000   0.0000   |            0.5337   0.6946   0.7026   0.0000   0.0000   - 13.5646 \u001b[0m\n",
      "Training 133    - 100    -            0.7554   0.0032   0.8143   0.8516   0.1034   0.0972   |            0.5362   0.7557   0.7592   0.6749   0.6860   - 411.2958\n",
      "\u001b[1;4mValidati 133    - 100    -            0.7557   0.0032   0.7724   0.7820   0.0000   0.0000   |            0.5368   0.6878   0.6968   0.0000   0.0000   - 13.7873 \u001b[0m\n",
      "Training 134    - 100    -            0.7556   0.0032   0.8148   0.8457   0.1017   0.0975   |            0.5393   0.7550   0.7584   0.6714   0.6824   - 406.4567\n",
      "\u001b[1;4mValidati 134    - 100    -            0.7559   0.0032   0.7817   0.7914   0.0000   0.0000   |            0.5398   0.6850   0.6911   0.0000   0.0000   - 13.8339 \u001b[0m\n",
      "Training 135    - 100    -            0.7559   0.0032   0.8116   0.8451   0.1017   0.0940   |            0.5425   0.7476   0.7537   0.6702   0.6819   - 410.7826\n",
      "\u001b[1;4mValidati 135    - 100    -            0.7561   0.0032   0.7757   0.7849   0.0000   0.0000   |            0.5431   0.6875   0.6937   0.0000   0.0000   - 13.6440 \u001b[0m\n",
      "Training 136    - 100    -            0.7561   0.0032   0.8118   0.8478   0.1006   0.0944   |            0.5457   0.7472   0.7517   0.6698   0.6812   - 408.8688\n",
      "\u001b[1;4mValidati 136    - 100    -            0.7564   0.0032   0.7783   0.7869   0.0000   0.0000   |            0.5463   0.6838   0.6941   0.0000   0.0000   - 14.1282 \u001b[0m\n",
      "Training 137    - 100    -            0.7560   0.0032   0.8211   0.8542   0.0999   0.0933   |            0.5487   0.7526   0.7574   0.6730   0.6840   - 413.7601\n",
      "\u001b[1;4mValidati 137    - 100    -            0.7563   0.0032   0.7723   0.7827   0.0000   0.0000   |            0.5493   0.6857   0.6950   0.0000   0.0000   - 14.2100 \u001b[0m\n",
      "Training 138    - 100    -            0.7561   0.0032   0.8232   0.8572   0.0971   0.0900   |            0.5518   0.7433   0.7485   0.6700   0.6809   - 413.4649\n",
      "\u001b[1;4mValidati 138    - 100    -            0.7563   0.0032   0.7744   0.7837   0.0000   0.0000   |            0.5523   0.6803   0.6915   0.0000   0.0000   - 13.9802 \u001b[0m\n",
      "Training 139    - 100    -            0.7561   0.0032   0.8234   0.8582   0.0975   0.0901   |            0.5549   0.7447   0.7490   0.6688   0.6799   - 414.2179\n",
      "\u001b[1;4mValidati 139    - 100    -            0.7563   0.0032   0.7804   0.7887   0.0000   0.0000   |            0.5554   0.6866   0.6952   0.0000   0.0000   - 14.8489 \u001b[0m\n",
      "Training 140    - 100    -            0.7561   0.0032   0.8167   0.8542   0.0964   0.0898   |            0.5579   0.7475   0.7533   0.6693   0.6804   - 415.5937\n",
      "\u001b[1;4mValidati 140    - 100    -            0.7564   0.0032   0.7675   0.7760   0.0000   0.0000   |            0.5584   0.6836   0.6935   0.0000   0.0000   - 14.2961 \u001b[0m\n",
      "Training 141    - 100    -            0.7565   0.0032   0.8077   0.8444   0.0946   0.0877   |            0.5610   0.7440   0.7495   0.6711   0.6825   - 417.6950\n",
      "\u001b[1;4mValidati 141    - 100    -            0.7568   0.0032   0.7745   0.7815   0.0000   0.0000   |            0.5615   0.6856   0.6950   0.0000   0.0000   - 14.1388 \u001b[0m\n",
      "Training 142    - 100    -            0.7569   0.0032   0.8071   0.8449   0.0938   0.0859   |            0.5640   0.7486   0.7531   0.6681   0.6791   - 415.6000\n",
      "\u001b[1;4mValidati 142    - 100    -            0.7572   0.0032   0.7706   0.7781   0.0000   0.0000   |            0.5645   0.6813   0.6898   0.0000   0.0000   - 14.3916 \u001b[0m\n",
      "Training 143    - 100    -            0.7571   0.0032   0.8153   0.8484   0.0921   0.0849   |            0.5671   0.7409   0.7470   0.6608   0.6729   - 414.0409\n",
      "\u001b[1;4mValidati 143    - 100    -            0.7574   0.0032   0.7692   0.7768   0.0000   0.0000   |            0.5676   0.6707   0.6796   0.0000   0.0000   - 14.1392 \u001b[0m\n",
      "Training 144    - 100    -            0.7574   0.0033   0.8095   0.8467   0.0917   0.0845   |            0.5702   0.7402   0.7418   0.6584   0.6696   - 414.7187\n",
      "\u001b[1;4mValidati 144    - 100    -            0.7577   0.0033   0.7754   0.7822   0.0000   0.0000   |            0.5708   0.6745   0.6807   0.0000   0.0000   - 14.3480 \u001b[0m\n",
      "Training 145    - 100    -            0.7576   0.0033   0.8156   0.8515   0.0898   0.0829   |            0.5734   0.7378   0.7440   0.6602   0.6714   - 413.4911\n",
      "\u001b[1;4mValidati 145    - 100    -            0.7579   0.0033   0.7708   0.7782   0.0000   0.0000   |            0.5739   0.6717   0.6826   0.0000   0.0000   - 14.3368 \u001b[0m\n",
      "Training 146    - 100    -            0.7578   0.0033   0.8192   0.8519   0.0888   0.0814   |            0.5765   0.7367   0.7463   0.6581   0.6695   - 420.9081\n",
      "\u001b[1;4mValidati 146    - 100    -            0.7581   0.0033   0.7685   0.7756   0.0000   0.0000   |            0.5770   0.6722   0.6808   0.0000   0.0000   - 14.4802 \u001b[0m\n",
      "Training 147    - 100    -            0.7580   0.0033   0.8082   0.8500   0.0881   0.0802   |            0.5796   0.7400   0.7431   0.6594   0.6714   - 412.3765\n",
      "\u001b[1;4mValidati 147    - 100    -            0.7583   0.0033   0.7612   0.7704   0.0000   0.0000   |            0.5802   0.6713   0.6800   0.0000   0.0000   - 14.4204 \u001b[0m\n",
      "Training 148    - 100    -            0.7583   0.0033   0.8130   0.8513   0.0857   0.0769   |            0.5829   0.7287   0.7356   0.6554   0.6675   - 417.5655\n",
      "\u001b[1;4mValidati 148    - 100    -            0.7586   0.0033   0.7623   0.7686   0.0000   0.0000   |            0.5834   0.6707   0.6801   0.0000   0.0000   - 14.4271 \u001b[0m\n",
      "Training 149    - 100    -            0.7585   0.0033   0.8134   0.8505   0.0859   0.0769   |            0.5859   0.7351   0.7394   0.6567   0.6669   - 415.3343\n",
      "\u001b[1;4mValidati 149    - 100    -            0.7587   0.0033   0.7739   0.7796   0.0000   0.0000   |            0.5864   0.6735   0.6808   0.0000   0.0000   - 14.4134 \u001b[0m\n",
      "Training 150    - 100    -            0.7587   0.0033   0.8118   0.8479   0.0839   0.0772   |            0.5889   0.7324   0.7392   0.6567   0.6678   - 411.0723\n",
      "\u001b[1;4mValidati 150    - 100    -            0.7590   0.0033   0.7645   0.7731   0.0000   0.0000   |            0.5894   0.6722   0.6816   0.0000   0.0000   - 14.6529 \u001b[0m\n",
      "Training 151    - 100    -            0.7590   0.0033   0.8131   0.8495   0.0840   0.0750   |            0.5918   0.7390   0.7437   0.6562   0.6672   - 413.4240\n",
      "\u001b[1;4mValidati 151    - 100    -            0.7593   0.0033   0.7622   0.7695   0.0000   0.0000   |            0.5923   0.6704   0.6804   0.0000   0.0000   - 13.8215 \u001b[0m\n",
      "Training 152    - 100    -            0.7593   0.0033   0.8139   0.8482   0.0818   0.0735   |            0.5947   0.7337   0.7403   0.6576   0.6689   - 413.7448\n",
      "\u001b[1;4mValidati 152    - 100    -            0.7596   0.0033   0.7590   0.7673   0.0000   0.0000   |            0.5952   0.6716   0.6789   0.0000   0.0000   - 13.9406 \u001b[0m\n",
      "Training 153    - 100    -            0.7595   0.0033   0.8129   0.8523   0.0822   0.0736   |            0.5976   0.7341   0.7410   0.6568   0.6682   - 410.1350\n",
      "\u001b[1;4mValidati 153    - 100    -            0.7598   0.0033   0.7605   0.7667   0.0000   0.0000   |            0.5981   0.6720   0.6811   0.0000   0.0000   - 14.1971 \u001b[0m\n",
      "Training 154    - 100    -            0.7597   0.0033   0.8172   0.8534   0.0808   0.0722   |            0.6006   0.7280   0.7321   0.6538   0.6656   - 410.7507\n",
      "\u001b[1;4mValidati 154    - 100    -            0.7601   0.0033   0.7527   0.7610   0.0000   0.0000   |            0.6011   0.6696   0.6750   0.0000   0.0000   - 15.0834 \u001b[0m\n",
      "Training 155    - 100    -            0.7601   0.0033   0.8089   0.8452   0.0815   0.0728   |            0.6035   0.7323   0.7378   0.6534   0.6635   - 413.3909\n",
      "\u001b[1;4mValidati 155    - 100    -            0.7605   0.0033   0.7475   0.7552   0.0000   0.0000   |            0.6040   0.6658   0.6744   0.0000   0.0000   - 14.1185 \u001b[0m\n",
      "Training 156    - 100    -            0.7605   0.0034   0.8114   0.8484   0.0801   0.0718   |            0.6065   0.7248   0.7290   0.6512   0.6614   - 410.3462\n",
      "\u001b[1;4mValidati 156    - 100    -            0.7609   0.0034   0.7499   0.7559   0.0000   0.0000   |            0.6070   0.6629   0.6736   0.0000   0.0000   - 14.0500 \u001b[0m\n",
      "Training 157    - 100    -            0.7609   0.0034   0.8111   0.8473   0.0784   0.0698   |            0.6094   0.7290   0.7321   0.6495   0.6588   - 414.2090\n",
      "\u001b[1;4mValidati 157    - 100    -            0.7612   0.0034   0.7513   0.7582   0.0000   0.0000   |            0.6099   0.6640   0.6754   0.0000   0.0000   - 14.7734 \u001b[0m\n",
      "Training 158    - 100    -            0.7611   0.0034   0.8150   0.8532   0.0786   0.0704   |            0.6123   0.7251   0.7328   0.6488   0.6591   - 420.3134\n",
      "\u001b[1;4mValidati 158    - 100    -            0.7615   0.0034   0.7509   0.7588   0.0000   0.0000   |            0.6128   0.6622   0.6722   0.0000   0.0000   - 14.7781 \u001b[0m\n",
      "Training 159    - 100    -            0.7616   0.0034   0.8015   0.8456   0.0767   0.0684   |            0.6152   0.7277   0.7319   0.6477   0.6583   - 420.5799\n",
      "\u001b[1;4mValidati 159    - 100    -            0.7620   0.0034   0.7504   0.7586   0.0000   0.0000   |            0.6157   0.6613   0.6725   0.0000   0.0000   - 15.1420 \u001b[0m\n",
      "Training 160    - 100    -            0.7623   0.0034   0.7949   0.8347   0.0773   0.0697   |            0.6181   0.7246   0.7277   0.6458   0.6561   - 420.6345\n",
      "\u001b[1;4mValidati 160    - 100    -            0.7627   0.0034   0.7440   0.7510   0.0000   0.0000   |            0.6186   0.6618   0.6700   0.0000   0.0000   - 14.5562 \u001b[0m\n",
      "Training 161    - 100    -            0.7628   0.0034   0.8041   0.8442   0.0754   0.0666   |            0.6210   0.7261   0.7300   0.6469   0.6577   - 420.3193\n",
      "\u001b[1;4mValidati 161    - 100    -            0.7632   0.0034   0.7462   0.7518   0.0000   0.0000   |            0.6215   0.6623   0.6703   0.0000   0.0000   - 15.5258 \u001b[0m\n",
      "Training 162    - 100    -            0.7632   0.0034   0.8120   0.8477   0.0762   0.0677   |            0.6238   0.7265   0.7336   0.6485   0.6590   - 424.6638\n",
      "\u001b[1;4mValidati 162    - 100    -            0.7636   0.0034   0.7363   0.7433   0.0000   0.0000   |            0.6243   0.6621   0.6723   0.0000   0.0000   - 14.8515 \u001b[0m\n",
      "Training 163    - 100    -            0.7637   0.0034   0.8054   0.8416   0.0740   0.0656   |            0.6266   0.7295   0.7325   0.6483   0.6576   - 420.8217\n",
      "\u001b[1;4mValidati 163    - 100    -            0.7641   0.0034   0.7453   0.7528   0.0000   0.0000   |            0.6271   0.6618   0.6691   0.0000   0.0000   - 15.0333 \u001b[0m\n",
      "Training 164    - 100    -            0.7643   0.0034   0.7959   0.8382   0.0741   0.0659   |            0.6294   0.7296   0.7349   0.6457   0.6556   - 417.4573\n",
      "\u001b[1;4mValidati 164    - 100    -            0.7647   0.0034   0.7400   0.7464   0.0000   0.0000   |            0.6299   0.6633   0.6700   0.0000   0.0000   - 15.5228 \u001b[0m\n",
      "Training 165    - 100    -            0.7648   0.0034   0.8045   0.8422   0.0733   0.0646   |            0.6322   0.7233   0.7291   0.6440   0.6545   - 420.0444\n",
      "\u001b[1;4mValidati 165    - 100    -            0.7652   0.0034   0.7454   0.7519   0.0000   0.0000   |            0.6327   0.6583   0.6675   0.0000   0.0000   - 15.9385 \u001b[0m\n",
      "Training 166    - 100    -            0.7653   0.0034   0.8054   0.8444   0.0727   0.0636   |            0.6350   0.7199   0.7246   0.6436   0.6531   - 423.2923\n",
      "\u001b[1;4mValidati 166    - 100    -            0.7656   0.0034   0.7382   0.7452   0.0000   0.0000   |            0.6355   0.6549   0.6683   0.0000   0.0000   - 15.0019 \u001b[0m\n",
      "Training 167    - 100    -            0.7657   0.0034   0.8058   0.8440   0.0723   0.0637   |            0.6379   0.7225   0.7251   0.6393   0.6509   - 419.1510\n",
      "\u001b[1;4mValidati 167    - 100    -            0.7661   0.0034   0.7400   0.7464   0.0000   0.0000   |            0.6384   0.6575   0.6660   0.0000   0.0000   - 14.6233 \u001b[0m\n",
      "Training 168    - 100    -            0.7663   0.0034   0.7996   0.8384   0.0712   0.0638   |            0.6407   0.7223   0.7279   0.6389   0.6506   - 417.9238\n",
      "\u001b[1;4mValidati 168    - 100    -            0.7667   0.0034   0.7394   0.7480   0.0000   0.0000   |            0.6412   0.6573   0.6647   0.0000   0.0000   - 14.8293 \u001b[0m\n",
      "Training 169    - 100    -            0.7669   0.0034   0.7981   0.8387   0.0714   0.0619   |            0.6435   0.7218   0.7243   0.6413   0.6520   - 418.0865\n",
      "\u001b[1;4mValidati 169    - 100    -            0.7673   0.0034   0.7422   0.7492   0.0000   0.0000   |            0.6440   0.6562   0.6661   0.0000   0.0000   - 14.9014 \u001b[0m\n",
      "Training 170    - 100    -            0.7674   0.0035   0.8000   0.8415   0.0710   0.0620   |            0.6463   0.7187   0.7221   0.6416   0.6515   - 419.9388\n",
      "\u001b[1;4mValidati 170    - 100    -            0.7678   0.0035   0.7412   0.7496   0.0000   0.0000   |            0.6468   0.6562   0.6646   0.0000   0.0000   - 14.9656 \u001b[0m\n",
      "Training 171    - 100    -            0.7682   0.0035   0.7893   0.8275   0.0700   0.0609   |            0.6490   0.7197   0.7265   0.6408   0.6512   - 422.8507\n",
      "\u001b[1;4mValidati 171    - 100    -            0.7686   0.0035   0.7324   0.7406   0.0000   0.0000   |            0.6495   0.6517   0.6610   0.0000   0.0000   - 15.3430 \u001b[0m\n",
      "Training 172    - 100    -            0.7691   0.0035   0.7849   0.8293   0.0703   0.0606   |            0.6518   0.7139   0.7236   0.6387   0.6475   - 419.4723\n",
      "\u001b[1;4mValidati 172    - 100    -            0.7695   0.0035   0.7367   0.7447   0.0000   0.0000   |            0.6522   0.6523   0.6630   0.0000   0.0000   - 14.1938 \u001b[0m\n",
      "Training 173    - 100    -            0.7699   0.0035   0.7905   0.8257   0.0687   0.0595   |            0.6546   0.7146   0.7161   0.6361   0.6461   - 415.3351\n",
      "\u001b[1;4mValidati 173    - 100    -            0.7703   0.0035   0.7299   0.7390   0.0000   0.0000   |            0.6551   0.6524   0.6620   0.0000   0.0000   - 14.5975 \u001b[0m\n",
      "Training 174    - 100    -            0.7709   0.0035   0.7799   0.8210   0.0682   0.0580   |            0.6574   0.7138   0.7188   0.6348   0.6459   - 413.3723\n",
      "\u001b[1;4mValidati 174    - 100    -            0.7712   0.0035   0.7370   0.7442   0.0000   0.0000   |            0.6579   0.6510   0.6598   0.0000   0.0000   - 14.6033 \u001b[0m\n",
      "Training 175    - 100    -            0.7719   0.0035   0.7792   0.8185   0.0683   0.0588   |            0.6601   0.7143   0.7171   0.6366   0.6464   - 416.3658\n",
      "\u001b[1;4mValidati 175    - 100    -            0.7722   0.0035   0.7349   0.7430   0.0000   0.0000   |            0.6606   0.6522   0.6593   0.0000   0.0000   - 14.4416 \u001b[0m\n",
      "Training 176    - 100    -            0.7728   0.0035   0.7814   0.8230   0.0687   0.0589   |            0.6628   0.7155   0.7182   0.6343   0.6445   - 411.4387\n",
      "\u001b[1;4mValidati 176    - 100    -            0.7731   0.0035   0.7346   0.7411   0.0000   0.0000   |            0.6633   0.6484   0.6564   0.0000   0.0000   - 14.6730 \u001b[0m\n",
      "Training 177    - 100    -            0.7738   0.0035   0.7777   0.8189   0.0665   0.0572   |            0.6654   0.7112   0.7184   0.6336   0.6441   - 416.0152\n",
      "\u001b[1;4mValidati 177    - 100    -            0.7741   0.0035   0.7377   0.7465   0.0000   0.0000   |            0.6659   0.6474   0.6558   0.0000   0.0000   - 14.5569 \u001b[0m\n",
      "Training 178    - 100    -            0.7748   0.0035   0.7731   0.8140   0.0653   0.0557   |            0.6682   0.7135   0.7173   0.6324   0.6432   - 417.6861\n",
      "\u001b[1;4mValidati 178    - 100    -            0.7751   0.0035   0.7385   0.7471   0.0000   0.0000   |            0.6687   0.6477   0.6564   0.0000   0.0000   - 14.6859 \u001b[0m\n",
      "Training 179    - 100    -            0.7758   0.0035   0.7782   0.8160   0.0657   0.0565   |            0.6708   0.7126   0.7180   0.6328   0.6436   - 416.2864\n",
      "\u001b[1;4mValidati 179    - 100    -            0.7761   0.0035   0.7341   0.7443   0.0000   0.0000   |            0.6713   0.6461   0.6553   0.0000   0.0000   - 14.7873 \u001b[0m\n",
      "Training 180    - 100    -            0.7768   0.0035   0.7727   0.8100   0.0653   0.0547   |            0.6734   0.7113   0.7173   0.6322   0.6430   - 418.7010\n",
      "\u001b[1;4mValidati 180    - 100    -            0.7772   0.0035   0.7333   0.7420   0.0000   0.0000   |            0.6739   0.6434   0.6517   0.0000   0.0000   - 14.6666 \u001b[0m\n",
      "Training 181    - 100    -            0.7780   0.0035   0.7661   0.8042   0.0649   0.0546   |            0.6760   0.7149   0.7226   0.6310   0.6415   - 418.1980\n",
      "\u001b[1;4mValidati 181    - 100    -            0.7783   0.0035   0.7314   0.7394   0.0000   0.0000   |            0.6764   0.6471   0.6562   0.0000   0.0000   - 14.9496 \u001b[0m\n",
      "Training 182    - 100    -            0.7791   0.0035   0.7669   0.8012   0.0646   0.0539   |            0.6785   0.7102   0.7135   0.6310   0.6424   - 414.2099\n",
      "\u001b[1;4mValidati 182    - 100    -            0.7795   0.0035   0.7309   0.7373   0.0000   0.0000   |            0.6790   0.6461   0.6561   0.0000   0.0000   - 14.9376 \u001b[0m\n",
      "Training 183    - 100    -            0.7804   0.0035   0.7596   0.7990   0.0637   0.0534   |            0.6811   0.7122   0.7154   0.6315   0.6417   - 416.3201\n",
      "\u001b[1;4mValidati 183    - 100    -            0.7808   0.0035   0.7292   0.7372   0.0000   0.0000   |            0.6815   0.6437   0.6541   0.0000   0.0000   - 15.0848 \u001b[0m\n",
      "Training 184    - 100    -            0.7819   0.0035   0.7519   0.7926   0.0644   0.0539   |            0.6836   0.7123   0.7177   0.6315   0.6429   - 415.6766\n",
      "\u001b[1;4mValidati 184    - 100    -            0.7822   0.0035   0.7346   0.7435   0.0000   0.0000   |            0.6840   0.6449   0.6517   0.0000   0.0000   - 14.6659 \u001b[0m\n",
      "Training 185    - 100    -            0.7833   0.0035   0.7501   0.7909   0.0629   0.0524   |            0.6861   0.7100   0.7165   0.6305   0.6414   - 421.0283\n",
      "\u001b[1;4mValidati 185    - 100    -            0.7836   0.0035   0.7329   0.7410   0.0000   0.0000   |            0.6865   0.6438   0.6545   0.0000   0.0000   - 14.5979 \u001b[0m\n",
      "Training 186    - 100    -            0.7847   0.0035   0.7542   0.7933   0.0623   0.0523   |            0.6885   0.7083   0.7142   0.6290   0.6408   - 416.8828\n",
      "\u001b[1;4mValidati 186    - 100    -            0.7851   0.0035   0.7281   0.7358   0.0000   0.0000   |            0.6890   0.6442   0.6551   0.0000   0.0000   - 14.9181 \u001b[0m\n",
      "Training 187    - 100    -            0.7862   0.0036   0.7430   0.7840   0.0622   0.0517   |            0.6909   0.7103   0.7130   0.6299   0.6413   - 418.2879\n",
      "\u001b[1;4mValidati 187    - 100    -            0.7866   0.0036   0.7346   0.7420   0.0000   0.0000   |            0.6913   0.6441   0.6542   0.0000   0.0000   - 14.6218 \u001b[0m\n",
      "Training 188    - 100    -            0.7878   0.0036   0.7411   0.7799   0.0623   0.0517   |            0.6933   0.7122   0.7184   0.6290   0.6408   - 417.7895\n",
      "\u001b[1;4mValidati 188    - 100    -            0.7882   0.0036   0.7300   0.7369   0.0000   0.0000   |            0.6937   0.6429   0.6531   0.0000   0.0000   - 14.7568 \u001b[0m\n",
      "Training 189    - 100    -            0.7894   0.0036   0.7440   0.7825   0.0619   0.0508   |            0.6957   0.7085   0.7125   0.6293   0.6396   - 426.9212\n",
      "\u001b[1;4mValidati 189    - 100    -            0.7898   0.0036   0.7333   0.7401   0.0000   0.0000   |            0.6961   0.6423   0.6516   0.0000   0.0000   - 15.1044 \u001b[0m\n",
      "Training 190    - 100    -            0.7910   0.0036   0.7445   0.7798   0.0606   0.0501   |            0.6980   0.7118   0.7143   0.6280   0.6387   - 417.7027\n",
      "\u001b[1;4mValidati 190    - 100    -            0.7914   0.0036   0.7311   0.7404   0.0000   0.0000   |            0.6985   0.6429   0.6519   0.0000   0.0000   - 14.9117 \u001b[0m\n",
      "Training 191    - 100    -            0.7928   0.0036   0.7330   0.7728   0.0601   0.0495   |            0.7004   0.7028   0.7078   0.6282   0.6393   - 419.4041\n",
      "\u001b[1;4mValidati 191    - 100    -            0.7932   0.0036   0.7308   0.7374   0.0000   0.0000   |            0.7009   0.6394   0.6508   0.0000   0.0000   - 16.6784 \u001b[0m\n",
      "Training 192    - 100    -            0.7947   0.0036   0.7277   0.7680   0.0604   0.0499   |            0.7028   0.7052   0.7093   0.6277   0.6391   - 421.4568\n",
      "\u001b[1;4mValidati 192    - 100    -            0.7950   0.0036   0.7296   0.7376   0.0000   0.0000   |            0.7033   0.6442   0.6522   0.0000   0.0000   - 15.0349 \u001b[0m\n",
      "Training 193    - 100    -            0.7965   0.0036   0.7275   0.7677   0.0604   0.0488   |            0.7052   0.7041   0.7083   0.6275   0.6400   - 417.3375\n",
      "\u001b[1;4mValidati 193    - 100    -            0.7968   0.0036   0.7284   0.7378   0.0000   0.0000   |            0.7056   0.6422   0.6499   0.0000   0.0000   - 15.0444 \u001b[0m\n",
      "Training 194    - 100    -            0.7984   0.0036   0.7218   0.7645   0.0596   0.0491   |            0.7075   0.7048   0.7096   0.6292   0.6410   - 425.6195\n",
      "\u001b[1;4mValidati 194    - 100    -            0.7988   0.0036   0.7301   0.7379   0.0000   0.0000   |            0.7079   0.6420   0.6515   0.0000   0.0000   - 15.7289 \u001b[0m\n",
      "Training 195    - 100    -            0.8004   0.0036   0.7165   0.7572   0.0594   0.0487   |            0.7097   0.7092   0.7156   0.6288   0.6395   - 422.6833\n",
      "\u001b[1;4mValidati 195    - 100    -            0.8008   0.0036   0.7300   0.7393   0.0000   0.0000   |            0.7101   0.6415   0.6506   0.0000   0.0000   - 15.1883 \u001b[0m\n",
      "Training 196    - 100    -            0.8025   0.0036   0.7126   0.7464   0.0592   0.0480   |            0.7119   0.7046   0.7140   0.6283   0.6394   - 423.0932\n",
      "\u001b[1;4mValidati 196    - 100    -            0.8029   0.0036   0.7295   0.7375   0.0000   0.0000   |            0.7123   0.6420   0.6498   0.0000   0.0000   - 15.1531 \u001b[0m\n",
      "Training 197    - 100    -            0.8049   0.0036   0.6978   0.7392   0.0597   0.0487   |            0.7140   0.7125   0.7135   0.6288   0.6395   - 420.1241\n",
      "\u001b[1;4mValidati 197    - 100    -            0.8053   0.0036   0.7264   0.7350   0.0000   0.0000   |            0.7144   0.6443   0.6515   0.0000   0.0000   - 15.0725 \u001b[0m\n",
      "Training 198    - 100    -            0.8073   0.0036   0.6982   0.7383   0.0586   0.0470   |            0.7162   0.7082   0.7152   0.6292   0.6403   - 421.5537\n",
      "\u001b[1;4mValidati 198    - 100    -            0.8076   0.0036   0.7310   0.7399   0.0000   0.0000   |            0.7166   0.6427   0.6523   0.0000   0.0000   - 15.0957 \u001b[0m\n",
      "Training 199    - 100    -            0.8096   0.0036   0.6991   0.7382   0.0584   0.0471   |            0.7183   0.7089   0.7118   0.6289   0.6392   - 419.0653\n",
      "\u001b[1;4mValidati 199    - 100    -            0.8100   0.0036   0.7283   0.7344   0.0000   0.0000   |            0.7187   0.6427   0.6510   0.0000   0.0000   - 15.2276 \u001b[0m\n",
      "Training 200    - 100    -            0.8123   0.0036   0.6875   0.7275   0.0579   0.0471   |            0.7203   0.7152   0.7164   0.6290   0.6392   - 425.3868\n",
      "\u001b[1;4mValidati 200    - 100    -            0.8126   0.0036   0.7300   0.7369   0.0000   0.0000   |            0.7207   0.6400   0.6491   0.0000   0.0000   - 15.3136 \u001b[0m\r"
     ]
    }
   ],
   "source": [
    "print(header)\n",
    "\n",
    "start_epoch = checkpoint.epoch_counter\n",
    "end_epoch = args.nb_epoch\n",
    "\n",
    "for e in range(start_epoch, args.nb_epoch):\n",
    "    train(e)\n",
    "    val(e)\n",
    "\n",
    "    tensorboard.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the hyper parameters and the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {}\n",
    "for key, value in args.__dict__.items():\n",
    "    hparams[key] = str(value)\n",
    "\n",
    "final_metrics = {\n",
    "    \"max_acc_student\": maximum_tracker.max[\"student_acc\"],\n",
    "    \"max_f1_student\": maximum_tracker.max[\"student_f1\"],\n",
    "    \"max_acc_teacher\": maximum_tracker.max[\"teacher_acc\"],\n",
    "    \"max_f1_teacher\": maximum_tracker.max[\"teacher_f1\"],\n",
    "}\n",
    "\n",
    "tensorboard.add_hparams(hparams, final_metrics)\n",
    "\n",
    "tensorboard.flush()\n",
    "tensorboard.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABrUAAAF7CAYAAAB8cwCaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAACMAUlEQVR4nOzdd3hW5f3H8fedTdhbBGSrTBHCUNwWBaviXqi4ijhata0ttb86OrVaa90LBFedxdHi1rqVJSiIAgJKANkri6zz+yMhDRAgYOBJ8rxf15Xrec45932e7wkCXvnwve8QRRGSJEmSJEmSJElSdZYQ6wIkSZIkSZIkSZKkHTHUkiRJkiRJkiRJUrVnqCVJkiRJkiRJkqRqz1BLkiRJkiRJkiRJ1Z6hliRJkiRJkiRJkqo9Qy1JkiRJkiRJkiRVe5UKtUIIQ0IIX4cQ5oUQRldw/doQwvTSr5khhKIQQpPSawtDCF+UXptS1Q8gSZIkSZIkSZKk2i9EUbT9ASEkAnOAwUAmMBk4O4qiL7cx/gTgmiiKjio9XghkRFG0sgrrliRJkiRJkiRJUhxJqsSY/sC8KIrmA4QQngKGARWGWsDZwD9/SFHNmjWL2rdv/0NuIUmSdrOpU6eujKKoeazrkCRJkirLnzlJklQzbOvnTpUJtVoDi8odZwIDKhoYQkgHhgBXljsdAa+HECLggSiKHtzRB7Zv354pU1ypUJKk6iyE8G2sa5AkSZJ2hj9zkiSpZtjWz50qE2qFCs5ta83CE4APoyhaXe7coCiKloQQWgBvhBC+iqLovQoKHAmMBNhnn30qUZYkSZIkSZIkSZLiRUIlxmQCbcsdtwGWbGPsWWyx9GAURUtKX5cDEyhZznArURQ9GEVRRhRFGc2bu5KRJEmSJEmSJEmS/qcyodZkoEsIoUMIIYWS4OqlLQeFEBoChwMvljtXN4RQf9N74BhgZlUULkmSJEmSJEmSpPixw+UHoygqDCFcCbwGJAJjoyiaFUIYVXr9/tKhJwOvR1GUXW56S2BCCGHTZz0ZRdGrVfkAkiRJkiRJkiSVV1BQQGZmJnl5ebEuRdJ2pKWl0aZNG5KTkys1vjJ7ahFF0URg4hbn7t/ieBwwbotz84EDKlWJJEmSJEmSJElVIDMzk/r169O+fXtKmy4kVTNRFLFq1SoyMzPp0KFDpeZUZvlBSZIkSZIkSZJqjLy8PJo2bWqgJVVjIQSaNm26Ux2VhlqSJEmSJEmSpFrHQEuq/nb296mhliRJkiRJkiRJkqo9Qy1JkiRJkiRJkqrQwoUL6dGjR6zLqDb+/Oc/V+n9li5dyjHHHLPHvs/XX389b7755m7/nIrccccd5OTk7NbPGD9+PF26dKFLly6MHz++wjEbN27kzDPPpHPnzgwYMICFCxfucP7dd99N586dCSGwcuXKKqnVUEuSJEmSJEmSpBqssLDwB9+jqKioCiqp2K6EWtur59VXX+XYY4/9ISVtJooiiouLt3n997//PT/60Y+q7PN25rN3d6i1evVqbrrpJj799FMmTZrETTfdxJo1a7YaN2bMGBo3bsy8efO45ppr+PWvf73D+YMGDeLNN9+kXbt2VVavoZYkSZIkSZKqRAhhSAjh6xDCvBDC6AquhxDCnaXXPw8h9NnR3BDC6SGEWSGE4hBCxhb3+03p+K9DCFX3001JtcpNL8/izAc+rtKvm16etcPPLSoq4ic/+Qndu3fnmGOOITc3l2+++YY+fcr+6GPu3Ln07dsXgPbt2/PrX/+a/v37079/f+bNmwfAihUrOPXUU+nXrx/9+vXjww8/BODGG29k5MiRHHPMMZx//vmMGzeOYcOGMWTIEPbbbz9uuummss856aST6Nu3L927d+fBBx8sO1+vXj2uv/56BgwYwMcff8zvf/97+vXrR48ePRg5ciRRFAFwxBFHcM0113DYYYfRtWtXJk+ezCmnnEKXLl34v//7v7L7Pf744/Tv35/evXtz6aWXUlRUxOjRo8nNzaV3794MHz58m+Mqqmf06NF069aNXr168ctf/rLsc1599VWGDh261ff72muvpV+/fvTq1YsHHngAgKysLI4++mj69OlDz549efHFF4GSbrquXbty+eWX06dPH95//326du261a8ZwAUXXMBzzz1X9ut0ww03lN3vq6++Kvt1Gjx4MH369OHSSy+lXbt22+xO2vKzFy1axGWXXUZGRgbdu3fnhhtuAODOO+9kyZIlHHnkkRx55JEAvP766xx00EH06dOH008/naysrG3/R1gJr732GoMHD6ZJkyY0btyYwYMH8+qrr2417sUXX2TEiBEAnHbaabz11ltEUbTd+QceeCDt27f/QfVtyVBLkiRJkiRJP1gIIRG4BxgKdAPODiF022LYUKBL6ddI4L5KzJ0JnAK8t8XndQPOAroDQ4B7S+8jSdXC3LlzueKKK5g1axaNGjXi+eefp1OnTjRs2JDp06cD8Mgjj3DBBReUzWnQoAGTJk3iyiuv5Oqrrwbgqquu4pprrmHy5Mk8//zzXHLJJWXjp06dyosvvsiTTz4JwKRJk3jiiSeYPn06zz77LFOmTAFg7NixTJ06lSlTpnDnnXeyatUqALKzs+nRoweffvophxxyCFdeeSWTJ09m5syZ5Obm8u9//7vss1JSUnjvvfcYNWoUw4YN45577mHmzJmMGzeOVatWMXv2bJ5++mk+/PBDpk+fTmJiIk888QQ333wzderUYfr06TzxxBPbHLdlPd26dWPChAnMmjWLzz//vCw8Kyoq4uuvv6Zbt83/ihkzZgwNGzZk8uTJTJ48mYceeogFCxaQlpbGhAkTmDZtGu+88w6/+MUvysK6r7/+mvPPP5/PPvuMdu3aVfhrVpFmzZoxbdo0LrvsMm677TYAbrrpJo466iimTZvGySefzHfffbfd/z62/Ow//elPTJkyhc8//5x3332Xzz//nJ/97GfsvffevPPOO7zzzjusXLmSP/7xj7z55ptMmzaNjIwMbr/99q3ufeutt9K7d++tvn72s59tNXbx4sW0bdu27LhNmzYsXrx4u+OSkpJo2LAhq1atqvT8qpK02+4sqXbIz4b1S2JdhaQfon4rSK0X6yokSZJU+/UH5kVRNB8ghPAUMAz4styYYcCjUclPEz8JITQKIbQC2m9rbhRFs0vPbfl5w4CnoijaCCwIIcwrreHj3fR8FcovLOb7dXlbnU9IgNSkRNKSE0hLTiQ50X9bLsXKDSd0j8nndujQgd69ewPQt2/fsj2ILrnkEh555BFuv/12nn76aSZNmlQ25+yzzy57veaaawB48803+fLL//1Run79ejZs2ADAiSeeSJ06dcquDR48mKZNmwJwyimn8MEHH5CRkcGdd97JhAkTAFi0aBFz586ladOmJCYmcuqpp5bNf+edd/jrX/9KTk4Oq1evpnv37pxwwgllnwXQs2dPunfvTqtWrQDo2LEjixYt4oMPPmDq1Kn069cPgNzcXFq0aLHV9+Wtt97a5rjy9TRo0IC0tDQuueQSfvzjH3P88ccD8OmnnzJgwICt7vv666/z+eefl3VUrVu3jrlz59KmTRuuu+463nvvPRISEli8eDHLli0DoF27dgwcOHCHv2ZbOuWUU8rG/Otf/wLggw8+KPseDxkyhMaNG1c4d5MtP/uZZ57hwQcfpLCwkKVLl/Lll1/Sq1evzeZ88sknfPnllwwaNAiA/Px8DjrooK3ufe2113Lttddu9/M32RTwlVfB37nbHFfZ+VXFUEvStm1YBg8eARsMtaQa7eynYb8hsa5CkiRJtV9rYFG540xgy586VjSmdSXnVvR5n1Rwrz0qc00OR/3t3R2OS0oINKiTTMPSr0bpyTRJT6FlwzRa1k+lZYM0WjZMo23jdJrVS9mtPxCUtGekpqaWvU9MTCxbyu7UU08t6+rp27dvWQgFm4cBm94XFxfz8ccfbxZebVK3bt3Njrf8syOEwH//+1/efPNNPv74Y9LT0zniiCPIyysJ49PS0khMLGlyzcvL4/LLL2fKlCm0bduWG2+8sWxc+edJSEjY7NkSEhIoLCwkiiJGjBjBX/7yl+1+X7Y3rnw9SUlJTJo0ibfeeounnnqKu+++m7fffptXXnmFIUO2/jlHFEXcddddW+21NW7cOFasWMHUqVNJTk6mffv2Zc+15fdvW79mW9o0LjExsWw/s4rCne0p/9kLFizgtttuY/LkyTRu3JgLLrhgs+99+WccPHgw//znP7d771tvvbWs+628ww47jDvvvHOzc23atOG///1v2XFmZiZHHHHEVnPbtGnDokWLaNOmDYWFhaxbt44mTZpUen5VMdSSVLGiQnj+YshdAyfcCSl1dzxHUvXUqteOx0iSJEk/XEUpzJY/4dvWmMrM3ZXPI4QwkpKlDtlnn312cMud17x+Kn87/YCtzhcVR2wsLCKvoJi8giJyC4rYkFfI2twC1ubksyY7n7nLsli+IY+Cos3LrpuSyD5N69KuSTodmtdlv5b12bdlfTq1qEtqkissSjVdWloaxx57LJdddhljxozZ7NrTTz/N6NGjefrpp8s6cI455hjuvvvuss6b6dOnl3UTbemNN95g9erV1KlThxdeeIGxY8eyePFiGjduTHp6Ol999RWffPJJhXM3hSjNmjUjKyuL5557jtNOO63Sz3X00UczbNgwrrnmGlq0aMHq1avZsGED7dq1Izk5mYKCApKTk7c7rrysrCxycnI47rjjGDhwIJ07dwZKOr0q6kI69thjue+++zjqqKNITk5mzpw5tG7dmnXr1tGiRQuSk5N55513+Pbbbyv9TDvjkEMO4ZlnnuHXv/41r7/+OmvWrKn03PXr11O3bl0aNmzIsmXLeOWVV8qCofr167NhwwaaNWvGwIEDueKKK5g3bx6dO3cmJyeHzMxM9t13383utzOdWsceeyzXXXddWb2vv/56hYHjiSeeyPjx4znooIN47rnnOOqoowghVHp+VTHUkvQ/xcWwbCYU5cMXz8HC9+Gk+6D3ObGuTJIkSZJU/WUCbcsdtwG2XPpjW2NSKjF3Vz6PKIoeBB4EyMjI2Ll/Rl8J9dOSObVvm12eX1wcsTonn2Xr8/h+XR7frc7h21U5fLc6h7nLN/DWV8vKQq/EhED7pun0bN2Qnm0acUCbhnTbuwHpKf6IT6pphg8fzr/+9S+OOeaYzc5v3LiRAQMGUFxcXNaNc+edd3LFFVfQq1cvCgsLOeyww7j//vsrvO8hhxzCeeedx7x58zjnnHPIyMigZ8+e3H///fTq1Yv99ttvsyXvymvUqBE/+clP6NmzJ+3bty9bHrCyunXrxh//+EeOOeYYiouLSU5O5p577qFdu3aMHDmSXr160adPH5544oltjitvw4YNDBs2jLy8PKIo4u9//zsrVqwgLS2NBg0abPX5l1xyCQsXLqRPnz5EUUTz5s154YUXGD58OCeccAIZGRn07t2b/ffff6eeq7JuuOEGzj77bJ5++mkOP/xwWrVqRf369Ss194ADDuDAAw+ke/fudOzYsWx5QYCRI0cydOhQWrVqxTvvvMO4ceM4++yz2bhxIwB//OMftwq1dkaTJk343e9+V/brff3119OkSZOy9xkZGZx44olcfPHFnHfeeXTu3JkmTZrw1FNP7XD+nXfeyV//+le+//57evXqxXHHHcfDDz+8y7UChJ1tidsTMjIyok0b2Enag2a9AM+O+N9xn/PhxLtiVo6k6i2EMDWKooxY1yFJkqTqIYSQBMwBjgYWA5OBc6IomlVuzI+BK4HjKFle8M4oivpXcu5/gV9GUTSl9Lg78CQl+2jtDbwFdImiqGhbNdbEnzkVFBWzYGU2X3+/gTnLNjB76QZmLl7H9+tLOioSAuy/VwMGdGzCwI5N6d++CY3rpsS4ain2Zs+eTdeuXWNdxjbddtttrFu3jj/84Q9l59q3b8+UKVNo1qzZLt1z3LhxTJkyhbvvvruqyqx2Hn/8cTIzMxk9enSsS9nKxo0bSUxMJCkpiY8//pjLLruM6dOnx7qsGqGi36/b+rmT/4xD0v+snFvyevbTkJIO7QZtf7wkSZIkSaWiKCoMIVwJvAYkAmOjKJoVQhhVev1+YCIlgdY8IAe4cHtzAUIIJwN3Ac2B/4QQpkdRdGzpvZ8BvgQKgSu2F2jVVMmJCexbuvxgecvX5zEjcx2fZ65l6rdrePLT73jkw4UA7L9XfQZ2bMqADk04uFMzGqYnx6BySdty8skn88033/D222/HupQa59xzz411Cdv03XffccYZZ1BcXExKSgoPPfRQrEuqlezUkvQ/L18FX/0Hrp0X60ok1QB2akmSJKmmqc0/c9pYWMTnmev45JtVfLpgNVO+XU1eQTGJCYG+7Rrzo64tOGr/lnRqXpcQKtqOTKpdqnunluLDqlWrOProo7c6/9Zbb9G0adMYVFQ92aklaZcUr11EdmpLPv1yWaxLkVSFDmjbiOb1U2NdhiRJkqTdKDUpkX7tm9CvfRN+CuQXFvN55lr++/UK3py9jD9P/Io/T/yK9k3TOWr/lhzdtQUDOjQhKTEh1qVLUq3VtGlTlyCsYoZakgCIoohlmfOZkdOMUY/Wzn+1JsWrMSMyOLpry1iXIUmSJGkPSklKIKN9EzLaN+GXx+7H4rW5vP3Vct6evYzHP/2WsR8uoGndFI7r2YoTe+9N330ak5BgB5dqlyiK7EyUqrmdXU3QUEsSAA+/v4Az876nWet+vHz8IbEuR1IV2qdpeqxLkCRJkhRjrRvV4byB7ThvYDty8gt5b85KXv58Cc9OXcRjn3zL3g3TOOGAvTnhgL3pvncDgwDVeGlpaaxatYqmTZv637NUTUVRxKpVq0hLS6v0HEMtKY69PGMJn323lo2FRfx70lf8JDWXvj17Eto0jHVpkiRJkiRpN0lPSWJIj70Y0mMvsjYW8uaXy3hpxhLGfLCAB96bT6fmdTmzX1tO7dOGpvVcylw1U5s2bcjMzGTFihWxLkXSdqSlpdGmTZtKjzfUkuLUG18u46f//Iw6yYkkJQSO26cIlkFoWPk/QCRJkiRJUs1WLzWJkw5szUkHtmZNdj6vzPye56dl8ueJX3Hra19zTPe9OKf/PhzUsanLE6pGSU5OpkOHDrEuQ1IVM9SS4tB3q3L4xTPT6dm6Ic+OOoi05ESY8zo8CRhqSZIkSZIUlxrXTeGcAftwzoB9mLNsA09NWsTz0zL5z+dLadc0nTP7teX0vm1pXt/uLUlSbBhqSbXcx9+s4vlpmZudm/btGgDuHd6nJNACWF86xlBLkiRJkqS4t2/L+lx/Qjd+NWQ/Xp35Pf+c9B1/ffVr7nhjLif23puLD+lA11YNYl2mJCnOGGpJtVhxccRvX/iC79fl0Tg9pex8alICd559IG2bpP9v8LpMSEiCei1jUKkkSZIkSaqO0pITy5Yn/GZFFuM/WsizUzJ5bmomgzo35ZJDOnL4vs1dmlCStEcYakm12H/nLGf+imz+cVZvhvVuvf3B6xZD/VaQkLhnipMkSZIkSTVKp+b1+P2wHvx88L78c9Iixn+0kAvHTaZj87pcfEgHTu3T5n8rwkiStBskxLoASbvPmA8WsFeDNI7r2WrHg9dluvSgJEmSJEnaoUbpKVx2RCfe//WR/OOs3tRNSeK3E2ZyyC3v8OB735CTXxjrEiVJtZShllRLzV66ng/nreL8g9uRnFiJ3+rrM6HBDrq5JEmSJEmSSiUnJjCsd2teunIQT40cyP571efPE7/ikFve4d7/ziNro+GWJKlqufygarw5yzYw4bPFRFGsK6lepixcTZ3kRM7pv8+OBxcXw/ol0M1OLUmSJEmStHNCCAzs2JSBHZsy9ds13PX2XP766tc8+N58LhrUgREHt6dhneRYlylJqgUMtVTjjf1gAU9NXkRKko2HWxp5aEcapafseGD2CijKd/lBSZIkSZL0g/Rt15hxF/ZnxqK13PX2XG5/Yw4PvT+fSw/ryEWHdCA9xR9HSpJ2nX+LqMZbk5PPvi3r8fo1h8e6lJprfWbJq8sPSpIkSZKkKnBA20Y8PKIfMxev444353Lb63MY//G3XHV0F87s17ZyWyVIkrQF//ZQjbcut4BGdSrRjaRtW1caatmpJUmSJEmSqlCP1g15eEQGz406iPZN0/m/F2ZyzN/f49+fLyFyLwlJ0k4y1FKNtzangAauy/zDrFtc8mqoJUmSJEmSdoOM9k145tKDePj8DJITA1c++RnD7vmQj+atjHVpkqQaxOUHq4vsVfD276EgL9aV1DhXrV9K8ygN/tU41qXUXN9/DsnpUMfvoSRJkiRJ2j1CCPyoW0uO3L8FEz5bzN/fmMM5D3/K4G4t+b8fd6Vd07qxLlGSVM0ZalUX37wNU8dBgzaQkBjramqUHkW51MtNgu/s1vpBepwCIcS6CkmSJEmSVMslJgRO69uG43u1YuyHC7j77XkMvv09Lj60A1ce2Zm6qf7IUpJUMf+GqC6yvi95vfwjSGsY21pqkIKiYg797Sv8/LB9+dnRXWJdjiRJkiRJkiopLTmRy4/ozKl92nDLK19x33+/4fmpmYweuj8n9W5NQoL/+FaStDn31KouNnwPSXUgtUGsK6lR1uUWANDQPbUkSZIkSZJqpJYN0rj9zN786/KDadUwjZ8/M4NT7/+IGYvWxro0SVI1Y6hVXWQtg/otXf5tJxlqSZIkSZIk1Q599mnMhMsHcetpvVi0OpeT7v2QG16cyfq8gliXJkmqJioVaoUQhoQQvg4hzAshjK7g+rUhhOmlXzNDCEUhhCaVmatSG76HenvFuooapyzUSjfUkiRJkiRJqukSEgKnZ7TlnV8ezoiD2vPoJ98y+PZ3eeWLpURRFOvyJEkxtsNQK4SQCNwDDAW6AWeHELqVHxNF0a1RFPWOoqg38Bvg3SiKVldmrkpt6tTSTlmXY6eWJEmSJElSbVM/LZkbT+zOhMsH0bRuKpc9MY1Lxk8hc01OrEuTJMVQZTq1+gPzoiiaH0VRPvAUMGw7488G/rmLc+PXhmVQz1BrZ7n8oCRJkiRJUu3Vu20jXrpyEP/346589M0qBt/+Hg+9N5/CouJYlyZJioHKhFqtgUXljjNLz20lhJAODAGe39m5ca0gFzauM9TaBZtCrUaGWpIkSZIkSbVSUmIClxzakTd+fhiDOjflTxNnc9K9H/LV9+tjXZokaQ+rTKgVKji3rQVsTwA+jKJo9c7ODSGMDCFMCSFMWbFiRSXKqkU2fF/yWt89tXbW2tLlBxsYakmSJEmSJNVqbRqn89D5Gdw7vA9L1+Zxwl0fcPfbcymwa0uS4kZlQq1MoG254zbAkm2MPYv/LT24U3OjKHowiqKMKIoymjdvXomyapGsZSWv9Qy1dta63ALqpiSSnFiZ/5QlSZIkSZJUk4UQOK5nK974+eEc230vbnt9DifbtSVJcaMyScBkoEsIoUMIIYWS4OqlLQeFEBoChwMv7uzcuFfWqeXygztrXW4BjdJTYl2GJEmSJEmS9qAmdVO4+5w+3Feua+uut+zakqTaboehVhRFhcCVwGvAbOCZKIpmhRBGhRBGlRt6MvB6FEXZO5pblQ9QK2QtL3m1U2unrcstcOlBSZIkSZKkODW0XNfW394o6dqas2xDrMuSJO0mSZUZFEXRRGDiFufu3+J4HDCuMnO1hazvISEJ0pvGupIaZ11uPg3rVOo/Y0mSJEmSJNVCm7q2ftxzKf/3wkxOuOsDrjuuK+cf1I4QQqzLkyRVITciqg42LIO6LSDBX46dtS63gEZ1XH5QkiRJkiQp3g3t2YpXrz6Mgzs15YaXZnHhuMks35AX67IkSVXIFKU6yPre/bR20brcAhq6/KAkSZIkSZKA5vVTGXtBP/4wrDsff7OKIXe8z5tfLot1WZKkKmKoVR1sWOZ+WrtobU4BDdMNtSRJkiRJklQihMB5B7XnPz87hFYN07jk0SlcN+ELcvILY12aJOkHMtSqDuzU2iV5BUVsLCy2U0uSJEmSJElb6dyiPhMuH8Slh3fkn5O+4/g7P2DWknWxLkuS9AMYasVaUSFkr7RTaxeszy0AMNSSJEmSJElShVKSEvjN0K48eclAcvKLOPnej3jsk2+JoijWpUmSdoGhVqxlLwciO7V2wVpDLUmSJEmSJFXCQZ2a8p+fHcJBHZvyuxdmcuU/P2N9XkGsy5Ik7SRDrVjb8H3Jaz1DrZ21zlBLkiRJkiRJldS0XiqPXNCPXw/Zn1dnfs/xd37A55lrY12WJGknGGrFWtaykleXH9xp63JKQq1G6YZakiRJkiRJ2rGEhMBlR3TimUsHUlhUzKn3fcQjHy5wOUJJqiEMtWJtU6eWyw/uNJcflCRJkiRJ0q7o264J//nZoRy+b3NuevlLRj0+lQ0uRyhJ1Z6hVizkZ8PU8fDmjfDFcyXn6raIaUk1kcsPSpIkSZIkaVc1rpvCQ+dn8NvjuvLm7OUMu/tD5izbEOuyJEnbkRTrAuLOB38v+cpbBwnJEALscxAkpcS6shpnXW4BIUD9NEMtSZIkSZIk7bwQAj85rCM92zTkyiencdI9H3LLqb044YC9Y12aJKkChlp7Utbyku6s9ofCUf8HbQeUhFraJetzC6ifmkRigt9DSZIkSZIk7bqBHZvy758eyuVPTOWn//yM6YvWMnro/iQnutCVJFUn/qm8J21YWvLafyTsM9BA6wdam5NPw3S7tCRJkiRJkvTD7dUwjadGHsSIg9ox5oMFnPvwp6zYsDHWZUmSyjHU2pM2LCt5rb9XbOuoJdblFtCojss2SpIkSZIkqWqkJCVw07Ae/P3MA5iRuZbj73qfqd+uiXVZkqRSLj+4J2V9X/Jar2Vs66jAotU5LFmbG+sydsqStXk0r58a6zIkSZIkSZJUy5x8YBv2a9mAUY9P5ewHP+GPJ/fgjIy2sS5LkuKeodaetKlTazeEWkvX5fLvGUspjqKdmpdXUMzbXy1jRua6Kq9pT+jRumGsS5AkSZIkSVIt1G3vBrx05SCueHIav3ruc75auoHrjtufJPfZkqSYMdTak7K+h7SGkJxWpbctLCrm4nFT+HLp+l2a36N1A357XFe67d2AmrbLV3dDLUmSJEmSJO0mjdJTGH9hf/74n9mM/XABc5dv4K6zD6RRultiSFIsGGrtSRu+h3pVv5/W4598y5dL1/OPs3ozuNvOdYElhEBacmKV1yRJkiRJkiTVBkmJCdx4Yne6tWrAb1/4gpPu+ZCHR2TQuUX9WJcmSXHHXtk9KWsZ1K/apQeXb8jjb6/P4dAuzTjxgL1JT0naqS8DLUmSJEmSJGnHzujXln/+ZCBZGws56Z6PeGv2sliXJElxx06tPSlrGbQdWOGltTn5PPT+fBavyWVVdj5FxZXbG+v7dXnkFRZx44ndCaGmLR4oSZIkSZIk1RwZ7Zvw0pWHMPKxKVzy6BR+M3R/fnJoR38uJ0l7iKHWnhJFsKHiTq2i4oif/vMzPvpmFXs3SqNJ3VRSEiv3F2HTeimMOqITnZrXq+qKJUmSJEmSJG1h70Z1ePbSg/nlszP488SvWLAyh98P605yootiSdLuZqi1p+SthaKNFe6pddfbc3l/7kr+ckpPzu6/z56vTZIkSZIkSVKl1UlJ5K6zD6R9s3TueecbFq3O4Z7hfWhYJznWpUlSrWaotRvNXLyOJz79liiCFnkL+Dnwz9n5zFj6edmY/KJiJny2mFP6tOasfm1jV6wkSZIkSZKkSktICFx77P60b1qX6yZ8wan3fcQjF/SjbZP0WJcmSbWWodZu9OSk73h68iKa108lo/gbAN5bmsC0Zcs3G3dYl+b88aQerr0rSZIkSZIk1TCnZ7SlTeN0Rj0+lZPu+ZAHz+9L33ZNYl2WJNVKhlq70aqsjXRuUY/XrzkcZqyCCXDfqB9Dsy6xLk2SJEmSJKnKhRCGAP8AEoGHoyi6eYvrofT6cUAOcEEURdO2NzeE0AR4GmgPLATOiKJoTQghGXgY6EPJz7gejaLoL7v7GaWKHNSpKRMuP5iLxk3m7Ic+5bbTD+DEA/aOdVmSVOu4e+FutCorn6Z1U0sOsr4vea3XMnYFSZIkSZIk7SYhhETgHmAo0A04O4TQbYthQ4EupV8jgfsqMXc08FYURV2At0qPAU4HUqMo6gn0BS4NIbTfPU8n7VjH5vWYcPkgerdtxM/++Rn3v/sNURTFuixJqlUMtXajVdn5NK2XUnKwYRkkp0Nq/dgWJUmSJEmStHv0B+ZFUTQ/iqJ84Clg2BZjhlHSURVFUfQJ0CiE0GoHc4cB40vfjwdOKn0fAXVDCElAHSAfWL97Hk2qnMZ1U3js4v6ceMDe3PzKV9z08pcUFRtsSVJVMdTajVZmbaRZvXKdWvVagvtmSZIkSZKk2qk1sKjccWbpucqM2d7cllEULQUofW1Rev45IBtYCnwH3BZF0eof/hjSD5OalMgdZ/Zm5GEdGffRQq54Yhp5BUWxLkuSagVDrd1kY2ERG/IKaVq3XKdW/b1iW5QkSZIkSdLuU9G/5N2yRWVbYyozd0v9gSJgb6AD8IsQQsetigphZAhhSghhyooVK3ZwS6lqJCQErjuuK9cf343Xvvyecx/+lLU5+bEuS5JqPEOt3WR1dslfUk3LOrWWQb0W25khSZIkSZJUo2UCbcsdtwGWVHLM9uYuK12ikNLX5aXnzwFejaKoIIqi5cCHQMaWRUVR9GAURRlRFGU0b958lx5M2lUXHdKBu8/uw+eZ6zj1vo/IXJMT65IkqUYz1NpNVmVtCrVKO7WylkE9O7UkSZIkSVKtNRnoEkLoEEJIAc4CXtpizEvA+aHEQGBd6ZKC25v7EjCi9P0I4MXS998BR5Xeqy4wEPhqdz2ctKt+3KsVj17cnxUbNnLKvR8xa8m6WJckSTWWodZusiJrI0DJnlr5ObBxPdRvGeOqJEmSJEmSdo8oigqBK4HXgNnAM1EUzQohjAohjCodNhGYD8wDHgIu397c0jk3A4NDCHOBwaXHAPcA9YCZlIRij0RR9PnufUpp1wzs2JTnLjuYxITAmQ98wgdzV8a6JEmqkZJiXUBttalTq1m9FMj6vuSknVqSJEmSJKkWi6JoIiXBVflz95d7HwFXVHZu6flVwNEVnM8CTv+BJUt7zL4t6zPh8kFc8MgkLnhkEree3ouTD2wT67IkqUaxU2s3WVXaqdW0XipsWFZy0k4tSZIkSZIkKW7t1TCNZ0YdRL/2Tbjm6RmM+WBBrEuSpBrFUGs3WZWdT2pSAnVTEmHp9JKTjTvEtCZJkiRJkiRJsdUgLZlxF/VjaI+9+MO/v+Rvr39NSROjJGlHKhVqhRCGhBC+DiHMCyGM3saYI0II00MIs0II75Y7vzCE8EXptSlVVXh1tzJrI83qpRJCgBn/hFYHQNNOsS5LkiRJkiRJUoylJiVy9zl9ODOjLXe9PY/rX5xFcbHBliTtyA731AohJFKy8eZgIBOYHEJ4KYqiL8uNaQTcCwyJoui7EEKLLW5zZBRFcbX74aqsfJrWS4FlX8LSGTDklliXJEmSJEmSJKmaSEwI3HxqTxrVTeaBd+ezLreA204/gJQkF9eSpG3ZYagF9AfmRVE0HyCE8BQwDPiy3JhzgH9FUfQdQBRFy6u60JpmVfZGmtdLhRlPQkIS9Dwt1iVJkiRJkiRJqkZCCPxmaFcap6dw8ytfsT6vgPuG96VOSmKsS5OkaqkysX9rYFG548zSc+XtCzQOIfw3hDA1hHB+uWsR8Hrp+ZE/rNyaY1VWPs3rJsLnz0CXY6Bus1iXJEmSJEmSJKkaGnV4J24+pSfvzVnBeWM+ZV1uQaxLkqRqqTKhVqjg3JYLvCYBfYEfA8cCvwsh7Ft6bVAURX2AocAVIYTDKvyQEEaGEKaEEKasWLGictVXU1EUsSorn75F0yFrGRxwdqxLkiRJkiRJklSNndV/H+4+pw8zMtdy1oOfsHxDXqxLkqRqpzKhVibQttxxG2BJBWNejaIou3TvrPeAAwCiKFpS+rocmEDJcoZbiaLowSiKMqIoymjevPnOPUU1s2FjIXsXL+bHC/8KdVvAvsfGuiRJkiRJkiRJ1dxxPVsx9oJ+fLsqm9Pv/5hFq3NiXZIkVSuVCbUmA11CCB1CCCnAWcBLW4x5ETg0hJAUQkgHBgCzQwh1Qwj1AUIIdYFjgJlVV341s3o+LJpMzqzXeCblDyRHG+Hc5yApNdaVSZIkSZIkSaoBDu3SnMcvGcDanAJOu/8j5i3fEOuSJKna2GGoFUVRIXAl8BowG3gmiqJZIYRRIYRRpWNmA68CnwOTgIejKJoJtAQ+CCHMKD3/nyiKXt09jxJjWSvgngEw5kfs9fJwIuCLwU9CqwNiXZkkSZIkSZKkGqTPPo155tKDKCqGMx/4hC+XrI91SZJULSRVZlAURROBiVucu3+L41uBW7c4N5/SZQhrvbmvQ1E+nHAnU9bUYeSbRTzWunusq5IkSZIkSZJUA+23V32euXQgwx/+lLMf+oTHLu5PrzaNYl2WJMVUZZYfVGXMfQ3qt4I+5/NVvQGspgHN6rnsoCRJkiRJkqRd07F5PZ659CAa1Eli+EOfMmXh6liXJEkxZahVFQrz4Zt3oMtgCIFVWfkANE5PiXFhkiRJkiRJkmqytk3SeebSg2heP5Xzxkzio3krY12SJMWMoVZV+O5j2Lge9h0CwKrsjTSsk0xKkt9eSZIkSZIkST9Mq4Z1ePrSg9inSToXjpvMO18vj3VJkhQTpi5VYe7rkJgCHQ4HYFVWPk3r2aUlSZIkSZIkqWo0r5/KP0cOpEvLeox8dAqvzvw+1iVJ0h5nqFUV5rwK7Q+F1HoArMzaSLO67qclSZIkSZIkqeo0qZvCE5cMpEfrhlzx5DRemrEk1iVJ0h5lqLWrvp8J//klvPRTWDUP9j227NLyDRtp3sBQS5IkSZIkSVLValgnmccuHkBGu8Zc9dRnPDNlUaxLkqQ9xlBrV00ZA5Mfhq/+A43bQ9cTACgujli8Jpc2jevEtj5JkiRJkiRJtVK91CTGXdifQzo341fPfc4/J30X65IkaY9IinUBNdaGZdCiG1z+0Wanl2/YSH5RMW0ap8eoMEmSJEmSJEm1XZ2URB4ekcGox6bym399QRTBOQP2iXVZkrRb2am1q7K+h3ottjqduSYHgLZ2akmSJEmSJEnajVKTErn/vL4ctX8LrpvwBY9/8m2sS5Kk3cpQa1dlLYf6e211OnNNLoCdWpIkSZIkSZJ2u9SkRO47tw8/6tqC/3thJo99vDDWJUnSbmOotSuiCLKWVdiptWh1SaeWe2pJkiRJkiRJ2hNSkxK5Z3gfftS1Jb97cRaPGmxJqqUMtXZF7hooyod6Lbe6lLkml+b1U0lLToxBYZIkSZIkSZLiUWpSIvcO78Pgbi25/sVZjPtwQaxLkqQqZ6i1K7KWl7xWFGqtzbFLS5IkSZIkSdIel5KUwD3n9OGYbi258eUvecRgS1ItY6i1K7K+L3mtINRatDrX/bQkSZIkSZIkxURKUgL3DO/Dsd1bctPLXzLmA4MtSbWHodau2NSpVX+vzU4XFUcsWZtLWzu1JEmSJEmSJMVIcmICd5/Th6E99uIP//6Sh9+fH+uSJKlKGGrtig2bOrVabHZ62fo8CosjO7UkSZIkSZIkxVRyYgJ3nn0gx/Xciz/+Z7ZLEUqqFZJiXUCNlLUMkupAaoPNTi9anQPgnlqSJEmSJEmSYi45MYF/nHUgRcXTuOnlL0lKTOC8ge1iXZYk7TI7tXZF1rKSLq0QNjuduSYXMNSSJEmSJEmSVD0kJyZw19l9OHr/FvzuhZk8Pfm7WJckSbvMUGtXZC3baj8t+F+o1dpQS5IkSZIkSVI1kZKUwL3n9uHwfZsz+l9f8PzUzFiXJEm7xFBrV2xYttV+WgCL1uTQskEqqUmJMShKkiRJkiRJkiqWmpTIA+f15eBOTbn2uRm8NGNJrEuSpJ1mqLUrspZBvYo6tXJo0zg9BgVJkiRJkiRJ0valJSfy8Pn96Ne+Cdc8PZ1Xvlga65IkaacYau2sgjzIWwv1Wm51KXNNLm1delCSJEmSJElSNVUnJZGxF/TjwLaN+Ok/P+ONL5fFuiRJqrSkWBdQ0zz936mcCTwwLZv/fv3JZteWrM2lTe/WsSlMkiRJkiRJkiqhbmoSj1zYj/PGTOLyJ6by4HkZHLn/1tutSFJ1Y6fWTnrvs5kArA6NKCqONvsa0KEpx3TfuoNLkiRJkiRJkqqT+mnJjL+oP/vtVZ9LH5/K+3NXxLokSdohO7V2UlreSgB+c8YRsHfvmNYiSZIkSZIkSbuqYZ1kHr94AGc/9CmXjJ/CIxf24+BOzWJdliRtk51aOyGKItLzS0KtivbUkiRJkiRJkqSapFF6Co9f3J92TdO5eNwUJi1YHeuSJGmbDLV2woaNhTRjLREB6jaPdTmSJEmSJEmS9IM1rZfKE5cMZO9GaVz4yCQ++25NrEuSpAoZau2EtdkFNGctG1MaQ6IrN0qSJEmSJEmqHZrXT+XJnwykWf1URoydxJdL1se6JEnaiqHWTlibm0/zsJaC9BaxLkWSJEmSJEmSqlTLBmk8cckA6qUmcd6YT5m3PCvWJUnSZmw3qsi3H8FHdwPR/84VF9L5+7n0SFjI+npHxKoySZIkSZIkSdpt2jRO5/FLBnDGA59w7sOf8uyog2jbJD3WZUkSYKdWxaY9CvPehHWL/ve14XvW1uvEXUUns+6Q62NdoSRJkiRJkiTtFh2b1+PxS/qTW1DE8Ic/5ft1ebEuSZIAO7UqtnIO7DMQRry02ek3PlrI3xfMYnib7jEqTJIkSZIkSZJ2v/33asCjF/Vn+MOfMvzhT3jm0oNoWi811mVJinN2am0pimDlPGi271aX1uYUANCoTvKerkqSJEmSJEmS9qgD2jZizIgMFq/N5bwxk1iXWxDrkiTFOUOtLWUth43roFmXrS6tycmnfmoSSYl+2yRJkiRJkiTVfgM6NuWB8zKYu3wDFzwyieyNhbEuSVIcM53Z0so5Ja8VhFrrcgtoVNcuLUmSJEmSJEnx4/B9m3PX2X34PHMdP3l0CnkFRbEuSVKcqlSoFUIYEkL4OoQwL4QwehtjjgghTA8hzAohvLszc6uVslBr6+UH1+Tk06hOyh4uSJIkSZIkSZJia0iPvbjt9F58PH8Vlz8xjfzC4liXJCkO7TDUCiEkAvcAQ4FuwNkhhG5bjGkE3AucGEVRd+D0ys6tdlbNg+S6UH/vrS6tzSmgUbqdWpIkSZIkSZLiz8kHtuGPJ/Xg7a+Wc80z0ykqjmJdkqQ4k1SJMf2BeVEUzQcIITwFDAO+LDfmHOBfURR9BxBF0fKdmFu9rJwDTTtBwtZ539qcfNo2SY9BUZIkSZIkSZIUe8MHtCNnYxF/mjibOsmJ/PXUXiQkhFiXJSlOVGb5wdbAonLHmaXnytsXaBxC+G8IYWoI4fydmAtACGFkCGFKCGHKihUrKlf97rByToVLDwKszS2gsZ1akiRJkiRJkuLYTw7ryFVHd+G5qZn8/t9fEkV2bEnaMyrTqVVRzL7ln1JJQF/gaKAO8HEI4ZNKzi05GUUPAg8CZGRkxOZPwYJcWLsIep+71aWi4oh1uQU0qmOoJUmSJEmSJCm+Xf2jLmRtLGTMBwtolJ7M1T+quFFAkqpSZUKtTKBtueM2wJIKxqyMoigbyA4hvAccUMm51ceqb4AImnXe6tKGvAKiCBqlp+z5uiRJkiRJkiSpGgkh8NvjurIut4A73pxLwzrJXDioQ6zLklTLVWb5wclAlxBChxBCCnAW8NIWY14EDg0hJIUQ0oEBwOxKzq0+Vs4pea1g+cE1OQUANHL5QUmSJEmSJEkiISFw8yk9ObZ7S256+Uv+NS0z1iVJquV2GGpFUVQIXAm8RklQ9UwURbNCCKNCCKNKx8wGXgU+ByYBD0dRNHNbc3fPo1SBlXOBAE06bXVpbU4+AI3t1JIkSZIkSZIkAJISE/jHWQcyqHNTrn3uc974clmsS5JUi1WmU4soiiZGUbRvFEWdoij6U+m5+6Mour/cmFujKOoWRVGPKIru2N7camvlHGjUFlLSt7q0trRTq6GdWpIkSZIkSRUKIQwJIXwdQpgXQhhdwfUQQriz9PrnIYQ+O5obQmgSQngjhDC39LVxuWu9QggfhxBmhRC+CCGk7f6nlLSltOREHjgvgx6tG3LFk9P4+JtVsS5JUi1VqVArbqyeX2GXFsDaXDu1JEmSJEmStiWEkAjcAwwFugFnhxC6bTFsKNCl9GskcF8l5o4G3oqiqAvwVukxIYQk4HFgVBRF3YEjgILd9XyStq9eahLjLuhHuybpXDJ+Mp9nro11SZJqIUOt8jauhzqNK7y0Jrt0T606dmpJkiRJkiRVoD8wL4qi+VEU5QNPAcO2GDMMeDQq8QnQKITQagdzhwHjS9+PB04qfX8M8HkURTMAoihaFUVR0W56NkmV0LhuCo9dPIDGdVMYMXYS85ZviHVJkmoZQ63yNmZBar0KL63NLSAEaGCoJUmSJEmSVJHWwKJyx5ml5yozZntzW0ZRtBSg9LVF6fl9gSiE8FoIYVoI4VcVFRVCGBlCmBJCmLJixYpdeCxJO2Ovhmk8fvEAEhMSOPfhSWSuyYl1SZJqEUOt8vKzIWUboVZOPg3SkklMCHu4KEmSJEmSpBqhoh+aRJUcU5m5W0oCDgGGl76eHEI4equbRNGDURRlRFGU0bx58x3cUlJVaN+sLo9d3J+c/ELOGzOJFRs2xrokSbWEodYmUQT5WdsJtQponG6XliRJkiRJ0jZkAm3LHbcBllRyzPbmLitdopDS1+Xl7vVuFEUroyjKASYCfargOSRVga6tGvDIhf1Yui6XEWMnsS7XLe8k/XCGWpsU5AARpNSt8PKanHwapqfs2ZokSZIkSZJqjslAlxBChxBCCnAW8NIWY14Czg8lBgLrSpcU3N7cl4ARpe9HAC+Wvn8N6BVCSA8hJAGHA1/uroeTtPP6tmvCA+dlMHf5Bi4ZP5ncfLe9k/TDGGptsjGr5HUbe2qty7VTS5IkSZIkaVuiKCoErqQkbJoNPBNF0awQwqgQwqjSYROB+cA84CHg8u3NLZ1zMzA4hDAXGFx6TBRFa4DbKQnEpgPToij6z+5+Tkk75/B9m/P3M3sz5ds1XPbEVPILi2NdkqQaLCnWBVQb+aWh1jaWH1yTk0/HZhV3cUmSJEmSJAmiKJpISXBV/tz95d5HwBWVnVt6fhWw1V5ZpdceBx7/ASVL2gOO77U363MLuW7CF/zi2RnccWZvEhMq2kpPkrbPUGuTHYRaa3MKaOTyg5IkSZIkSZK0084ZsA/rcgu45dWvaFgniT8M60EIBluSdo6h1ib52SWv5fbUWpdTwN/fnENeQREb8gpp5PKDkiRJkiRJkrRLLjuiE2tz83ng3fk0qZvKzwfvG+uSJNUwhlqblO2pVb/s1EffrGTcRwtpWjeF1o3q0K99kxgVJ0mSJEmSJEk13+gh+7MmO58735pLk/RkLhjUIdYlSapBDLU2KVt+8H+dWmtzCwD4988OoVXDOrGoSpIkSZIkSZJqjRACfz65J2tyCrjx5S9pXDeFYb1bx7osSTVEQqwLqDYq2FNrXWmo1bCOyw5KkiRJkiRJUlVISkzgrrMPpH+HJvzimRm8O2dFrEuSVEMYam1S0Z5auQUkJwbqJCfGqChJkiRJkiRJqn3SkhN5eEQGXVrW57LHp/LZd2tiXZKkGsBQa5ONW3dqrc0poGGdZEIIMSpKkiRJkiRJkmqnBmnJjL+oH83qpXLRuMnMW74h1iVJquYMtTbJz4LEFEhKKTu1PreABi49KEmSJEmSJEm7RYv6aTx2cX8SExI4f8wklqzNjXVJkqoxQ61N8rM269KCkuUHGxlqSZIkSZIkSdJu065pXcZf1I8NeYWcP3YSa7LzY12SpGrKUGuT/OwKQ62GhlqSJEmSJEmStFt137shD43I4LvVOVw4bjLZGwtjXZKkashQa5ONGyB181BrbW6+oZYkSZIkSZIk7QEDOzblrrMP5PPMtVz2xDTyC4tjXZKkasZQa5P8bEipu9mpdTl2akmSJEmSJEnSnnJs9734yyk9eW/OCn757AyKi6NYlySpGkmKdQHVxhZ7ahUXR2zYWGioJUmSJEmSJEl70Jn99mFVdj5/ffVrmtRN4YYTuhFCiHVZkqoBQ61N8rOhXsuyww15hUQRNExPiWFRkiRJkiRJkhR/Lju8E6uy8hnzwQKa1UvhyqO6xLokSdWAodYmG7MgtX7Z4brcAgA7tSRJkiRJkiRpDwsh8NvjurI6O5/bXp9Dk7qpnDNgn1iXJSnGDLU2yc/abE+ttbn5gKGWJEmSJEmSJMVCQkLgr6f1Ym1OPv/3whc0Tk9maM9WsS5LUgwlxLqAamOLPbXs1JIkSZIkSZKk2EpOTODe4X05cJ/GXPXUdD76ZmWsS5IUQ4ZaAIX5UJRvqCVJkiRJkiRJ1UydlETGjMigfbN0Rj46lZmL18W6JEkxYqgFJV1aAKlbh1qN0g21JEmSJEmSJCmWGqWn8OhFA2hYJ5kRYyexYGV2rEuSFAOGWvC/UKvcnlp2akmSJEmSJElS9bFXwzQevbg/EXDemE9Ztj4v1iVJ2sMMtQDyS1P98ssP5hSQkpRAWnJijIqSJEmSJEmSJJXXqXk9xl3YjzXZ+Zw/ZhLrcgpiXZKkPchQC2Djpk6tzZcftEtLkiRJkiRJkqqXXm0a8cB5GcxfmcXF4yeTm18U65Ik7SGGWrDNPbUMtSRJkiRJkiSp+jmkSzP+fmZvpn63hiufnEZBUXGsS5K0BxhqwTb31GpkqCVJkiRJkiRJ1dLxvfbm98N68NZXyxn9/BdEURTrkiTtZkmxLqBaqGhPrdwC9mqQFqOCJEmSJEmSJEk7ct7AdqzOyufvb86hab0Urjuua6xLkrQbGWoBbNxQ8lou1FqbU8B+LevHqCBJkiRJkiRJUmX87OjOrMreyIPvzadp3RQuPbxTrEuStJsYasH/OrXK7am1PreABi4/KEmSJEmSJEnVWgiBG0/ozursfP7yylc0rpvCGRltY12WpN2gUntqhRCGhBC+DiHMCyGMruD6ESGEdSGE6aVf15e7tjCE8EXp+SlVWXyVyc8CAiSnA1BUHLFhYyENDbUkSZIkSZIkqdpLSAjcfkZvDu3SjN/86wve+HJZrEuStBvsMNQKISQC9wBDgW7A2SGEbhUMfT+Kot6lX7/f4tqRpeczfnjJu0F+dsnSgyEAJV1aAI3SDbUkSZIkSZIkqSZISUrgvnP70mPvBlz55DQmLVgd65IkVbHKdGr1B+ZFUTQ/iqJ84Clg2O4taw/buAFS6pYdrisNtezUkiRJkiRJkqSao15qEmMv6EfrxnW4ePxkZi9dH+uSJFWhyoRarYFF5Y4zS89t6aAQwowQwishhO7lzkfA6yGEqSGEkdv6kBDCyBDClBDClBUrVlSq+CqTn73ZflprDbUkSZIkSZIkqUZqWi+VRy/qT92UJM4fO4lFq3NiXZKkKlKZUCtUcC7a4nga0C6KogOAu4AXyl0bFEVRH0qWL7wihHBYRR8SRdGDURRlRFGU0bx580qUVYXys+zUkiRJkiRJkqRaok3jdB69uD/5hcWcO+ZTVmzYGOuSJFWByoRamUDbcsdtgCXlB0RRtD6KoqzS9xOB5BBCs9LjJaWvy4EJlCxnWL3kZ0NK/bJDQy1JkiRJkiRJqtn2bVmfsRf0Y/n6jYwYO4n1eQWxLknSD1SZUGsy0CWE0CGEkAKcBbxUfkAIYa8QQih937/0vqtCCHVDCPVLz9cFjgFmVuUDVIlt7amVbqglSZIkSZIkSTVV33aNuffcPsxZtoGRj04hr6Ao1iVJ+gF2GGpFUVQIXAm8BswGnomiaFYIYVQIYVTpsNOAmSGEGcCdwFlRFEVAS+CD0vOTgP9EUfTq7niQH6R0T62NhUVkrskhc03JGqt2akmSJEmSJElSzXbkfi247fQD+GT+aq566jOKirfcXUdSTZFUmUGlSwpO3OLc/eXe3w3cXcG8+cABP7DG3a90T62Rj07l3TkrAKifmkRqUmKMC5MkSZIkSZIk/VAnHdia1dn5/P7fX/J/L3zBn0/uSeniY5JqkEqFWrVe6Z5aS9bm0qtNQ84d2I5OzevueJ4kSZIkSZIkqUa46JAOrMreyD3vfEPTuqn88tj9Yl2SpJ0Uv6FWFMGqb6Agu6xTK7egiJ6tG3JGRttYVydJkiRJkiRJqmK/PGY/VmXlc/c782hSN4WLDukQ65Ik7YT4DbW++xgeGfq/47rNyCsoIi3FJQclSZIkSZIkqTYKIfDHk3qwJqdkKcImdVM46cDWsS5LUiUlxLqAmFm/pOT1uNvg7KfhwPPIKygmzX20JEmSJEmSJKnWSkpM4B9nHciADk345bMzeOfr5bEuSVIlxW+olZ9d8rrfUNhvCKSkk1tQRJ2U+P2WSJIkSZIkSVI8SEtO5KERGezbsj6XPT6Vqd+uiXVJkiohfhOcTaFWSl0ACoqKKSqOqJNsp5YkSZIkSZIk1XYN0pIZf1F/WjZI46Jxk5mzbEOsS5K0A/EbahWUhlrJJaFWbkERUJLQS5IkSZIkSZJqv+b1U3nsogGkJCVw/phJLF6bG+uSJG1H/IZa+dmQmAJJKQDk5RtqSZIkSZIkSVK82adpOo9e1J/s/ELOG/Mpq7PzY12SpG2I71CrdOlBgLyCYsBQS5IkSZIkSZLiTddWDRgzoh+L1+Ry4SOTyN5YGOuSJFUgvkOt5P+FWpuWH3RPLUmSJEmSJEmKP/07NOHuc/owc8l6Rj0+lY2FRbEuSdIW4jjUytqiU6s01EqJ32+JJEmSJEmSJMWzwd1a8pdTevL+3JX84pkZFBVHsS5JUjlJsS4gZvJzNgu1NnVqpSXZqSVJkiRJkiRJ8eqMjLaszs7n5le+okndFG46sTshhFiXJYm4DrWyKw61Ugy1JEmSJEmSJCmeXXpYR1ZlbeSh9xfQtG4qV/2oS6xLkkRch1pZ0KB12eFGO7UkSZIkSZIkSUAIgd8M7cqq7Hz+/uYcmtRL4byB7WJdlhT34jjUqrhTq46dWpIkSZIkSZIU9xISArec2ou1OQVc/+JMmqSn8ONerWJdlhTXEmJdQMzkZ0NKetlhbn4xAHWSDbUkSZIkSZIkSZCcmMA95/Sh7z6Nufrpz/hg7spYlyTFtfgNtQpyIKVe2WHepuUHk+P3WyJJkiRJkiRJ2lydlETGjOhHx2b1uPSxKcxYtDbWJUlxKz4TnCgq2VOrguUH0+zUkiRJkiRJkiSV0zA9mUcv7k/juimMeGQSc5dtiHVJUlyKz1CrMA+i4s1CrY0FRYQAqUnx+S2RJEmSJEn6oUIIQ0IIX4cQ5oUQRldwPYQQ7iy9/nkIoc+O5oYQmoQQ3gghzC19bbzFPfcJIWSFEH65e59OUrxr2SCNJy4ZQHJiAueO+ZRFq3NiXZIUd+IzwcnPLnktt/xgbkERaUmJhBBiVJQkSZIkSVLNFUJIBO4BhgLdgLNDCN22GDYU6FL6NRK4rxJzRwNvRVHUBXir9Li8vwOvVPkDSVIF2jWty2MX9yevoJhzx3zK8vV5sS5JiitxGmpllbwmp5edyi0ook6KSw9KkiRJkiTtov7AvCiK5kdRlA88BQzbYsww4NGoxCdAoxBCqx3MHQaML30/Hjhp081CCCcB84FZu+eRJGlr++/VgEcu7MeKDRs5f+wk1ubkx7okKW7Eaai1qVPrf8sP5hUUk+bSg5IkSZIkSbuqNbCo3HFm6bnKjNne3JZRFC0FKH1tARBCqAv8Grhpe0WFEEaGEKaEEKasWLFipx5Ikralzz6NefC8DOavyObCcZPJ3lgY65KkuBCfKU5+6VqnWy4/aKeWJEmSJEnSrqpoT4eokmMqM3dLNwF/j6Ioa3uDoih6MIqijCiKMpo3b76DW0pS5R3SpRl3nt2bGYvWMurxqWwsLIp1SVKtF6ehVun/65Tr1NpYUESdZEMtSZIkSZKkXZQJtC133AZYUskx25u7rHSJQkpfl5eeHwD8NYSwELgauC6EcOUPfgpJ2glDerTillN78f7clVz91HQKi4pjXZJUq8VpqLX18oO5BUWkGWpJkiRJkiTtqslAlxBChxBCCnAW8NIWY14Czg8lBgLrSpcU3N7cl4ARpe9HAC8CRFF0aBRF7aMoag/cAfw5iqK7d9/jSVLFTs9oy++O78YrM7/nuglfEEU7ajSVtKuSYl1ATFQUauUXkZ4Sn98OSZIkSZKkHyqKosLSTqnXgERgbBRFs0IIo0qv3w9MBI4D5gE5wIXbm1t665uBZ0IIFwPfAafvwceSpEq5+JAOrMst4M635tIgLZnf/rgrIVS0sqqkHyI+U5wKlh/MKyimSd34bFyTJEmSJEmqClEUTaQkuCp/7v5y7yPgisrOLT2/Cjh6B5974y6UK0lV6pofdWF9bgEPf7CAhnWS+enRXWJdklTrxGeoVZBT8rpZqOXyg5IkSZIkSZKkXRNC4Prju7E+r4C/vTGHBnWSGXFw+1iXJdUq8RlqbVp+MDm97FReQRF1DLUkSZIkSZIkSbsoISHw11N7sSGvkBtemkXDOsmcdGDrWJcl1Rrxud5eflZJoJXwvxAr104tSZIkSZIkSdIPlJSYwF1nH8jBnZryi2dn8OaXy2JdklRrxGmolb1ZlxaUhFp1Ugy1JEmSJEmSJEk/TFpyIg+en0GP1g25/MlpfPzNqliXJNUK8RtqldtPK4oi8gqKSUuKz2+HJEmSJEmSJKlq1UtNYtwF/WjfNJ1Lxk/ms+/WxLokqcaLzxQnPxtS6pUdbiwsBiDNTi1JkiRJkiRJUhVpXDeFxy8eQLP6qYwYO4lZS9bFuiSpRovjUOt/nVq5+UUA1HFPLUmSJEmSJElSFWrRII0nLhlAvdQkzhsziXnLN8S6JKnGMtQC8gpLQq00Qy1JkiRJkiRJUhVr0zidJ34ykMSEwDkPfcq3q7JjXZJUI1Uq1AohDAkhfB1CmBdCGF3B9SNCCOtCCNNLv66v7NyYsFNLkiRJkiRJkrQHdWhWl8cvHkBBUTHnPPQpS9bmxrokqcbZYagVQkgE7gGGAt2As0MI3SoY+n4URb1Lv36/k3P3rPyszTu1Ckr31DLUkiRJkiRJkiTtJvvtVZ9HLxrA+twChj/8Kcs35MW6JKlGqUynVn9gXhRF86MoygeeAoZV8v4/ZO7uU5CzeadWwablB+NzNUZJkiRJkiRJ0p7Rs01Dxl3Uj+/X5XHew5NYk50f65KkGqMyKU5rYFG548zSc1s6KIQwI4TwSgih+07OJYQwMoQwJYQwZcWKFZUo6wfYck+tApcflCRJkiRJkiTtGX3bNeHhERksWJXN+WMnsT6vINYlSTVCZUKtUMG5aIvjaUC7KIoOAO4CXtiJuSUno+jBKIoyoijKaN68eSXK2kXFRaWdWvXKTuWVdWoZakmSJEmSJEmSdr9BnZtx/7l9mL10PRc9Mpmc/MJYlyRVe5UJtTKBtuWO2wBLyg+Iomh9FEVZpe8nAskhhGaVmbvHFeSUvFaw/GCdFEMtSZIkSZIkSdKecdT+LfnHWQcy7bs1jHx0alkDhqSKVSbUmgx0CSF0CCGkAGcBL5UfEELYK4QQSt/3L73vqsrM3ePys0tek9PLTuUVFAMuPyhJkiRJkiRJ2rN+3KsVfz3tAD6Yt5Irn5xGQVFxrEuSqq0dhlpRFBUCVwKvAbOBZ6IomhVCGBVCGFU67DRgZghhBnAncFZUosK5u+NBKm1TqFVu+cFNnVqpyZXJ+CRJkiRJkiRJqjqn9W3DH07qwZuzl3P109MpNNiSKpRUmUGlSwpO3OLc/eXe3w3cXdm5MVUWav1v+cG8/NLlB+3UkiRJkiRJkiTFwHkD25GbX8ifJ35FckLgb2f0JjEhxLosqVqpVKhVq1QUapV2aqUZakmSJEmSJEmSYmTkYZ0oKIq49bWvSUxI4NbTepFgsCWVieNQa/PlB5MSAsmJLj8oSZIkSZIkSYqdK47sTGFRxN/fnENSQuAvp/Q02JJKxWGolVXympJediq3oMilByVJkiRJkiRJ1cJVP+pCYXExd709j8TEwJ9O6kEIBltS/IVaBTklr5stP1hMqqGWJEmSJEmSJKma+PngfSksjrjvv9+QlBC46cTuBluKe/EXalWw/GBeQRF1Ulx6UJIkSZIkSZJUPYQQ+NWx+1FYVMxD7y8gMSFw/fHdDLYU1+Iw1Nq0/GD5Tq0i0pLs1JIkSZIkSZIkVR8hBK47riuFxRGPfLiQpISSY4Mtxas4DLWyISRAUlrZqdyCIuqkGGpJkiRJkiRJkqqXEEo6tIqKIx56fwFJiQn86tj9DLYUl+Iz1EquC+V+w+fmF5HmnlqSJEmSJEmSpGoohMCNJ3Qv22MrOSHw82P2i3VZ0h4Xn6FWuaUHAfIKi2lYJzlGBUmSJEmSJEmStH0JCYE/DutBUVHEnW/PIzEhgat+1CXWZUl7lKEWkJdfxF4NUmNUkCRJkiRJkiRJO5aQEPjLKT0pLI74+5tzSEoMXHFk51iXJe0xhlpAXmERdVx+UJIkSZIkSZJUzSUkBP56Wi+Kiou59bWvAQy2FDfiMNTKgpR6m51yTy1JkiRJkiRJUk2RmBC47fQDiIBbX/uaouKInx3tUoSq/eIw1MqG9KabncotMNSSJEmSJEmSJNUcSYkJ3H5GbxJD4PY35lBUHHH1j7oQQoh1adJuE3+hVkEOpLTd7NTGgmJDLUmSJEmSJElSjZKYELj19ANISAj84625FEcRPx+8r8GWaq24C7Wi/CzWFqawKHMtAMUR5BcVu6eWJEmSJEmSJKnGSUwI/PXUXiQlBO56ex5FxRHXHrufwZZqpbgLtfKyN/Dil+u48fMPNzvfuG5yjCqSJEmSJEmSJGnXJSQE/nxyTxISAvf+9xuKiiNGD93fYEu1TtyFWslFORQk1mHM8Iyyc4kJgYEdm25nliRJkiRJkiRJ1VdCQuBPJ/UgMQQeeG8+RcURv/1xV4Mt1SrxFWoV5pMUFVCQmM7RXVvGuhpJkiRJkiRJkqpMCIHfD+tOYkLg4Q8WUBRFXH98N4Mt1RrxFWoVZJe8JNSJcSGSJEmSJEmSJFW9EAI3nNCNxITAmA8WUFQccdOJ3Q22VCvEV6iVn1PykmioJUmSJEmSJEmqnUII/N+Pu5KUULIUYX5hMX86uSeJCQZbqtniLNTa1KmVHuNCJEmSJEmSJEnafUIIjB66P6lJCdz59jxyC4q47fQDSE5MiHVp0i6Ls1ArC4CCJDu1JEmSJEmSJEm1WwiBnx+zH3VSkrjl1a/IyS/i7nMOJDUpMdalSbskviLZ0k6twkQ7tSRJkiRJkiRJ8eGyIzpx04ndeePLZVwyfgq5+UWxLknaJXEZahXZqSVJkiRJkiRJiiMjDm7PX0/rxYfzVjJi7CQ25BXEuiRpp8VXqFWwKdSqG+NCJEmSJEmSJEnas87IaMs/zjqQad+t4dyHP2VtTn6sS5J2SnyFWqWdWsXJLj8oSZIkSZIkSYo/JxywN/ed25fZSzdw1oOfsGLDxliXJFVafIZadmpJkiRJkiRJkuLU4G4tGXtBP75dlcOZD3zM0nW5sS5JqpQ4C7WyAIhS7NSSJEmSJEmSJMWvQ7o049GL+7N8w0ZOv/9jFq7MjnVJ0g7FWaiVTQFJJCSlxroSSZIkSZIkSZJiql/7Jjz5kwFkbyzktPs/ZtaSdbEuSdquOAu1csghjZSk+HpsSZIkSZIkSZIq0qtNI54ddRDJiYGzHviET+evinVJ0jbFV7qTn00OqaQkxtdjS5IkSZIkSZK0LZ1b1Of5yw6mRYNUzh87iTe+XBbrkqQKxVe6k59FdmSnliRJkiRJkiRJ5e3dqA7PjjqY/Vs1YNTjU3l2yqJYlyRtJb7SnfzsklDLTi1JkiRJkiRJkjbTpG4KT14ygIM7NeXa5z7ngXe/iXVJ0mbiKt2J8rPIjlJJNtSSJEmSJEmSJGkrdVOTeHhEBj/u1Yq/vPIVf5k4myiKYl2WBFQy1AohDAkhfB1CmBdCGL2dcf1CCEUhhNPKnVsYQvgihDA9hDClKoreVVF+TsmeWi4/KEmSJEmSJElShVKTErnzrAM5d+A+PPDefH79/OcUFhXHuiyJpB0NCCEkAvcAg4FMYHII4aUoir6sYNwtwGsV3ObIKIpWVkG9P0iUn0UOLQy1JEmSJEmSJEnajsSEwB+G9aBJ3VTufGsua3MKuPPsA0lLTox1aYpjlUl3+gPzoiiaH0VRPvAUMKyCcT8FngeWV2F9VatsT60Q60okSZIkSZIkSarWQgj8fPC+3HhCN96YvYzhD3/K6uz8WJelOFaZUKs1sKjccWbpuTIhhNbAycD9FcyPgNdDCFNDCCN3tdCqEPKzySHNTi1JkiRJkqTdYEdbWIQSd5Ze/zyE0GdHc0MITUIIb4QQ5pa+Ni49P7j0501flL4etWeeUpLizwWDOnDPOX34YvE6Tr3vI75blRPrkhSnKpPuVNTWtOWucHcAv46iqKiCsYOiKOoDDAWuCCEcVuGHhDAyhDAlhDBlxYoVlShrJ0URoSCbbFJJTjTUkiRJkiRJqkrltrAYCnQDzg4hdNti2FCgS+nXSOC+SswdDbwVRVEX4K3SY4CVwAlRFPUERgCP7aZHkyQBx/VsxZOXDGBNTj4n3/sh0xetjXVJikOVSXcygbbljtsAS7YYkwE8FUJYCJwG3BtCOAkgiqIlpa/LgQmULGe4lSiKHoyiKCOKoozmzZvvzDNUTuFGQlRMbmSnliRJkiRJ0m5QmS0shgGPRiU+ARqFEFrtYO4wYHzp+/HASQBRFH226edOwCwgLYSQupueTZIEZLRvwvOXHUx6aiJnPfgxb365LNYlKc5UJt2ZDHQJIXQIIaQAZwEvlR8QRVGHKIraR1HUHngOuDyKohdCCHVDCPUBQgh1gWOAmVX6BJWVnw1gp5YkSZIkSdLuscMtLLYzZntzW0ZRtBSg9LVFBZ99KvBZFEUbt7yw21cHkqQ406l5Pf512SD2bVmfkY9N4bFPvo11SYojO0x3oigqBK4EXgNmA89EUTQrhDAqhDBqB9NbAh+EEGYAk4D/RFH06g8tepfkZwG4p5YkSZIkSdLuUZktLLY1pjJzK/7QELoDtwCXVnR9t68OJElxqHn9VJ4aOZAj92vB716Yyc2vfEVxcaX+2JZ+kKTKDIqiaCIwcYtz929j7AXl3s8HDvgB9VWdTZ1aURqpdmpJkiRJkiRVtcpsYbGtMSnbmbsshNAqiqKlpUsVLt80KITQhpLtLs6PouibKnkKSVKlpKck8cB5fbnx5Vnc/+43LFmby62n9yI1KTHWpakWi590pzTUyiGNZDu1JEmSJEmSqtoOt7AoPT4/lBgIrCtdUnB7c18CRpS+HwG8CBBCaAT8B/hNFEUf7sbnkiRtQ1JiAn8Y1oNfD9mfl2Ys4fwxk1ibkx/rslSLxU+6U1AaakWppNipJUmSJEmSVKUquYXFRGA+MA94CLh8e3NL59wMDA4hzAUGlx5TOr4z8LsQwvTSr4r225Ik7UYhBC47ohP/OKs3n323lpPv/Yj5K7JiXZZqqUotP1grlO/UMtSSJEmSJEmqcjvawiKKogi4orJzS8+vAo6u4PwfgT/+wJIlSVVkWO/W7N2oDpc+NpWT7/2I+4b34eDOzWJdlmqZ+El3Nu2pRSopLj8oSZIkSZIkSVKV6te+CS9cPogW9VM5f+wknvz0u1iXpFomftKd/JJ2x+wojVRDLUmSJEmSJEmSqtw+TdN5/vKDGdS5GddN+ILfv/wlRcVRrMtSLRE/6Y7LD0qSJEmSJEmStNs1SEtmzIgMLji4PWM/XMAl4yezIa8g1mWpFoifdCc/B4Bclx+UJEmSJEmSJGm3SkpM4MYTu/OHk3rw3tyVnHrfR3y7KjvWZamGi590Jz+LwoQ0ikkgOTHEuhpJkiRJkiRJkmq98wa2Y/yF/Vm2fiMn3v0h789dEeuSVIPFUaiVTX5iHQA7tSRJkiRJkiRJ2kMO6dKMl64cxF4N0hgxdhIPvTefKHKfLe28+El38rMpSCgJtZIT4uexJUmSJEmSJEmKtXZN6/Kvyw/mmG578aeJs7nm6enkFRTFuizVMPGT7uRnsTExneTEQEKCyw9KkiRJkiRJkrQn1U1N4t7hffjF4H15ccYSTrv/IxavzY11WapB4ifUKsghP6SRkhg/jyxJkiRJkiRJUnWSkBD46dFdeOi8DBauzOHEuz7gk/mrYl2Waoj4SXjys9mYkEay+2lJkiRJkiRJkhRTP+rWkheuGETD9GSGP/wpD773jftsaYfiJ+Gp34oVKW3s1JIkSZIkSZIkqRro3KIeL14xiMFdW/LniV8x6vGprM8riHVZqsbiJ+E5YzzPtLyGZEMtSZIkSZIkSZKqhfppydx3bh/+78ddeXP2ck686wNmL10f67JUTcVVwlNQFJHq8oOSJEmSJEmSJFUbIQQuObQjT40cSE5+ESff+yHPTc2MdVmqhuIq4ckvLLJTS5IkSZIkSZKkaqhf+yb852eHcmDbxvzy2RmMfv5z8gqKYl2WqpG4SngKiiJS7NSSJEmSJEmSJKlaal4/lccu7s/lR3TiqcmLOPnej5i3PCvWZamaiKuEJ7+w2FBLkiRJkiRJkqRqLCkxgV8N2Z+xF2SwbH0eJ9z1Ac9MWUQURbEuTTEWVwlPflExyYkh1mVIkiRJkiRJkqQdOGr/lkz82aEc0LYhv3ruc65+ejob8gpiXZZiKL5CrcJiUpISY12GJEmSJEmSJEmqhL0apvHEJQP5xeB9eXnGEo6/6wM+z1wb67IUI/EXatmpJUmSJEmSJElSjZGYEPjp0V14+tKDKCgs5tT7PuLh9+dTXOxyhPEmrkKtgiL31JIkSZIkSZIkqSbq174JE686lCP3a8Ef/zOb88dOYum63FiXpT0orhKe/KJiUhLj6pElSZIkSZIkSao1GqWn8MB5ffnTyT2Y+u0ajv37e7w4fXGsy9IeElcJT0FhMcmGWpIkSZIkSZIk1VghBIYPaMcrVx1K5xb1uOqp6Vz55DTW5uTHujTtZnGV8OS7/KAkSZIkSZIkSbVC+2Z1eebSg7j22P14deb3HHvHe7w3Z0Wsy9JuFFcJT76dWpIkSZIkSZIk1RpJiQlccWRnXrhiEPXTkjl/7CSuf3Em2RsLY12adoO4Snjyi4pJtVNLkiRJkiRJkqRapUfrhvz7p4dw0aAOPPrxtxx7x3t8MHdlrMtSFYurhMdOLUmSJEmSJEmSaqe05ESuP6Ebz446iJTEBM4d8ym/em4G63ILYl2aqkjcJDxFxRHFEe6pJUmSJEmSJElSLdavfRMmXnUoow7vxPPTFjP49nd5fdb3sS5LVSBuEp78wmLAUEuSJEmSJEmSpNouLTmR0UP354XLB9GkbgojH5vKlU9OY2XWxliXph8gbhKe/KKSUMvlByVJkiRJkiRJig892zTkpSsP4eeD9+W1Wd9z9N/e5clPv6O4OIp1adoFcZPw2KklSZIkSZIkSVL8SUlK4GdHd2Hizw5l/73qc92ELzjlvo+YuXhdrEvTToqbhKegtFMrJTHEuBJJkiRJkiRJkrSndWlZn6dGDuT2Mw4gc00OJ979ATe+NIsNeQWxLk2VFDehlp1akiRJkiRJkiTFtxACp/Rpw1s/P4LhA9ox/uOFHP23d3lpxhKiyCUJq7tKJTwhhCEhhK9DCPNCCKO3M65fCKEohHDazs7d3dxTS5IkSZIkSZIkATRMT+YPJ/XghcsH0bJBGj/752ec+eAnLklYzSXtaEAIIRG4BxgMZAKTQwgvRVH0ZQXjbgFe29m5e0JZp5ahlrTbFBQUkJmZSV5eXqxLkVSF0tLSaNOmDcnJybEuRZIkSZIkqUod0LYRL1wxiH9O+o7b35jDCXd/wKl92nDtsfvRskFarMvTFnYYagH9gXlRFM0HCCE8BQwDtgymfgo8D/Tbhbm73aZOLZcflHafzMxM6tevT/v27QnB/euk2iCKIlatWkVmZiYdOnSIdTmSJEmSJElVLjEhcO7AdpzYe2/ueXsej3y4kIlfLGXU4Z34yaEdqZOSGOsSVaoyCU9rYFG548zSc2VCCK2Bk4H7d3bunlJgp5a02+Xl5dG0aVMDLakWCSHQtGlTOzAlSZIkSVKt1yAtmd8c15U3fn4Yh+/bnNvfmMNRf/svL3y2mOJi99uqDiqT8FT00+ktf/XuAH4dRVHRLswtGRjCyBDClBDClBUrVlSirJ1jp5a0ZxhoSbWPv68lSZIkSVI8ade0Lved25enRw6kab0Urn56Oj++6wPemr2MKDLciqXKJDyZQNtyx22AJVuMyQCeCiEsBE4D7g0hnFTJuQBEUfRgFEUZURRlNG/evHLV74SC0lAr2U4tSZIkSZIkSZK0AwM6NuWlKw7hjjN7k5NfyMXjp3DKfR/x0TcrY11a3KpMwjMZ6BJC6BBCSAHOAl4qPyCKog5RFLWPoqg98BxweRRFL1Rm7p6SX2inlqRdV1BQQN++fbd5/Y477iAnJ2en71uvXr1drmncuHEsWVLhvxOoNl599VX2228/OnfuzM0331zhmFtvvZXevXvTu3dvevToQWJiIqtXr2bRokUceeSRdO3ale7du/OPf/xjs3l33XUX++23H927d+dXv/oVAAsXLqROnTpl9xs1alTZ+Pz8fEaOHMm+++7L/vvvz/PPPw/A7bffTrdu3ejVqxdHH3003377bdmcxMTEsnudeOKJZecvuOACOnToUHZt+vTpAKxZs4aTTz6ZXr160b9/f2bOnFk2Z+3atZx22mnsv//+dO3alY8//hiA1atXM3jwYLp06cLgwYNZs2bNZs/53XffUa9ePW677bYdPssmzz33HCEEpkyZstn59evX07p1a6688sqyc2+99RZ9+vShd+/eHHLIIcybN6/CXydJkiRJkqR4lJAQOOnA1rz588P588k9Wbo2j3Me+pRzH/6U6YvWxrq8uJO0owFRFBWGEK4EXgMSgbFRFM0KIYwqvb7lPlo7nFs1pe+cjYZakn6ADz74gIMPPnib1++44w7OPfdc0tPT91hN48aNo0ePHuy999577DN3RlFREVdccQVvvPEGbdq0oV+/fpx44ol069Zts3HXXnst1157LQAvv/wyf//732nSpAkbN27kb3/7G3369GHDhg307duXwYMH061bN9555x1efPFFPv/8c1JTU1m+fHnZ/Tp16lQWMpX3pz/9iRYtWjBnzhyKi4tZvXo1AAceeCBTpkwhPT2d++67j1/96lc8/fTTANSpU6fCe0FJGHfaaadtdu7Pf/4zvXv3ZsKECXz11VdcccUVvPXWWwBcddVVDBkyhOeee478/PyyEPTmm2/m6KOPZvTo0dx8883cfPPN3HLLLWX3vOaaaxg6dGilngVgw4YN3HnnnQwYMGCrmn/3u99x+OGHb3busssu48UXX6Rr167ce++9/PGPf2TcuHEVPrMkSZIkSVK8Sk5M4JwB+3BKn9Y8/sm33Pvfbzjpng/5UdcWXHlUF3q3bRTrEuPCDkMtgCiKJgITtzhXYZgVRdEFO5obCwVFJetcprj8oLRH3PTyLL5csr5K79lt7wbccEL3bV5fuHAhQ4YM4ZBDDuGTTz7hgAMO4MILL+SGG25g+fLlPPHEE/Tv359JkyZx9dVXk5ubS506dXjkkUfYb7/9uP3225k5cyZjx47liy++4Oyzz2bSpEmkp6fz6quvMnToULKzsznjjDPIzMykqKiI3/3udyxbtowlS5Zw5JFH0qxZM9555x3q1atHVlYWUNI18+9//5tx48axYMECzjnnHAoLCxkyZMhm9d96660888wzbNy4kZNPPpmbbrqJhQsXMnToUA455BA++ugjWrduzYsvvsh//vMfpkyZwvDhw6lTpw4ff/wxderU2ep78vvf/56XX36Z3NxcDj74YB544AFCCMybN49Ro0axYsUKEhMTefbZZ+nUqRN//etfeeyxx0hISGDo0KHb7K6qjEmTJtG5c2c6duwIwFlnncWLL764VahV3j//+U/OPvtsAFq1akWrVq0AqF+/Pl27dmXx4sV069aN++67j9GjR5OamgpAixYtdljP2LFj+eqrrwBISEigWbNmABx55JFlYwYOHMjjjz++C09b4ssvv+Q3v/kNAPvvvz8LFy5k2bJl1KlTh/fee68sLEpJSSElJQWAF198kf/+978AjBgxgiOOOKIs1HrhhRfo2LEjdevWrdSzQElw9atf/Wqzzi6AqVOnsmzZMoYMGbJZB1cIgfXrS36vrlu3rtqGpJIkSZIkSdVBWnIilxzakbP678MjHyzg4Q8WcNI9HzKoc1MuP6IzB3dq6v7ku1HcJDwuPyjFh3nz5nHVVVfx+eef89VXX/Hkk0/ywQcfcNttt/HnP/8ZKAkb3nvvPT777DN+//vfc9111wFw9dVXM2/ePCZMmMCFF17IAw88UNZ59c4773DEEUfw6quvsvfeezNjxgxmzpzJkCFD+NnPfsbee+/NO++8wzvvvLPd+q666iouu+wyJk+ezF577VV2/vXXX2fu3LlMmjSJ6dOnM3XqVN577z0A5s6dyxVXXMGsWbNo1KgRzz//PKeddhoZGRk88cQTTJ8+vcJAC+DKK69k8uTJzJw5k9zcXP79738DMHz4cK644gpmzJjBRx99RKtWrXjllVd44YUX+PTTT5kxY0bZkn7lPfHEE2VL7pX/2rJjCWDx4sW0bfu/bRXbtGnD4sWLt/m9ycnJ4dVXX+XUU0/d6trChQv57LPPyrqP5syZw/vvv8+AAQM4/PDDmTx5ctnYBQsWcOCBB3L44Yfz/vvvAyVL/0FJ4NOnTx9OP/10li1bttXnjBkzZrOuqLy8PDIyMhg4cCAvvPDCZmN/+9vf0qtXL6655ho2btwIwAEHHMC//vUvoCTU+/bbb8nMzGT+/Pk0b96cCy+8kAMPPJBLLrmE7OxsAJYtW1YW3rVq1aqs6yw7O5tbbrmFG264YbPP3d6zfPbZZyxatIjjjz9+sznFxcX84he/4NZbb93qmR9++GGOO+442rRpw2OPPcbo0aO3GiNJkiRJkqTN1UtN4qdHd+HD0Udx3XH7M3dZFsMf/pST7v2I12d9T3FxFOsSa6VKdWrVBgVFJaFWsp1a0h6xvY6q3alDhw707NkTgO7du3P00UcTQqBnz54sXLgQKOlGGTFiBHPnziWEQEFBAVDS8TJu3Dh69erFpZdeyqBBgwBYsmQJTZo0IT09nZ49e/LLX/6SX//61xx//PEceuihO1Xfhx9+WLb/0Xnnncevf/1roCTUev311znwwAMByMrKYu7cueyzzz5lezcB9O3bt+w5KuOdd97hr3/9Kzk5OaxevZru3btzxBFHsHjxYk4++WQA0tLSAHjzzTe58MILy4K8Jk2abHW/4cOHM3z48Ep9dhRt/Rf39v6Vyssvv8ygQYO2+tysrCxOPfVU7rjjDho0aABAYWEha9as4ZNPPmHy5MmcccYZzJ8/n1atWvHdd9/RtGlTpk6dykknncSsWbMoLCwkMzOTQYMGcfvtt3P77bfzy1/+kscee6zscx5//HGmTJnCu+++W3buu+++Y++992b+/PkcddRR9OzZk06dOvGXv/yFvfbaq2xvq1tuuYXrr7+e0aNHc9VVV9G7d2969uzJgQceSFJSEgUFBUybNo277rqLAQMGcNVVV3HzzTfzhz/8YZvfjxtuuIFrrrlmq33XtvUs48eP55prrqlw6cB7772X4447brOQcZO///3vTJw4kQEDBnDrrbfy85//nIcffnibdUmSJEmSJOl/6qUmMfKwTpx/UHuen5bJ/e9+w8jHprJvy3qMPKwTx/dqRVpyYqzLrDXiJtSyU0uKD5uWo4OSkGrTcUJCAoWFhUBJh8uRRx7JhAkTWLhwIUcccUTZnLlz51KvXj2WLFlSdu6VV17h2GOPBWDfffdl6tSpTJw4kd/85jccc8wxXH/99VvVUT68ycvL2+a1TaIo4je/+Q2XXnrpZucXLly42TMlJiaSm5u7w+/Dps+9/PLLmTJlCm3btuXGG28kLy+vwrBpUw07ao1+4oknKuz26dy5M88999xm59q0acOiRYvKjjMzM7e7tN1TTz1VtvTgJgUFBZx66qkMHz6cU045ZbN7n3LKKYQQ6N+/PwkJCaxcuZLmzZuXfb/69u1Lp06dmDNnDn379iU9Pb0syDv99NMZM2ZM2f3efPNN/vSnP/Huu+9u9v3eVG/Hjh054ogj+Oyzz+jUqVNZZ1VqaioXXnhh2VJ/DRo04JFHHgFKvp8dOnSgQ4cO5OTk0KZNm7JOs9NOO61saceWLVuydOlSWrVqxdKlS8uWUvz000957rnn+NWvfsXatWtJSEggLS2NK664osJn2bBhAzNnziz77/n777/nxBNP5KWXXuLjjz/m/fff59577yUrK4v8/Hzq1avHL37xC2bMmFFW15lnnrnVspiSJEmSJEnasbTkRIYPaMeZGW35zxdLufedb/jlszP4y8TZnDNgH4YPaMdeDdNiXWaNFzcJT35Zp5ZrWUrxbt26dbRu3Rpgs66WdevWcdVVV/Hee++xatWqspBm035aUNK1lZ6ezrnnnssvf/lLpk2bBpTs+bRhw4aye7Vs2ZLZs2dTXFzMhAkTys4PGjSIp556CigJiDY59thjGTt2bNk+XIsXLy5bhm5btvzMLW0K05o1a0ZWVlbZ8zRo0IA2bdqULae3ceNGcnJyOOaYYxg7diw5OTkArF69eqt7Dh8+nOnTp2/1tWWgBdCvXz/mzp3LggULyM/P56mnnuLEE0+ssNZ169bx7rvvMmzYsLJzURRx8cUX07VrV37+859vNv6kk07i7bffBkqWIszPz6dZs2asWLGCoqIiAObPn8/cuXPp2LEjIQROOOGEsr2r3nrrrbK9vT777DMuvfRSXnrppc325lqzZk3ZsoIrV67kww8/LJuzdOnSshpfeOEFevToAZQsDZifnw+ULOt32GGH0aBBA/baay/atm3L119/vdXnn3jiiYwfPx6A8ePHl30P3n//fRYuXMjChQu5+uqrue6667jyyiu3+SwNGzZk5cqVZXMGDhzISy+9VLZM5XfffcfChQu57bbbOP/887n55ptp3Lgx69atY86cOQC88cYbdO3atcJfI0mSJEmSJO1YUmICw3q35tWrD+WJSwZw4D6NufudeRxyy9tc+eQ0pn67epv/6Fw7Fn+dWi4/KMW9X/3qV4wYMYLbb7+do446quz8Nddcw+WXX86+++7LmDFjOPLIIzn00EOZO3cu+++/PwBffPEF1157LQkJCSQnJ3PfffcBMHLkSIYOHUqrVq145513uPnmmzn++ONp27YtPXr0KAur/vGPf3DOOefwj3/8Y7O9o4455hhmz57NQQcdBEC9evV4/PHHSUzcdmvyBRdcwKhRo6hTpw4ff/zxVvtqNWrUiJ/85Cf07NmT9u3b069fv7Jrjz32GJdeeinXX389ycnJPPvsswwZMoTp06eTkZFBSkoKxx13XNk+ZLsiKSmJu+++m2OPPZaioiIuuugiuncvWZby/vvvB2DUqFEATJgwgWOOOYa6deuWzf/www957LHH6NmzZ9nyi3/+85857rjjuOiii7jooovo0aMHKSkpjB8/nhAC7733Htdffz1JSUkkJiZy//33ly1neMstt3Deeedx9dVX07x587KOqmuvvZasrCxOP/10APbZZx9eeuklZs+ezaWXXkpCQgLFxcWMHj26LIgaPnw4K1asIIoievfuXfY8s2fP5vzzzycxMZFu3bpt1g121113MXz4cPLz8+nYsWPZ548ePZozzjiDMWPGsM8++/Dss8/u8Hu7rWfZlV+jhx56iFNPPZWEhAQaN27M2LFjd+lekiRJkiRJ+p8QAoM6N2NQ52Z8tyqHRz9eyNNTFvHvz5fSs3VDzhmwD8f3akX9tORYl1qjhOqYCGZkZERTpkyp0nve8upXjHl/AXP+NLRK7yvpf2bPnl3rujw++OADHn/88bLQQopXFf3+DiFMjaIoI0YlSZIkSTttd/zMSZKkysreWMiEzxbz6McLmbMsi7TkBI7r0YrTMtowsENTEhJcaW6Tbf3cKW46tQoKi116UNJOO+SQQzjkkENiXYYkSZIkSZKkGq5uahLnDmzH8AH7MCNzHc9MWcTL05fwr88W07ZJHU7t04bT+rahTeP0WJdabcVNqJVfVExKkksPSqqdTj75ZBYsWLDZuVtuuYVjjz02RhVJkiRJkiRJqkgIgd5tG9G7bSOuP74br836nmemLOIfb83ljjfn0mefRhzXsxXH9WzF3o3q7PiGcSRuQq2ComKS3U9LUi01YcKEWJcgSZIkSZIkaSelJScyrHdrhvVuTeaaHF6cvoT/fL6UP/5nNn/8z2x6t23Ej3u2YmjPvezgIo5CrY2FdmpJkiRJkiRJkqTqqU3jdK44sjNXHNmZBSuzmfjFUl6ZuZQ/TZzNnybOplebhhyxb3MO3685B7RpRFIcNvLETahVUBSREoe/wJIkSZIkSZIkqWbp0KxuWcD17apsJn7xPW/OXsbd78zjzrfn0SAtiUO7NOfwfZtz2L7N2athWqxL3iPiJtTKLyyyU0uSJEmSJEmSJNUo7ZrW5bIjOnHZEZ1Yl1PAB/NW8u6c5bw7ZwX/+WIpAJ2a1yWjXRMy2jcmo30T2jdNJ4QQ48qrXhyFWi4/KEmSJEmStDuFEIYA/wASgYejKLp5i+uh9PpxQA5wQRRF07Y3N4TQBHgaaA8sBM6IomhN6bXfABcDRcDPoih6bTc/oiRJMdUwPZkf92rFj3u1Iooivl62gXe/XsGnC1bz6qzveXrKIgCa1Uuhzz6NyWjfmJ6tG9Ft7wY0rJMc4+p/uLhJeQqKIpJdflDSLiooKKBv376sXbuWe++9t8rvf+ONN3LbbbdV+X1j7dVXX2W//fajc+fO3HzzzRWOufXWW+nduze9e/emR48eJCYmsnr1avLy8ujfvz8HHHAA3bt354Ybbthq7m233UYIgZUrVwIwadKksnsdcMABTJgwAYANGzaUne/duzfNmjXj6quvBmDcuHE0b9687NrDDz9cdv/x48fTpUsXunTpwvjx48vOX3DBBXTo0KFszvTp0wFYs2YNJ598Mr169aJ///7MnDmzbM7atWs57bTT2H///fn/9u4/qsoq3+P4e/NDENGU8WdiE5a/NS01K8mh8aLgJI7VVA6ruHZXWuoataYyZ6nlpKFOesccx2qkHDPNH6PSrDLQNHKWymjpzdSCEhUlIFH8AYjAvn9weAYSSBM7nMPntRbLczbP85zv9+zzeM75bvZ+unXrxo4dOwDIy8sjMjKSTp06ERkZyalTp6rkePToUYKDg6u8PoqLixkzZgydO3ema9eurFu3rso+a9euxRjD7t27q7SfOXOG9u3bM2HCBKdty5Yt3HbbbfTp04fw8HDS09Or7ScRERERkR9ijPEF/gJEA92BUcaY7t/bLBro5PoZA/z1MvadAmyx1nYCtrju4/r9w0APIApY7DqOiIhIg2CMoWvbZoz9xU0k/Hd/PpsWSfLkQcwe2YtBnVvxZfZZZr9/iFFv7KT3i0mEz/mIMX/fzYLkr0jcd4IvTuRTWFzq7jSuSMOaqaVBLRH5kbZv385dd93lDGqNGzfO3SFVUVpaiq9v/fruVlpayvjx40lOTiY0NJT+/fsTExND9+5Vv9M+88wzPPPMMwC89957LFiwgJCQEKy1fPTRRwQHB3Px4kXCw8OJjo7mjjvuAODYsWMkJydzww03OMfq2bMnu3fvxs/Pj6ysLHr37s3w4cNp2rSpM/AE0LdvX+677z7n/kMPPcSiRYuqxJWXl8eLL77I7t27McbQt29fYmJiaNGiBVA+GPfAAw9U2Wf27Nn06dOH9evXc+jQIcaPH8+WLVsAmDhxIlFRUaxdu5bi4mIKCgoAiI+PZ/DgwUyZMoX4+Hji4+OZM2eOc8zJkycTHR1d5XFmzZpF69at+eqrrygrKyMvL8/53dmzZ1m4cCEDBgy4pE+mTZvGL37xiyptTz75JBs3bqRbt24sXryYl156ibfeeuuSfUVERERELsPtQLq19hsAY8wqYARwoNI2I4C/W2stsNMY09wY047yWVg17TsCiHDtvwzYBjznal9lrb0AHDbGpLti2HENcxQREam3fHwMndo0pVObpvx2QHnNLPfsBb44kc+BrDMcOFH+k3wwG2v/s1/75o3pENKY65s3JrR5Y9q3aEy76xrzs+BGtAwOIKRJo3ozaajhDGqVltGskedPrRPxGB9MgW8/r9tjtu0F0dXP9gHIyMggKiqK8PBwdu7cSe/evRk9ejQzZswgJyeHFStWcPvtt5OamsqkSZMoLCykcePGvPnmm3Tp0oX58+ezf/9+EhIS+Pzzzxk1ahSpqakEBQWxadMmoqOjmTJlCl9//TV9+vQhMjKSefPmMW/ePFavXs2FCxcYOXIkL774IgC//vWvOXbsGEVFRUycOJExY8YA5bOXpk6dSmlpKS1btnQGPQ4cOEBERARHjx5l0qRJ/O53vwPg7bffZuHChRQXFzNgwAAWL16Mr68vwcHBPPXUU3z44Ye88sorhIeHX/KczJw5k/fee4/CwkLuuusuXnvtNYwxpKen88QTT5Cbm4uvry9r1qzhpptuYu7cuSxfvhwfHx+io6NrnF11OVJTU7n55pvp2LEjAA8//DAbN268ZFCrspUrVzJq1Cig/C9NgoODgfKZchcvXqyyDvDkyZOZO3cuI0aMcNqCgoKc20VFRdWuG5yWlkZOTg533313rfF/+OGHREZGEhISAkBkZCSbNm1y4qvOgQMHeP755wHo2rUrGRkZZGdn07hxY1JSUpzBokaNGtGoUSMANm7cyLZt2wCIi4sjIiLCGdTasGEDHTt2pEmTJlUeJyEhgUOHDgHg4+NDy5Ytnd9NmzaNZ5999pKZf3v27CE7O5uoqKgqM7iMMZw5cwaA/Px8rr/++lqfFxERERGRWrQHjlW6nwl8/6+tqtum/Q/s28ZamwVgrc0yxrSudKyd1RxLREREXFo1DSCiS2siurR22ooulpJx8jxf55znm9xzfJ17jsxThez4+iTZZ4oos5ce57rG/jQN9CM4oPynSYAfLYL8+d+Hb/0Js2lAg1qhLRrTIqiRu8MQkWssPT2dNWvW8Prrr9O/f3/eeecdtm/fTmJiIrNnz2bDhg107dqVlJQU/Pz82Lx5M1OnTmXdunVMmjSJiIgI1q9fz6xZs3jttdecQZKtW7cyY8YMunfvzv79+51ZP0lJSaSlpZGamoq1lpiYGFJSUhg0aBAJCQmEhIRQWFhI//79uf/++ykrK+Pxxx8nJSWFsLCwKjNsDh06xNatWzl79ixdunThySefJD09nXfffZd//etf+Pv7M27cOFasWMGjjz7K+fPn6dmzJzNnzqzx+ZgwYQLTp08H4JFHHuGf//wnw4cPJzY2lilTpjBy5EiKioooKyvjgw8+YMOGDezatYugoKAqsVVYsWIF8+bNu6T95ptvZu3atVXajh8/TocOHZz7oaGh7Nq1q8ZYCwoK2LRpU5UZU6WlpfTt25f09HTGjx/vzD5KTEykffv29O7d+5Lj7Nq1i8cee4wjR46wfPly/PyqvtWtXLmShx56qMqA17p160hJSaFz584sWLCADh06VBv/8ePHnft/+MMfmDlzJoMHDyY+Pp6AgAB69+7NP/7xD8LDw0lNTeXIkSNkZmbi6+tLq1atGD16NPv27aNv3778+c9/pkmTJmRnZ9OuXTsA2rVrR05ODgDnz59nzpw5JCcnVxmgOn36NFA+eLVt2zZuuukmFi1aRJs2bfjss884duwY9957b5V9ysrKePrpp1m+fLkziFrhb3/7G8OGDaNx48Y0a9aMnTt3IiIiIiLyI1V3Nfrvl8Vq2uZy9v0xj4cxZgzlSx1WWelBRESkoQr096Vr22Z0bdvskt9dLC3j2/wisvKLyDt/ge/OFXPyXDF55y9wtqiEsxdKOH+hhNMFxeQXXvzJY28wg1qLfnubu0MQaVhqmVF1LYWFhdGrVy8AevToweDBgzHG0KtXLzIyMoDy2ShxcXGkpaVhjOHixfL/fH18fHjrrbe45ZZbGDt2LAMHDgTgxIkThISEVJkFVCEpKYmkpCRuvbX8LxLOnTtHWloagwYNYuHChc41nY4dO0ZaWhq5ubkMGjSIsLAwAGcWEMCvfvUrAgICCAgIoHXr1mRnZ7Nlyxb27NlD//79ASgsLKR16/K/qvD19eX++++v9fnYunUrc+fOpaCggLy8PHr06EFERATHjx9n5MiRAAQGBgKwefNmRo8e7eRZObYKsbGxxMbG1vqYFay99PtndTOnKrz33nsMHDiwyuP6+vqyd+9eTp8+zciRI9m/fz8dO3Zk1qxZJCUlVXucAQMG8MUXX3Dw4EHi4uKIjo52cgRYtWoVy5cvd+4PHz6cUaNGERAQwJIlS4iLi+Ojjz6qNf6XX36Ztm3bOte2mjNnDtOnT2fKlClMnDiRPn360KtXL2699Vb8/Py4ePEin376Ka+++ioDBgxg4sSJxMfH88c//rHG52PGjBlMnjzZma1WoaSkhMzMTAYOHMj8+fOZP38+v//971m2bBmTJ0+udunAxYsXM2zYsCqDdBUWLFjA+++/z4ABA5g3bx5PPfVUleuKiYiIiIhcgUyg8ofOUODEZW7TqJZ9s40x7VyztNoBOVfweFhrXwdeB+jXr98PDZSJiIg0aP6+PnQICaJDyKW10PqgwQxqiUjDEBAQ4Nz28fFx7vv4+FBSUgKUz3C55557WL9+PRkZGURERDj7pKWlERwczIkT//ke9MEHHzB06NBqH89ay/PPP8/YsWOrtG/bto3NmzezY8cOgoKCiIiIoKioCGttjQM7lWP39fWlpKQEay1xcXG8/PLLl2wfGBhY63W0ioqKGDduHLt376ZDhw688MILTgw15VLboBNc2Uyt0NBQjh37z+ohmZmZtS5tt2rVqhqX9mvevDkRERFs2rSJoUOHcvjwYWeWVmZmJrfddhupqam0bdvW2adbt240adKE/fv3069fPwD27dtHSUkJffv2dbb72c9+5tx+/PHHee6555z4K5YFrHicitdKxcyqgIAARo8e7cyKatasGW+++SZQ/nyGhYURFhZGQUEBoaGhzkyzBx54wFnasU2bNmRlZdGuXTuysrKcQctdu3axdu1ann32WU6fPo2Pjw+BgYGMHz+eoKAgZ1DyN7/5DUuXLuXs2bPs37/fifHbb78lJiaGxMREduzYwSeffMLixYs5d+4cxcXFBAcH8/TTT7Nv3z4nroceeoioqKga+0hERERE5Af8G+hkjAkDjgMPA7/93jaJwATXNbMGAPmuwarcWvZNBOKAeNe/Gyu1v2OMmQ9cD3QCUq9VciIiIuJ+9ePKXiIiP6H8/Hzaty9fZr3yrJb8/HwmTpxISkoKJ0+edAZpKq6nBdC0aVPOnj3r7DN06FASEhI4d+4cUL7kXk5ODvn5+bRo0YKgoCAOHTrkLOl255138vHHH3P48GGAapf4q2zw4MGsXbvWWZIuLy+PI0eOXFaeRUVFALRs2ZJz5845+TRr1ozQ0FA2bNgAwIULFygoKGDIkCEkJCRQUFBQY2yxsbHs3bv3kp/vD2gB9O/fn7S0NA4fPkxxcTGrVq0iJiam2ljz8/P5+OOPq1wfKzc311lqr7CwkM2bN9O1a1d69epFTk4OGRkZZGRkEBoayqeffkrbtm05fPiwM3h55MgRvvzyS2688UbnmJWv2VUhKyvLuZ2YmEi3bt2A8r5NSkri1KlTnDp1iqSkJGdws2Ifay0bNmygZ8+eQPnSgMXFxUD5sn6DBg2iWbNmtG3blg4dOvDll18CsGXLFufaYjExMSxbtgyAZcuWOc/BJ5984uQ4adIkpk6dyoQJEzDGMHz4cGfAreJY1113Hd99952zzx133EFiYiL9+vVjxYoVHD16lIyMDP70pz/x6KOPEh8fT4sWLcjPz+err74CIDk52clfRERERORKWWtLgAnAh8BBYLW19gtjzBPGmCdcm70PfAOkA28A42rb17VPPBBpjEkDIl33cf1+NXAA2ASMt9aWXvNERURExG00U0tEGpxnn32WuLg45s+fzy9/+UunffLkyYwbN47OnTuzdOlS7rnnHu6++27S0tLo2rUrUD6rZ+DAgfTs2ZPo6GjmzZvHwYMHufPOOwEIDg7m7bffJioqiiVLlnDLLbfQpUsX7rjjDgBatWrF66+/zn333UdZWRmtW7cmOTm5xli7d+/OSy+9xJAhQygrK8Pf35+//OUv/PznP//BPJs3b87jjz9Or169uPHGG50lDAGWL1/O2LFjmT59Ov7+/qxZs4aoqCj27t1Lv379aNSoEcOGDWP27Nk/6jkG8PPzY9GiRQwdOpTS0lIee+wxevToAcCSJUsAeOKJ8u+169evZ8iQITRp0sTZPysri7i4OEpLSykrK+PBBx/k3nvvrfUxt2/fTnx8PP7+/vj4+LB48WJatmzp/H716tW8//77VfZZuHAhiYmJ+Pn5ERIS4gx0hoSEMG3aNOd5mz59urM0YmxsLLm5uVhr6dOnj5PPwYMHefTRR/H19aV79+4sXbrUeZxXX32V2NhYiouL6dixozOja8qUKTz44IMsXbqUG264gTVr1vzgcztnzhweeeQRJk2aRKtWrZxjXSk/Pz/eeOMN7r//fnx8fGjRogUJCQk/6lgiIiIiIgDW2vcpH7iq3Lak0m0LjL/cfV3tJ4HBNewzC5h1FSGLiIiIBzE1LUPlTv369bO7d+92dxgicoUOHjzodbM8tm/fzttvv+0MWog0VNWd38aYPdbafm4KSURERETkiqnmJCIi4hlqqjtpppaISC3Cw8MJDw93dxgiIiIiIiIiIiIiDZ4GtUREvMDIkSOd63RVmDNnjnMNKBERERERERERERFPp0EtEREvsH79eneHICIiIiIiIiIiInJN+bg7ABHxLvXxOn0icnV0XouIiIiIiIiISH2gQS0RqTOBgYGcPHlSBXARL2Kt5eTJkwQGBro7FBERERERERERaeC0/KCI1JnQ0FAyMzPJzc11dygiUocCAwMJDQ11dxgiIiIiIiIiItLAaVBLROqMv78/YWFh7g5DRERERERERERERLyQlh8UERERERERERERERGRek+DWiIiIiIiIiIiIiIiIlLvaVBLRERERERERERERERE6j1jrXV3DJcwxuQCR67BoVsC312D49YXys/zeXuOys+zeXt+4P051nV+P7fWtqrD44mIiIiIXFPXsOYE+j7h6ZSf5/P2HJWfZ1N+V67aulO9HNS6Vowxu621/dwdx7Wi/Dyft+eo/Dybt+cH3p+jt+cnIiIiIuJO3v55W/l5Nm/PD7w/R+Xn2ZRf3dHygyIiIiIiIiIiIiIiIlLvaVBLRERERERERERERERE6r2GNqj1ursDuMaUn+fz9hyVn2fz9vzA+3P09vxERERERNzJ2z9vKz/P5u35gffnqPw8m/KrIw3qmloiIiIiIiIiIiIiIiLimRraTC0RERERERERERERERHxQA1iUMsYE2WM+dIYk26MmeLueOqCMaaDMWarMeagMeYLY8xEV/sLxpjjxpi9rp9h7o71xzLGZBhjPnflsdvVFmKMSTbGpLn+beHuOH8MY0yXSn201xhzxhgzyZP7zxiTYIzJMcbsr9RWY38ZY553nZNfGmOGuifqK1NDjvOMMYeMMf9njFlvjGnuar/RGFNYqS+XuC3wy1RDfjW+Jj2tD2vI791KuWUYY/a62j2x/2p6X/Cq81BEREREpL7xtrqTak6eXXMC1Z1cv/Oo77veXnMC1Z1Ud6q7PvT65QeNMb7AV0AkkAn8GxhlrT3g1sCukjGmHdDOWvupMaYpsAf4NfAgcM5a+yd3xlcXjDEZQD9r7XeV2uYCedbaeNcHxRbW2ufcFWNdcL1GjwMDgNF4aP8ZYwYB54C/W2t7utqq7S9jTHdgJXA7cD2wGehsrS11U/iXpYYchwAfWWtLjDFzAFw53gj8s2I7T1BDfi9QzWvSE/uwuvy+9/tXgHxr7UwP7b+a3hf+Gy86D0VERERE6hNvrDup5uQ9NSdQ3QkP+b7r7TUnUN1Jdae668OGMFPrdiDdWvuNtbYYWAWMcHNMV81am2Wt/dR1+yxwEGjv3qh+EiOAZa7byyg/cTzdYOBra+0RdwdyNay1KUDe95pr6q8RwCpr7QVr7WEgnfJztV6rLkdrbZK1tsR1dycQ+pMHVkdq6MOaeFwf1pafMcZQ/gVt5U8aVB2q5X3Bq85DEREREZF6xuvqTqo5Ad5TcwLVnTzi+66315xAdSdUd6qzPmwIg1rtgWOV7mfiZW/ErpHdW4FdrqYJrmmpCcaDp0oDFkgyxuwxxoxxtbWx1mZB+YkEtHZbdHXnYar+h+Yt/Qc195e3npePAR9Uuh9mjPnMGPOxMeZudwVVB6p7TXpbH94NZFtr0yq1eWz/fe99oaGdhyIiIiIiPyWv/lytmpNXUN3JO85Lb605gepOHteH7q47NYRBLVNNm9esuWiMCQbWAZOstWeAvwI3AX2ALOAV90V31QZaa28DooHxrimcXsUY0wiIAda4mryp/2rjdeelMeYPQAmwwtWUBdxgrb0VeAp4xxjTzF3xXYWaXpPe1oejqPoh32P7r5r3hRo3rabNk/tQRERERMQdvPZztWpOnk91pyo89rz04poTqO7kcX1YH+pODWFQKxPoUOl+KHDCTbHUKWOMP+UvoBXW2n8AWGuzrbWl1toy4A3q+bTM2lhrT7j+zQHWU55Ltmv9zop1PHPcF2GdiAY+tdZmg3f1n0tN/eVV56UxJg64F4i1rgsVuqbWnnTd3gN8DXR2X5Q/Ti2vSa/pQ2OMH3Af8G5Fm6f2X3XvCzSQ81BERERExE288nO1ak5eUXMC1Z0qeOx56c01J1DdyXXbY/qwvtSdGsKg1r+BTsaYMNdfJzwMJLo5pqvmWodzKXDQWju/Unu7SpuNBPb/1LHVBWNME9cF5zDGNAGGUJ5LIhDn2iwO2OieCOtMlVF6b+m/Smrqr0TgYWNMgDEmDOgEpLohvqtmjIkCngNirLUFldpbuS7GijGmI+U5fuOeKH+8Wl6TXtOHwH8Bh6y1mRUNnth/Nb0v0ADOQxERERERN/K6upNqToB31JxAdSeP/r7r7TUnUN3Jddsj+rA+1Z386uIg9Zm1tsQYMwH4EPAFEqy1X7g5rLowEHgE+NwYs9fVNhUYZYzpQ/lUvgxgrDuCqwNtgPXl5wp+wDvW2k3GmH8Dq40x/wMcBX7jxhivijEmCIikah/N9dT+M8asBCKAlsaYTGAGEE81/WWt/cIYsxo4QPn06fHW2lK3BH4FasjxeSAASHa9Xndaa58ABgEzjTElQCnwhLX2ci+G6RY15BdR3WvSE/uwuvystUu5dH1x8MD+o+b3Ba86D0VERERE6hMvrTup5uThNSdQ3cnTvu96e80JVHf63uae2If1pu5kXLMWRUREREREREREREREROqthrD8oIiIiIiIiIiIiIiIiHg4DWqJiIiIiIiIiIiIiIhIvadBLREREREREREREREREan3NKglIiIiIiIiIiIiIiIi9Z4GtURERERERERERERERKTe06CWiIiIiIiIiIiIiIiI1Hsa1BIREREREREREREREZF6T4NaIiIiIiIiIiIiIiIiUu/9P8EiJxaVbqi1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 2160x1008 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = list(range(checkpoint.epoch_counter))\n",
    "sm = lambda y, w: np.convolve(y, np.ones(w) / w, mode=\"same\")\n",
    "pp = lambda k: plt.plot(\n",
    "    x, tensorboard.history[k], label=f\"{k} = {max(tensorboard.history[k])}\"\n",
    ")\n",
    "spp = lambda k: plt.plot(\n",
    "    x, sm(tensorboard.history[k], 5), label=f\"{k} = {max(tensorboard.history[k])}\"\n",
    ")\n",
    "\n",
    "\n",
    "plt.figure(0, figsize=(30, 14))\n",
    "plt.subplot(2, 3, 1)\n",
    "pp(\"max/student_acc\")\n",
    "pp(\"max/teacher_acc\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "pp(\"hyperparameters/learning_rate\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleAttributeError",
     "evalue": "'MixUpBatchShuffle' object has no attribute 'lambda_history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-27c4ea1a1ca9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixup_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambda_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/pytorch-dev/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    937\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m    940\u001b[0m             type(self).__name__, name))\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'MixUpBatchShuffle' object has no attribute 'lambda_history'"
     ]
    }
   ],
   "source": [
    "plt.hist(mixup_fn.lambda_history, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "320000 * 2 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "64 * 500 * 4 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "625 / 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ssl)",
   "language": "python",
   "name": "ssl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
