{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1703.01780.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/lcances/.miniconda3/envs/dct/bin/python'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import time\n",
    "import pprint\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from SSL.util.loaders import load_dataset, load_optimizer, load_callbacks, load_preprocesser\n",
    "from SSL.util.model_loader import load_model\n",
    "from SSL.util.checkpoint import CheckPoint, mSummaryWriter\n",
    "from SSL.util.utils import reset_seed, get_datetime, track_maximum, DotDict\n",
    "from SSL.ramps import Warmup, sigmoid_rampup\n",
    "from SSL.losses import JensenShanon\n",
    "\n",
    "from metric_utils.metrics import CategoricalAccuracy, FScore, ContinueAverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--from_config\", default=\"\", type=str)\n",
    "parser.add_argument(\"-d\", \"--dataset_root\", default=\"../../datasets\", type=str)\n",
    "parser.add_argument(\"-D\", \"--dataset\", default=\"ubs8k\", type=str)\n",
    "\n",
    "group_t = parser.add_argument_group(\"Commun parameters\")\n",
    "group_t.add_argument(\"-m\", \"--model\", default=\"wideresnet28_2\", type=str)\n",
    "group_t.add_argument(\"--supervised_ratio\", default=0.1, type=float)\n",
    "group_t.add_argument(\"--batch_size\", default=64, type=int)\n",
    "group_t.add_argument(\"--nb_epoch\", default=200, type=int)\n",
    "group_t.add_argument(\"--learning_rate\", default=0.001, type=float)\n",
    "group_t.add_argument(\"--resume\", action=\"store_true\", default=False)\n",
    "group_t.add_argument(\"--seed\", default=1234, type=int)\n",
    "\n",
    "group_m = parser.add_argument_group(\"Model parameters\")\n",
    "group_m.add_argument(\"--num_classes\", default=10, type=int)\n",
    "\n",
    "group_u = parser.add_argument_group(\"Datasets parameters\")\n",
    "group_u.add_argument(\"-t\", \"--train_folds\", nargs=\"+\", default=[1, 2, 3, 4, 5, 6, 7, 8, 9], type=int)\n",
    "group_u.add_argument(\"-v\", \"--val_folds\", nargs=\"+\", default=[10], type=int)\n",
    "\n",
    "group_s = parser.add_argument_group(\"Student teacher parameters\")\n",
    "group_s.add_argument(\"--ema_alpha\", default=0.999, type=float)\n",
    "group_s.add_argument(\"--warmup_length\", default=50, type=int)\n",
    "group_s.add_argument(\"--lambda_cost_max\", default=1, type=float)\n",
    "group_s.add_argument(\"--teacher_noise\", default=0, type=float)\n",
    "group_s.add_argument(\"--ccost_softmax\", action=\"store_false\", default=True)\n",
    "group_s.add_argument(\"--ccost_method\", type=str, default=\"mse\")\n",
    "\n",
    "group_l = parser.add_argument_group(\"Logs\")\n",
    "group_l.add_argument(\"--checkpoint_root\", default=\"../model_save/\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_root\", default=\"../tensorboard/\", type=str)\n",
    "group_l.add_argument(\"--checkpoint_path\", default=\"mean-teacher\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_path\", default=\"mean-teacher\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_sufix\", default=\"\", type=str)\n",
    "\n",
    "args=parser.parse_args(\"\")\n",
    "\n",
    "tensorboard_path = os.path.join(args.tensorboard_root, args.dataset, args.tensorboard_path)\n",
    "checkpoint_path = os.path.join(args.checkpoint_root, args.dataset, args.checkpoint_path)"
   ]
  },
  {
   "source": [
    "## Basic verification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_datasets = [\"esc10\", \"ubs8k\", \"speechcommand\"]\n",
    "available_models = [\"cnn03\", \"wideresnet28_2\", \"wideresnet28_4\", \"wideresnet28_8\"]\n",
    "available_ccost_method = [\"mse\", \"js\"]\n",
    "\n",
    "assert args.dataset in available_datasets\n",
    "assert args.model in available_models\n",
    "assert args.ccost_method in available_ccost_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'batch_size': 64,\n 'ccost_method': 'mse',\n 'ccost_softmax': True,\n 'checkpoint_path': 'mean-teacher',\n 'checkpoint_root': '../model_save/',\n 'dataset': 'ubs8k',\n 'dataset_root': '../../datasets',\n 'ema_alpha': 0.999,\n 'from_config': '',\n 'lambda_cost_max': 1,\n 'learning_rate': 0.003,\n 'model': 'wideresnet28_2',\n 'nb_epoch': 200,\n 'num_classes': 10,\n 'resume': False,\n 'seed': 1234,\n 'supervised_ratio': 0.1,\n 'teacher_noise': 0,\n 'tensorboard_path': 'mean-teacher',\n 'tensorboard_root': '../tensorboard/',\n 'tensorboard_sufix': '',\n 'train_folds': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n 'val_folds': [10],\n 'warmup_length': 50}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(vars(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "reset_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "################################################################################\n### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n################################################################################\n\n"
     ]
    }
   ],
   "source": [
    "train_transform, val_transform = load_preprocesser(args.dataset, \"mean-teacher\")\n",
    "train_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f346e96418094d0fa8dd38706e4ef5bb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ns_batch_size:  6\nu_batch_size:  58\n"
     ]
    }
   ],
   "source": [
    "manager, train_loader, val_loader = load_dataset(\n",
    "    args.dataset,\n",
    "    \"mean-teacher\",\n",
    "    \n",
    "    dataset_root = args.dataset_root,\n",
    "    supervised_ratio = args.supervised_ratio,\n",
    "    batch_size = args.batch_size,\n",
    "    train_folds = args.train_folds,\n",
    "    val_folds = args.val_folds,\n",
    "\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    "    \n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(64, 173)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "input_shape = tuple(train_loader._iterables[0].dataset[0][0].shape)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_func = load_model(args.dataset, args.model)\n",
    "\n",
    "student = model_func(input_shape=input_shape, num_classes = args.num_classes)\n",
    "teacher = model_func(input_shape=input_shape, num_classes = args.num_classes)\n",
    "\n",
    "student = student.cuda()\n",
    "teacher = teacher.cuda()\n",
    "\n",
    "# We do not need gradient for the teacher model\n",
    "for p in teacher.parameters():\n",
    "    p.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1          [-1, 32, 64, 173]             864\n       BatchNorm2d-2          [-1, 32, 64, 173]              64\n              ReLU-3          [-1, 32, 64, 173]               0\n         MaxPool2d-4           [-1, 32, 32, 87]               0\n            Conv2d-5           [-1, 32, 32, 87]           9,216\n       BatchNorm2d-6           [-1, 32, 32, 87]              64\n              ReLU-7           [-1, 32, 32, 87]               0\n            Conv2d-8           [-1, 32, 32, 87]           9,216\n       BatchNorm2d-9           [-1, 32, 32, 87]              64\n             ReLU-10           [-1, 32, 32, 87]               0\n       BasicBlock-11           [-1, 32, 32, 87]               0\n           Conv2d-12           [-1, 32, 32, 87]           9,216\n      BatchNorm2d-13           [-1, 32, 32, 87]              64\n             ReLU-14           [-1, 32, 32, 87]               0\n           Conv2d-15           [-1, 32, 32, 87]           9,216\n      BatchNorm2d-16           [-1, 32, 32, 87]              64\n             ReLU-17           [-1, 32, 32, 87]               0\n       BasicBlock-18           [-1, 32, 32, 87]               0\n           Conv2d-19           [-1, 32, 32, 87]           9,216\n      BatchNorm2d-20           [-1, 32, 32, 87]              64\n             ReLU-21           [-1, 32, 32, 87]               0\n           Conv2d-22           [-1, 32, 32, 87]           9,216\n      BatchNorm2d-23           [-1, 32, 32, 87]              64\n             ReLU-24           [-1, 32, 32, 87]               0\n       BasicBlock-25           [-1, 32, 32, 87]               0\n           Conv2d-26           [-1, 32, 32, 87]           9,216\n      BatchNorm2d-27           [-1, 32, 32, 87]              64\n             ReLU-28           [-1, 32, 32, 87]               0\n           Conv2d-29           [-1, 32, 32, 87]           9,216\n      BatchNorm2d-30           [-1, 32, 32, 87]              64\n             ReLU-31           [-1, 32, 32, 87]               0\n       BasicBlock-32           [-1, 32, 32, 87]               0\n           Conv2d-33           [-1, 64, 16, 44]          18,432\n      BatchNorm2d-34           [-1, 64, 16, 44]             128\n             ReLU-35           [-1, 64, 16, 44]               0\n           Conv2d-36           [-1, 64, 16, 44]          36,864\n      BatchNorm2d-37           [-1, 64, 16, 44]             128\n           Conv2d-38           [-1, 64, 16, 44]           2,048\n      BatchNorm2d-39           [-1, 64, 16, 44]             128\n             ReLU-40           [-1, 64, 16, 44]               0\n       BasicBlock-41           [-1, 64, 16, 44]               0\n           Conv2d-42           [-1, 64, 16, 44]          36,864\n      BatchNorm2d-43           [-1, 64, 16, 44]             128\n             ReLU-44           [-1, 64, 16, 44]               0\n           Conv2d-45           [-1, 64, 16, 44]          36,864\n      BatchNorm2d-46           [-1, 64, 16, 44]             128\n             ReLU-47           [-1, 64, 16, 44]               0\n       BasicBlock-48           [-1, 64, 16, 44]               0\n           Conv2d-49           [-1, 64, 16, 44]          36,864\n      BatchNorm2d-50           [-1, 64, 16, 44]             128\n             ReLU-51           [-1, 64, 16, 44]               0\n           Conv2d-52           [-1, 64, 16, 44]          36,864\n      BatchNorm2d-53           [-1, 64, 16, 44]             128\n             ReLU-54           [-1, 64, 16, 44]               0\n       BasicBlock-55           [-1, 64, 16, 44]               0\n           Conv2d-56           [-1, 64, 16, 44]          36,864\n      BatchNorm2d-57           [-1, 64, 16, 44]             128\n             ReLU-58           [-1, 64, 16, 44]               0\n           Conv2d-59           [-1, 64, 16, 44]          36,864\n      BatchNorm2d-60           [-1, 64, 16, 44]             128\n             ReLU-61           [-1, 64, 16, 44]               0\n       BasicBlock-62           [-1, 64, 16, 44]               0\n           Conv2d-63           [-1, 128, 8, 22]          73,728\n      BatchNorm2d-64           [-1, 128, 8, 22]             256\n             ReLU-65           [-1, 128, 8, 22]               0\n           Conv2d-66           [-1, 128, 8, 22]         147,456\n      BatchNorm2d-67           [-1, 128, 8, 22]             256\n           Conv2d-68           [-1, 128, 8, 22]           8,192\n      BatchNorm2d-69           [-1, 128, 8, 22]             256\n             ReLU-70           [-1, 128, 8, 22]               0\n       BasicBlock-71           [-1, 128, 8, 22]               0\n           Conv2d-72           [-1, 128, 8, 22]         147,456\n      BatchNorm2d-73           [-1, 128, 8, 22]             256\n             ReLU-74           [-1, 128, 8, 22]               0\n           Conv2d-75           [-1, 128, 8, 22]         147,456\n      BatchNorm2d-76           [-1, 128, 8, 22]             256\n             ReLU-77           [-1, 128, 8, 22]               0\n       BasicBlock-78           [-1, 128, 8, 22]               0\n           Conv2d-79           [-1, 128, 8, 22]         147,456\n      BatchNorm2d-80           [-1, 128, 8, 22]             256\n             ReLU-81           [-1, 128, 8, 22]               0\n           Conv2d-82           [-1, 128, 8, 22]         147,456\n      BatchNorm2d-83           [-1, 128, 8, 22]             256\n             ReLU-84           [-1, 128, 8, 22]               0\n       BasicBlock-85           [-1, 128, 8, 22]               0\n           Conv2d-86           [-1, 128, 8, 22]         147,456\n      BatchNorm2d-87           [-1, 128, 8, 22]             256\n             ReLU-88           [-1, 128, 8, 22]               0\n           Conv2d-89           [-1, 128, 8, 22]         147,456\n      BatchNorm2d-90           [-1, 128, 8, 22]             256\n             ReLU-91           [-1, 128, 8, 22]               0\n       BasicBlock-92           [-1, 128, 8, 22]               0\nAdaptiveAvgPool2d-93            [-1, 128, 1, 1]               0\n           Linear-94                   [-1, 10]           1,290\n================================================================\nTotal params: 1,472,554\nTrainable params: 1,472,554\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.04\nForward/backward pass size (MB): 43.29\nParams size (MB): 5.62\nEstimated Total Size (MB): 48.95\n----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "s = summary(student, input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "../tensorboard/ubs8k/mean-teacher/wideresnet28_2/0.1S/2020-12-14_16:50:36_wideresnet28_2_MSE_True-softmax_0-n\n"
     ]
    }
   ],
   "source": [
    "# tensorboard\n",
    "title_element = (args.model, args.supervised_ratio, get_datetime(), model_func.__name__,\n",
    "                 args.ccost_method.upper(), args.ccost_softmax, args.teacher_noise)\n",
    "tensorboard_title = \"%s/%sS/%s_%s_%s_%s-softmax_%s-n\" % title_element\n",
    "\n",
    "title_element = (args.model, args.supervised_ratio, get_datetime(), model_func.__name__,\n",
    "                 args.ccost_method.upper(), args.ccost_softmax, args.teacher_noise)\n",
    "checkpoint_title = \"%s/%sS/%s_%s_%s_%s-softmax_%s-n\" % title_element\n",
    "\n",
    "tensorboard = mSummaryWriter(log_dir=\"%s/%s\" % (tensorboard_path, tensorboard_title), comment=model_func.__name__)\n",
    "print(os.path.join(tensorboard_path, tensorboard_title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer & callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = load_optimizer(args.dataset, \"mean-teacher\", student=student, learning_rate=args.learning_rate)\n",
    "callbacks = load_callbacks(args.dataset, \"mean-teacher\", optimizer=optimizer, nb_epoch=args.nb_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "# losses\n",
    "loss_ce = nn.CrossEntropyLoss(reduction=\"mean\") # Supervised loss\n",
    "\n",
    "if args.ccost_method == \"mse\":\n",
    "    consistency_cost = nn.MSELoss(reduction=\"mean\") # Unsupervised loss\n",
    "elif args.ccost_method == \"js\":\n",
    "    consistency_cost = JensenShanon\n",
    "        \n",
    "lambda_cost = Warmup(args.lambda_cost_max, args.warmup_length, sigmoid_rampup)\n",
    "callbacks += [lambda_cost]\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint = CheckPoint(student, optimizer, mode=\"max\", name=\"%s/%s.torch\" % (checkpoint_path, checkpoint_title))\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MSELoss()"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "consistency_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_calculator():\n",
    "    def c(logits, y):\n",
    "        with torch.no_grad():\n",
    "            y_one_hot = F.one_hot(y, num_classes=args.num_classes)\n",
    "            \n",
    "            pred = torch.softmax(logits, dim=1)\n",
    "            arg = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            acc = c.fn.acc(arg, y).mean\n",
    "            f1 = c.fn.f1(pred, y_one_hot).mean\n",
    "            \n",
    "            return acc, f1,\n",
    "            \n",
    "    c.fn = DotDict(\n",
    "        acc = CategoricalAccuracy(),\n",
    "        f1 = FScore(),\n",
    "    )\n",
    "    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_student_s_metrics = metrics_calculator()\n",
    "calc_student_u_metrics = metrics_calculator()\n",
    "calc_teacher_s_metrics = metrics_calculator()\n",
    "calc_teacher_u_metrics = metrics_calculator()\n",
    "\n",
    "avg_Sce = ContinueAverage()\n",
    "avg_Tce = ContinueAverage()\n",
    "avg_ccost = ContinueAverage()\n",
    "\n",
    "softmax_fn = lambda x: x\n",
    "if args.ccost_softmax:\n",
    "    softmax_fn = nn.Softmax(dim=1)\n",
    "\n",
    "def reset_metrics():\n",
    "    for d in [calc_student_s_metrics.fn, calc_student_u_metrics.fn, calc_teacher_s_metrics.fn, calc_teacher_u_metrics.fn]:\n",
    "        for fn in d.values():\n",
    "            fn.reset()\n",
    "\n",
    "maximum_tracker = track_maximum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Can resume previous training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if args.resume:\n",
    "    checkpoint.load_last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "args.resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ".        Epoch  - %      - Student:   ce       ccost    acc_s    f1_s     acc_u    f1_u     | Teacher:   ce       acc_s    f1_s     acc_u    f1_u     - Time    \n"
     ]
    }
   ],
   "source": [
    "UNDERLINE_SEQ = \"\\033[1;4m\"\n",
    "RESET_SEQ = \"\\033[0m\"\n",
    "\n",
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<10.8} {:<8.6} {:<8.6} {:<8.6} {:<8.6} {:<8.6} {:<8.6} | {:<10.8} {:<8.6} {:<8.6} {:<8.6} {:<8.6} {:<8.6} - {:<8.6}\"\n",
    "value_form  = \"{:<8.8} {:<6d} - {:<6d} - {:<10.8} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} | {:<10.8} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} - {:<8.4f}\"\n",
    "header = header_form.format(\".               \", \"Epoch\",  \"%\", \"Student:\", \"ce\", \"ccost\", \"acc_s\", \"f1_s\", \"acc_u\", \"f1_u\", \"Teacher:\", \"ce\", \"acc_s\", \"f1_s\", \"acc_u\", \"f1_u\" , \"Time\")\n",
    "\n",
    "train_form = value_form\n",
    "val_form = UNDERLINE_SEQ + value_form + RESET_SEQ\n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_teacher_model(student_model, teacher_model, alpha, epoch):\n",
    "    \n",
    "    # Use the true average until the exponential average is more correct\n",
    "    alpha = min(1 - 1 / (epoch + 1), alpha)\n",
    "    \n",
    "    for param, ema_param in zip(student_model.parameters(), teacher_model.parameters()):\n",
    "        ema_param.data.mul_(alpha).add_(param.data,  alpha = 1-alpha)\n",
    "\n",
    "\n",
    "noise_fn = lambda x: x\n",
    "if args.teacher_noise != 0:\n",
    "    n_db = args.teacher_noise\n",
    "    noise_fn = transforms.Lambda(lambda x: x + (torch.rand(x.shape).cuda() * n_db + n_db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "    \n",
    "    nb_batch = len(train_loader)\n",
    "\n",
    "    reset_metrics()\n",
    "    student.train()\n",
    "\n",
    "    for i, (S, U) in enumerate(train_loader):        \n",
    "        x_s, y_s = S\n",
    "        x_u, y_u = U\n",
    "        \n",
    "        x_s, x_u = x_s.cuda(), x_u.cuda()\n",
    "        y_s, y_u = y_s.cuda(), y_u.cuda()\n",
    "        \n",
    "        # Predictions\n",
    "        student_s_logits = student(x_s)        \n",
    "        student_u_logits = student(x_u)\n",
    "        teacher_s_logits = teacher(noise_fn(x_s))\n",
    "        teacher_u_logits = teacher(noise_fn(x_u))\n",
    "        \n",
    "        # Calculate supervised loss (only student on S)\n",
    "        loss = loss_ce(student_s_logits, y_s)\n",
    "        \n",
    "        # Calculate consistency cost (mse(student(x), teacher(x))) x is S + U\n",
    "        student_logits = torch.cat((student_s_logits, student_u_logits), dim=0)\n",
    "        teacher_logits = torch.cat((teacher_s_logits, teacher_u_logits), dim=0)\n",
    "        ccost = consistency_cost(softmax_fn(student_logits), softmax_fn(teacher_logits))\n",
    "\n",
    "        total_loss = loss + lambda_cost() * ccost\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            # Teacher prediction (for metrics purpose)\n",
    "            _teacher_loss = loss_ce(teacher_s_logits, y_s)\n",
    "            \n",
    "            # Update teacher\n",
    "            update_teacher_model(student, teacher, args.ema_alpha, epoch*nb_batch + i)\n",
    "            \n",
    "            # Compute the metrics for the student\n",
    "            student_s_metrics = calc_student_s_metrics(student_s_logits, y_s)\n",
    "            student_u_metrics = calc_student_u_metrics(student_u_logits, y_u)\n",
    "            student_s_acc, student_s_f1, student_u_acc, student_u_f1 = *student_s_metrics, *student_u_metrics\n",
    "            \n",
    "            # Compute the metrics for the teacher\n",
    "            teacher_s_metrics = calc_teacher_s_metrics(teacher_s_logits, y_s)\n",
    "            teacher_u_metrics = calc_teacher_u_metrics(teacher_u_logits, y_u)\n",
    "            teacher_s_acc, teacher_s_f1, teacher_u_acc, teacher_u_f1 = *teacher_s_metrics, *teacher_u_metrics\n",
    "            \n",
    "            # Running average of the two losses\n",
    "            student_running_loss = avg_Sce(loss.item()).mean\n",
    "            teacher_running_loss = avg_Tce(_teacher_loss.item()).mean\n",
    "            running_ccost = avg_ccost(ccost.item()).mean\n",
    "\n",
    "            # logs\n",
    "            print(train_form.format(\n",
    "                \"Training: \", epoch + 1, int(100 * (i + 1) / nb_batch),\n",
    "                \"\", student_running_loss, running_ccost, *student_s_metrics, *student_u_metrics,\n",
    "                \"\", teacher_running_loss, *teacher_s_metrics, *teacher_u_metrics,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"train/student_acc_s\", student_s_acc, epoch)\n",
    "    tensorboard.add_scalar(\"train/student_acc_u\", student_u_acc, epoch)\n",
    "    tensorboard.add_scalar(\"train/student_f1_s\", student_s_f1, epoch)\n",
    "    tensorboard.add_scalar(\"train/student_f1_u\", student_u_f1, epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"train/teacher_acc_s\", teacher_s_acc, epoch)\n",
    "    tensorboard.add_scalar(\"train/teacher_acc_u\", teacher_u_acc, epoch)\n",
    "    tensorboard.add_scalar(\"train/teacher_f1_s\", teacher_s_f1, epoch)\n",
    "    tensorboard.add_scalar(\"train/teacher_f1_u\", teacher_u_f1, epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"train/student_loss\", student_running_loss, epoch)\n",
    "    tensorboard.add_scalar(\"train/teacher_loss\", teacher_running_loss, epoch)\n",
    "    tensorboard.add_scalar(\"train/consistency_cost\", running_ccost, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "    reset_metrics()\n",
    "    student.eval()\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (X, y) in enumerate(val_loader):\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            # Predictions\n",
    "            student_logits = student(X)        \n",
    "            teacher_logits = teacher(X)\n",
    "\n",
    "            # Calculate supervised loss (only student on S)\n",
    "            loss = loss_ce(student_logits, y)\n",
    "            _teacher_loss = loss_ce(teacher_logits, y) # for metrics only\n",
    "            ccost = consistency_cost(softmax_fn(student_logits), softmax_fn(teacher_logits))\n",
    "            \n",
    "            # Compute the metrics\n",
    "            y_one_hot = F.one_hot(y, num_classes=args.num_classes)\n",
    "            \n",
    "            # ---- student ----\n",
    "            student_metrics = calc_student_s_metrics(student_logits, y)\n",
    "            student_acc, student_f1 = student_metrics\n",
    "            \n",
    "            # ---- teacher ----\n",
    "            teacher_metrics = calc_teacher_s_metrics(teacher_logits, y)\n",
    "            teacher_acc, teacher_f1 = teacher_metrics\n",
    "\n",
    "            # Running average of the two losses\n",
    "            student_running_loss = avg_Sce(loss.item()).mean\n",
    "            teacher_running_loss = avg_Tce(_teacher_loss.item()).mean\n",
    "            running_ccost = avg_ccost(ccost.item()).mean\n",
    "\n",
    "            # logs\n",
    "            print(val_form.format(\n",
    "                \"Validation: \", epoch + 1, int(100 * (i + 1) / len(val_loader)),\n",
    "                \"\", student_running_loss, running_ccost, *student_metrics, 0.0, 0.0,\n",
    "                \"\", teacher_running_loss, *teacher_metrics, 0.0, 0.0,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"val/student_acc\", student_acc, epoch)\n",
    "    tensorboard.add_scalar(\"val/student_f1\", student_f1, epoch)\n",
    "    tensorboard.add_scalar(\"val/teacher_acc\", teacher_acc, epoch)\n",
    "    tensorboard.add_scalar(\"val/teacher_f1\", teacher_f1, epoch)\n",
    "    tensorboard.add_scalar(\"val/student_loss\", student_running_loss, epoch)\n",
    "    tensorboard.add_scalar(\"val/teacher_loss\", teacher_running_loss, epoch)\n",
    "    tensorboard.add_scalar(\"val/consistency_cost\", running_ccost, epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"hyperparameters/learning_rate\", get_lr(optimizer), epoch)\n",
    "    tensorboard.add_scalar(\"hyperparameters/lambda_cost_max\", lambda_cost(), epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"max/student_acc\", maximum_tracker(\"student_acc\", student_acc), epoch )\n",
    "    tensorboard.add_scalar(\"max/teacher_acc\", maximum_tracker(\"teacher_acc\", teacher_acc), epoch )\n",
    "    tensorboard.add_scalar(\"max/student_f1\", maximum_tracker(\"student_f1\", student_f1), epoch )\n",
    "    tensorboard.add_scalar(\"max/teacher_f1\", maximum_tracker(\"teacher_f1\", teacher_f1), epoch )\n",
    "\n",
    "    checkpoint.step(teacher_acc)\n",
    "    for c in callbacks:\n",
    "        c.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "36   |            0.6275   1.0000   0.9993   0.8339   0.8365   - 21.7862 \n",
      "\n",
      "Training 78     - 100    -            0.6719   0.0089   0.9987   0.9974   0.8236   0.8266   |            0.6216   1.0000   1.0000   0.8374   0.8392   - 22.2147 \n",
      "\n",
      "Training 79     - 100    -            0.6658   0.0089   0.9987   0.9993   0.8286   0.8306   |            0.6160   0.9987   0.9987   0.8358   0.8371   - 21.9520 \n",
      "\n",
      "Training 80     - 100    -            0.6598   0.0088   0.9962   0.9962   0.8335   0.8345   |            0.6106   0.9987   0.9987   0.8373   0.8396   - 21.9570 \n",
      "\n",
      "Training 81     - 100    -            0.6540   0.0087   1.0000   0.9986   0.8377   0.8389   |            0.6051   1.0000   1.0000   0.8372   0.8387   - 21.9810 \n",
      "\n",
      "Training 82     - 100    -            0.6481   0.0087   0.9987   0.9987   0.8263   0.8291   |            0.5996   0.9987   0.9987   0.8349   0.8371   - 21.7058 \n",
      "\n",
      "Training 83     - 100    -            0.6425   0.0086   1.0000   1.0000   0.8372   0.8396   |            0.5943   1.0000   1.0000   0.8393   0.8423   - 22.2131 \n",
      "\n",
      "Training 84     - 100    -            0.6372   0.0085   0.9886   0.9892   0.8121   0.8143   |            0.5891   1.0000   1.0000   0.8372   0.8387   - 22.0892 \n",
      "\n",
      "Training 85     - 100    -            0.6343   0.0086   0.9356   0.9376   0.7780   0.7799   |            0.5843   0.9987   0.9987   0.8371   0.8403   - 22.2369 \n",
      "\n",
      "Training 86     - 100    -            0.6301   0.0086   0.9760   0.9751   0.8032   0.8062   |            0.5799   0.9962   0.9967   0.8383   0.8415   - 21.9603 \n",
      "\n",
      "Training 87     - 100    -            0.6255   0.0086   0.9760   0.9731   0.8121   0.8145   |            0.5751   1.0000   1.0000   0.8382   0.8407   - 21.9955 \n",
      "\n",
      "Training 88     - 100    -            0.6206   0.0086   0.9962   0.9961   0.8156   0.8210   |            0.5705   0.9987   0.9987   0.8413   0.8433   - 21.8716 \n",
      "\n",
      "Training 89     - 100    -            0.6157   0.0086   0.9949   0.9943   0.8193   0.8223   |            0.5660   1.0000   1.0000   0.8373   0.8392   - 21.7069 \n",
      "\n",
      "Training 90     - 100    -            0.6108   0.0085   1.0000   1.0000   0.8306   0.8329   |            0.5616   1.0000   1.0000   0.8394   0.8402   - 21.9961 \n",
      "\n",
      "Training 91     - 100    -            0.6061   0.0085   0.9861   0.9863   0.8123   0.8140   |            0.5572   1.0000   0.9986   0.8386   0.8400   - 22.1787 \n",
      "\n",
      "Training 92     - 100    -            0.6017   0.0085   0.9987   0.9980   0.8234   0.8261   |            0.5529   0.9987   0.9993   0.8387   0.8402   - 21.9593 \n",
      "\n",
      "Training 93     - 100    -            0.5972   0.0084   0.9987   0.9987   0.8217   0.8240   |            0.5489   0.9975   0.9975   0.8342   0.8358   - 22.1903 \n",
      "\n",
      "Training 94     - 100    -            0.5929   0.0084   0.9899   0.9899   0.8130   0.8150   |            0.5448   0.9962   0.9962   0.8356   0.8382   - 21.9565 \n",
      "\n",
      "Training 95     - 100    -            0.5886   0.0083   1.0000   0.9993   0.8265   0.8288   |            0.5407   1.0000   1.0000   0.8365   0.8387   - 21.7934 \n",
      "\n",
      "Training 96     - 100    -            0.5842   0.0083   0.9949   0.9940   0.8224   0.8256   |            0.5367   1.0000   1.0000   0.8356   0.8373   - 22.3646 \n",
      "\n",
      "Training 97     - 100    -            0.5805   0.0083   0.9912   0.9917   0.8254   0.8272   |            0.5331   0.9975   0.9979   0.8374   0.8399   - 21.9874 \n",
      "\n",
      "Training 98     - 100    -            0.5768   0.0083   0.9874   0.9878   0.8170   0.8185   |            0.5293   1.0000   1.0000   0.8387   0.8415   - 21.9866 \n",
      "\n",
      "Training 99     - 100    -            0.5735   0.0083   0.9987   0.9987   0.8137   0.8161   |            0.5256   1.0000   1.0000   0.8380   0.8399   - 21.9745 \n",
      "\n",
      "Training 100    - 100    -            0.5694   0.0082   1.0000   0.9993   0.8291   0.8302   |            0.5220   1.0000   1.0000   0.8325   0.8339   - 21.8107 \n",
      "\n",
      "Training 101    - 100    -            0.5655   0.0081   1.0000   1.0000   0.8319   0.8338   |            0.5184   1.0000   1.0000   0.8350   0.8374   - 21.9229 \n",
      "\n",
      "Training 102    - 100    -            0.5616   0.0081   1.0000   1.0000   0.8343   0.8350   |            0.5150   1.0000   1.0000   0.8364   0.8389   - 22.2255 \n",
      "\n",
      "Training 103    - 100    -            0.5579   0.0080   1.0000   1.0000   0.8303   0.8320   |            0.5116   1.0000   1.0000   0.8356   0.8372   - 22.1446 \n",
      "\n",
      "Training 104    - 100    -            0.5544   0.0080   0.9924   0.9929   0.8188   0.8212   |            0.5082   1.0000   0.9993   0.8357   0.8381   - 22.3918 \n",
      "\n",
      "Training 105    - 100    -            0.5512   0.0080   0.9962   0.9948   0.8173   0.8183   |            0.5051   1.0000   1.0000   0.8360   0.8381   - 21.9750 \n",
      "\n",
      "Training 106    - 100    -            0.5479   0.0080   0.9924   0.9924   0.8285   0.8310   |            0.5019   0.9975   0.9980   0.8359   0.8376   - 22.0323 \n",
      "\n",
      "Training 107    - 100    -            0.5449   0.0080   0.9811   0.9814   0.8073   0.8080   |            0.4988   0.9987   0.9987   0.8390   0.8397   - 21.7706 \n",
      "\n",
      "Training 108    - 100    -            0.5416   0.0080   0.9975   0.9980   0.8269   0.8305   |            0.4957   1.0000   1.0000   0.8378   0.8402   - 22.0511 \n",
      "\n",
      "Training 109    - 100    -            0.5387   0.0079   0.9987   0.9980   0.8343   0.8359   |            0.4930   1.0000   1.0000   0.8385   0.8402   - 22.2602 \n",
      "\n",
      "Training 110    - 100    -            0.5355   0.0079   1.0000   1.0000   0.8365   0.8370   |            0.4901   1.0000   1.0000   0.8386   0.8395   - 22.1631 \n",
      "\n",
      "Training 111    - 100    -            0.5323   0.0078   0.9975   0.9980   0.8330   0.8355   |            0.4872   1.0000   0.9993   0.8376   0.8400   - 22.3052 \n",
      "\n",
      "Training 112    - 100    -            0.5292   0.0078   0.9975   0.9980   0.8353   0.8378   |            0.4845   1.0000   1.0000   0.8374   0.8394   - 21.6002 \n",
      "\n",
      "Training 113    - 100    -            0.5261   0.0077   1.0000   1.0000   0.8372   0.8387   |            0.4816   1.0000   1.0000   0.8413   0.8430   - 21.8899 \n",
      "\n",
      "Training 114    - 100    -            0.5232   0.0077   1.0000   1.0000   0.8396   0.8419   |            0.4793   0.9987   0.9987   0.8439   0.8439   - 22.0660 \n",
      "\n",
      "Training 115    - 100    -            0.5204   0.0076   0.9899   0.9899   0.8231   0.8248   |            0.4766   0.9975   0.9975   0.8397   0.8425   - 22.0419 \n",
      "\n",
      "Training 116    - 100    -            0.5177   0.0076   0.9962   0.9962   0.8224   0.8243   |            0.4740   1.0000   1.0000   0.8395   0.8402   - 22.6311 \n",
      "\n",
      "Training 117    - 100    -            0.5151   0.0076   0.9848   0.9854   0.8146   0.8159   |            0.4714   1.0000   1.0000   0.8410   0.8415   - 22.3041 \n",
      "\n",
      "Training 118    - 100    -            0.5126   0.0076   0.9962   0.9955   0.8236   0.8248   |            0.4690   1.0000   0.9993   0.8421   0.8437   - 22.3667 \n",
      "\n",
      "Training 119    - 100    -            0.5100   0.0076   0.9975   0.9980   0.8257   0.8279   |            0.4668   0.9987   0.9987   0.8387   0.8409   - 22.2475 \n",
      "\n",
      "Training 120    - 100    -            0.5073   0.0076   1.0000   1.0000   0.8365   0.8384   |            0.4645   1.0000   1.0000   0.8431   0.8431   - 22.1753 \n",
      "\n",
      "Training 121    - 100    -            0.5047   0.0075   0.9987   0.9987   0.8350   0.8366   |            0.4621   0.9987   0.9987   0.8423   0.8437   - 22.3784 \n",
      "\n",
      "Training 122    - 100    -            0.5020   0.0075   1.0000   1.0000   0.8355   0.8377   |            0.4597   1.0000   1.0000   0.8396   0.8401   - 22.1392 \n",
      "\n",
      "Training 123    - 100    -            0.4993   0.0074   1.0000   1.0000   0.8391   0.8396   |            0.4573   1.0000   0.9993   0.8391   0.8410   - 22.1523 \n",
      "\n",
      "Training 124    - 100    -            0.4965   0.0074   1.0000   1.0000   0.8361   0.8383   |            0.4550   1.0000   0.9993   0.8379   0.8389   - 22.2318 \n",
      "\n",
      "Training 125    - 100    -            0.4938   0.0073   0.9987   0.9987   0.8419   0.8424   |            0.4527   1.0000   1.0000   0.8444   0.8463   - 22.1112 \n",
      "\n",
      "Training 126    - 100    -            0.4913   0.0073   0.9975   0.9975   0.8287   0.8305   |            0.4504   1.0000   0.9993   0.8426   0.8438   - 22.0841 \n",
      "\n",
      "Training 127    - 100    -            0.4891   0.0073   0.9987   0.9987   0.8293   0.8326   |            0.4481   1.0000   1.0000   0.8401   0.8407   - 22.3272 \n",
      "\n",
      "Training 128    - 100    -            0.4865   0.0072   1.0000   1.0000   0.8373   0.8384   |            0.4459   1.0000   1.0000   0.8415   0.8423   - 22.1315 \n",
      "\n",
      "Training 129    - 100    -            0.4840   0.0072   1.0000   1.0000   0.8431   0.8441   |            0.4437   1.0000   1.0000   0.8413   0.8421   - 22.1686 \n",
      "\n",
      "Training 130    - 100    -            0.4818   0.0072   1.0000   1.0000   0.8394   0.8416   |            0.4416   1.0000   1.0000   0.8404   0.8420   - 22.2073 \n",
      "\n",
      "Training 131    - 100    -            0.4794   0.0071   1.0000   1.0000   0.8417   0.8431   |            0.4395   1.0000   1.0000   0.8413   0.8425   - 22.1768 \n",
      "\n",
      "Training 132    - 100    -            0.4770   0.0071   1.0000   1.0000   0.8441   0.8453   |            0.4374   1.0000   1.0000   0.8445   0.8457   - 22.1509 \n",
      "\n",
      "Training 133    - 100    -            0.4746   0.0070   0.9987   0.9993   0.8415   0.8420   |            0.4353   1.0000   1.0000   0.8411   0.8426   - 22.1875 \n",
      "\n",
      "Training 134    - 100    -            0.4724   0.0070   0.9975   0.9980   0.8389   0.8409   |            0.4334   1.0000   1.0000   0.8427   0.8438   - 22.1525 \n",
      "\n",
      "Training 135    - 100    -            0.4702   0.0069   1.0000   1.0000   0.8321   0.8342   |            0.4314   1.0000   1.0000   0.8405   0.8408   - 22.2741 \n",
      "\n",
      "Training 136    - 100    -            0.4681   0.0069   1.0000   1.0000   0.8423   0.8446   |            0.4293   1.0000   1.0000   0.8419   0.8430   - 22.0982 \n",
      "\n",
      "Training 137    - 100    -            0.4659   0.0069   1.0000   1.0000   0.8460   0.8476   |            0.4275   1.0000   1.0000   0.8438   0.8454   - 22.3306 \n",
      "\n",
      "Training 138    - 100    -            0.4637   0.0068   1.0000   1.0000   0.8406   0.8420   |            0.4255   0.9987   0.9987   0.8412   0.8426   - 21.9494 \n",
      "\n",
      "Training 139    - 100    -            0.4617   0.0068   1.0000   1.0000   0.8436   0.8445   |            0.4236   1.0000   1.0000   0.8421   0.8438   - 22.2044 \n",
      "\n",
      "Training 140    - 100    -            0.4595   0.0067   1.0000   1.0000   0.8439   0.8451   |            0.4218   1.0000   1.0000   0.8442   0.8464   - 22.2990 \n",
      "\n",
      "Training 141    - 100    -            0.4575   0.0067   1.0000   1.0000   0.8434   0.8456   |            0.4199   1.0000   1.0000   0.8434   0.8449   - 22.0606 \n",
      "\n",
      "Training 142    - 100    -            0.4554   0.0067   1.0000   1.0000   0.8413   0.8441   |            0.4182   1.0000   1.0000   0.8428   0.8445   - 22.1896 \n",
      "\n",
      "Training 143    - 100    -            0.4534   0.0066   1.0000   1.0000   0.8451   0.8461   |            0.4164   1.0000   1.0000   0.8465   0.8473   - 22.0845 \n",
      "\n",
      "Training 144    - 100    -            0.4515   0.0066   1.0000   1.0000   0.8432   0.8442   |            0.4148   1.0000   1.0000   0.8421   0.8439   - 22.1207 \n",
      "\n",
      "Training 145    - 100    -            0.4496   0.0065   1.0000   1.0000   0.8450   0.8463   |            0.4131   1.0000   1.0000   0.8423   0.8446   - 22.2056 \n",
      "\n",
      "Training 146    - 100    -            0.4477   0.0065   0.9962   0.9967   0.8396   0.8404   |            0.4115   0.9975   0.9979   0.8414   0.8432   - 22.3018 \n",
      "\n",
      "Training 147    - 100    -            0.4460   0.0065   0.9962   0.9968   0.8269   0.8278   |            0.4098   1.0000   1.0000   0.8437   0.8448   - 22.0903 \n",
      "\n",
      "Training 148    - 100    -            0.4443   0.0065   0.9987   0.9987   0.8320   0.8349   |            0.4083   1.0000   1.0000   0.8424   0.8443   - 22.0855 \n",
      "\n",
      "Training 149    - 100    -            0.4425   0.0064   1.0000   1.0000   0.8433   0.8448   |            0.4067   1.0000   1.0000   0.8440   0.8445   - 22.2651 \n",
      "\n",
      "Training 150    - 100    -            0.4408   0.0064   1.0000   1.0000   0.8445   0.8455   |            0.4051   1.0000   1.0000   0.8458   0.8470   - 22.3445 \n",
      "\n",
      "Training 151    - 100    -            0.4392   0.0064   0.9987   0.9987   0.8434   0.8444   |            0.4037   1.0000   1.0000   0.8465   0.8481   - 22.1228 \n",
      "\n",
      "Training 152    - 100    -            0.4375   0.0064   1.0000   1.0000   0.8409   0.8426   |            0.4022   1.0000   1.0000   0.8440   0.8448   - 22.2105 \n",
      "\n",
      "Training 153    - 100    -            0.4360   0.0063   1.0000   1.0000   0.8441   0.8460   |            0.4008   1.0000   1.0000   0.8441   0.8460   - 22.2472 \n",
      "\n",
      "Training 154    - 100    -            0.4344   0.0063   1.0000   1.0000   0.8438   0.8452   |            0.3993   1.0000   1.0000   0.8462   0.8473   - 22.2272 \n",
      "\n",
      "Training 155    - 100    -            0.4329   0.0062   1.0000   1.0000   0.8441   0.8461   |            0.3980   1.0000   1.0000   0.8444   0.8458   - 22.1993 \n",
      "\n",
      "Training 156    - 100    -            0.4312   0.0062   1.0000   1.0000   0.8462   0.8486   |            0.3966   1.0000   1.0000   0.8472   0.8484   - 22.2921 \n",
      "\n",
      "Training 157    - 100    -            0.4295   0.0062   1.0000   1.0000   0.8486   0.8497   |            0.3951   1.0000   1.0000   0.8481   0.8494   - 22.3132 \n",
      "\n",
      "Training 158    - 100    -            0.4279   0.0061   1.0000   1.0000   0.8496   0.8509   |            0.3937   1.0000   1.0000   0.8496   0.8501   - 22.1587 \n",
      "\n",
      "Training 159    - 100    -            0.4264   0.0061   1.0000   1.0000   0.8445   0.8458   |            0.3923   1.0000   1.0000   0.8443   0.8449   - 22.3287 \n",
      "\n",
      "Training 160    - 100    -            0.4249   0.0061   0.9975   0.9975   0.8428   0.8438   |            0.3910   0.9987   0.9987   0.8475   0.8488   - 22.1267 \n",
      "\n",
      "Training 161    - 100    -            0.4233   0.0060   1.0000   1.0000   0.8443   0.8455   |            0.3896   1.0000   1.0000   0.8456   0.8474   - 22.0195 \n",
      "\n",
      "Training 162    - 100    -            0.4219   0.0060   1.0000   1.0000   0.8462   0.8480   |            0.3883   1.0000   1.0000   0.8481   0.8492   - 22.1468 \n",
      "\n",
      "Training 163    - 100    -            0.4205   0.0060   1.0000   1.0000   0.8445   0.8456   |            0.3871   1.0000   1.0000   0.8455   0.8472   - 22.1782 \n",
      "\n",
      "Training 164    - 100    -            0.4191   0.0059   0.9987   0.9980   0.8470   0.8475   |            0.3859   0.9987   0.9993   0.8474   0.8480   - 22.3700 \n",
      "\n",
      "Training 165    - 100    -            0.4177   0.0059   1.0000   1.0000   0.8441   0.8453   |            0.3846   1.0000   1.0000   0.8463   0.8482   - 22.1166 \n",
      "\n",
      "Training 166    - 100    -            0.4162   0.0059   1.0000   1.0000   0.8447   0.8458   |            0.3833   1.0000   1.0000   0.8452   0.8474   - 22.2041 \n",
      "\n",
      "Training 167    - 100    -            0.4147   0.0058   1.0000   1.0000   0.8449   0.8458   |            0.3820   1.0000   1.0000   0.8461   0.8475   - 22.2160 \n",
      "\n",
      "Training 168    - 100    -            0.4133   0.0058   1.0000   1.0000   0.8476   0.8487   |            0.3808   1.0000   1.0000   0.8468   0.8483   - 22.3448 \n",
      "\n",
      "Training 169    - 100    -            0.4119   0.0058   0.9987   0.9993   0.8426   0.8433   |            0.3795   0.9987   0.9993   0.8442   0.8448   - 22.2781 \n",
      "\n",
      "Training 170    - 100    -            0.4105   0.0058   1.0000   1.0000   0.8445   0.8459   |            0.3783   1.0000   1.0000   0.8471   0.8489   - 22.1681 \n",
      "\n",
      "Training 171    - 100    -            0.4092   0.0057   0.9987   0.9987   0.8432   0.8442   |            0.3771   0.9987   0.9987   0.8449   0.8458   - 22.4378 \n",
      "\n",
      "Training 172    - 100    -            0.4079   0.0057   0.9987   0.9979   0.8493   0.8501   |            0.3759   0.9987   0.9987   0.8484   0.8490   - 22.3517 \n",
      "\n",
      "Training 173    - 100    -            0.4066   0.0057   1.0000   1.0000   0.8451   0.8466   |            0.3747   1.0000   1.0000   0.8469   0.8487   - 22.4062 \n",
      "\n",
      "Training 174    - 100    -            0.4053   0.0056   0.9987   0.9987   0.8471   0.8481   |            0.3736   0.9987   0.9987   0.8480   0.8491   - 22.3317 \n",
      "\n",
      "Training 175    - 100    -            0.4040   0.0056   0.9987   0.9987   0.8476   0.8488   |            0.3724   0.9987   0.9987   0.8508   0.8515   - 22.1369 \n",
      "\n",
      "Training 176    - 100    -            0.4027   0.0056   1.0000   1.0000   0.8425   0.8445   |            0.3713   1.0000   1.0000   0.8459   0.8470   - 22.2054 \n",
      "\n",
      "Training 177    - 100    -            0.4015   0.0056   1.0000   1.0000   0.8418   0.8436   |            0.3702   1.0000   1.0000   0.8427   0.8440   - 22.1639 \n",
      "\n",
      "Training 178    - 100    -            0.4003   0.0055   1.0000   1.0000   0.8469   0.8479   |            0.3691   0.9987   0.9987   0.8478   0.8485   - 22.0841 \n",
      "\n",
      "Training 179    - 100    -            0.3991   0.0055   1.0000   1.0000   0.8476   0.8484   |            0.3680   1.0000   1.0000   0.8472   0.8482   - 22.3304 \n",
      "\n",
      "Training 180    - 100    -            0.3979   0.0055   1.0000   1.0000   0.8454   0.8470   |            0.3670   1.0000   1.0000   0.8464   0.8479   - 22.2179 \n",
      "\n",
      "Training 181    - 100    -            0.3968   0.0054   1.0000   1.0000   0.8435   0.8456   |            0.3660   1.0000   1.0000   0.8441   0.8462   - 22.3934 \n",
      "\n",
      "Training 182    - 100    -            0.3956   0.0054   1.0000   1.0000   0.8414   0.8425   |            0.3650   1.0000   1.0000   0.8409   0.8418   - 22.4822 \n",
      "\n",
      "Training 183    - 100    -            0.3944   0.0054   1.0000   1.0000   0.8452   0.8462   |            0.3640   1.0000   1.0000   0.8450   0.8470   - 22.3877 \n",
      "\n",
      "Training 184    - 100    -            0.3932   0.0054   1.0000   1.0000   0.8434   0.8457   |            0.3629   1.0000   1.0000   0.8434   0.8447   - 22.3604 \n",
      "\n",
      "Training 185    - 100    -            0.3922   0.0053   0.9987   0.9987   0.8450   0.8468   |            0.3620   0.9987   0.9987   0.8459   0.8474   - 22.2862 \n",
      "\n",
      "Training 186    - 100    -            0.3911   0.0053   1.0000   1.0000   0.8458   0.8465   |            0.3611   1.0000   1.0000   0.8461   0.8464   - 22.2484 \n",
      "\n",
      "Training 187    - 100    -            0.3899   0.0053   1.0000   1.0000   0.8445   0.8458   |            0.3601   1.0000   1.0000   0.8440   0.8455   - 22.2941 \n",
      "\n",
      "Training 188    - 100    -            0.3889   0.0053   1.0000   1.0000   0.8411   0.8421   |            0.3592   1.0000   1.0000   0.8407   0.8421   - 22.3299 \n",
      "\n",
      "Training 189    - 100    -            0.3878   0.0052   1.0000   1.0000   0.8447   0.8465   |            0.3583   1.0000   1.0000   0.8447   0.8464   - 22.4738 \n",
      "\n",
      "Training 190    - 100    -            0.3867   0.0052   1.0000   1.0000   0.8471   0.8481   |            0.3573   1.0000   1.0000   0.8473   0.8483   - 22.3602 \n",
      "\n",
      "Training 191    - 100    -            0.3857   0.0052   1.0000   1.0000   0.8444   0.8458   |            0.3565   1.0000   1.0000   0.8439   0.8456   - 22.3679 \n",
      "\n",
      "Training 192    - 100    -            0.3847   0.0051   1.0000   1.0000   0.8462   0.8477   |            0.3555   1.0000   1.0000   0.8459   0.8472   - 22.3031 \n",
      "\n",
      "Training 193    - 100    -            0.3836   0.0051   1.0000   1.0000   0.8453   0.8471   |            0.3546   1.0000   1.0000   0.8455   0.8470   - 22.3637 \n",
      "\n",
      "Training 194    - 100    -            0.3825   0.0051   1.0000   1.0000   0.8454   0.8478   |            0.3537   1.0000   1.0000   0.8452   0.8472   - 22.3853 \n",
      "\n",
      "Training 195    - 100    -            0.3815   0.0051   0.9987   0.9987   0.8451   0.8469   |            0.3528   0.9987   0.9987   0.8452   0.8468   - 22.3844 \n",
      "\n",
      "Training 196    - 100    -            0.3805   0.0051   1.0000   1.0000   0.8452   0.8466   |            0.3519   1.0000   1.0000   0.8457   0.8469   - 22.3337 \n",
      "\n",
      "Training 197    - 100    -            0.3795   0.0050   1.0000   1.0000   0.8444   0.8454   |            0.3511   1.0000   1.0000   0.8443   0.8456   - 22.1266 \n",
      "\n",
      "Training 198    - 100    -            0.3785   0.0050   1.0000   1.0000   0.8435   0.8458   |            0.3502   1.0000   1.0000   0.8430   0.8453   - 22.2753 \n",
      "\n",
      "Training 199    - 100    -            0.3777   0.0050   1.0000   1.0000   0.8453   0.8470   |            0.3495   1.0000   1.0000   0.8452   0.8472   - 22.3282 \n",
      "\n",
      "Training 200    - 100    -            0.3767   0.0050   1.0000   1.0000   0.8422   0.8443   |            0.3487   1.0000   1.0000   0.8426   0.8443   - 22.2177 \n"
     ]
    }
   ],
   "source": [
    "print(header)\n",
    "\n",
    "start_epoch = checkpoint.epoch_counter\n",
    "end_epoch = args.nb_epoch\n",
    "\n",
    "for e in range(start_epoch, args.nb_epoch):\n",
    "    train(e)\n",
    "    val(e)\n",
    "    \n",
    "    tensorboard.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the hyper parameters and the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_source_as_img(sourcepath: str):\n",
    "    # Create a zip file of the current source code\n",
    "    from zipfile import ZipFile, ZIP_DEFLATED, ZIP_LZMA\n",
    "\n",
    "    with ZipFile(sourcepath + \".zip\", \"w\", compression=ZIP_DEFLATED, compresslevel=9) as myzip:\n",
    "        myzip.write(sourcepath)\n",
    "\n",
    "    # Read the just created zip file and store it into\n",
    "    # a uint8 numpy array\n",
    "    with open(sourcepath + \".zip\", \"rb\") as myzip:\n",
    "        zip_bin = myzip.read()\n",
    "\n",
    "    zip_bin_n = numpy.array(list(map(int, zip_bin)), dtype=numpy.uint8)\n",
    "\n",
    "    # Convert it into a 2d matrix\n",
    "    desired_dimension = 500\n",
    "    missing = desired_dimension - (zip_bin_n.size % desired_dimension)\n",
    "    zip_bin_p = numpy.concatenate((zip_bin_n, numpy.array([0]*missing, dtype=numpy.uint8)))\n",
    "    zip_bin_i = numpy.asarray(zip_bin_p).reshape((desired_dimension, zip_bin_p.size // desired_dimension))\n",
    "\n",
    "    return zip_bin_i, missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'student-teacher.ipynb'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-fc4c94f8885e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msource_bin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_source_as_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"student-teacher.ipynb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_bin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-c5814275eb65>\u001b[0m in \u001b[0;36msave_source_as_img\u001b[0;34m(sourcepath)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msourcepath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mZIP_DEFLATED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmyzip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mmyzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msourcepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Read the just created zip file and store it into\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/dct/lib/python3.8/zipfile.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, filename, arcname, compress_type, compresslevel)\u001b[0m\n\u001b[1;32m   1720\u001b[0m             )\n\u001b[1;32m   1721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1722\u001b[0;31m         zinfo = ZipInfo.from_file(filename, arcname,\n\u001b[0m\u001b[1;32m   1723\u001b[0m                                   strict_timestamps=self._strict_timestamps)\n\u001b[1;32m   1724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/dct/lib/python3.8/zipfile.py\u001b[0m in \u001b[0;36mfrom_file\u001b[0;34m(cls, filename, arcname, strict_timestamps)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m         \u001b[0misdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS_ISDIR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0mmtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocaltime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'student-teacher.ipynb'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "source_bin, _ = save_source_as_img(\"student-teacher.ipynb\")\n",
    "plt.imshow(source_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {}\n",
    "for key, value in args.__dict__.items():\n",
    "    hparams[key] = str(value)\n",
    "    \n",
    "final_metrics = {\n",
    "    \"max_student_acc\": maximum_tracker.max[\"student_acc\"],\n",
    "    \"max_teacher_acc\": maximum_tracker.max[\"teacher_acc\"],\n",
    "    \"max_student_f1\": maximum_tracker.max[\"student_f1\"],\n",
    "    \"max_teacher_f1\": maximum_tracker.max[\"teacher_f1\"],\n",
    "}\n",
    "tensorboard.add_hparams(hparams, final_metrics)\n",
    "\n",
    "source_code_img, padding_size = save_source_as_img(\"student-teacher.ipynb\")\n",
    "tensorboard.add_image(\"student-teacher___%s\" % padding_size, source_code_img , 0, dataformats=\"HW\")\n",
    "tensorboard.flush()\n",
    "tensorboard.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((500, 948), 474000)"
      ]
     },
     "metadata": {},
     "execution_count": 110
    }
   ],
   "source": [
    "source_code_img.shape, source_code_img.size"
   ]
  },
  {
   "source": [
    "from hashlib import md5\n",
    "md5(source_bin.flatten()).hexdigest(), md5(source_code_img.flatten()).hexdigest()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from PIL import Image\n",
    "\n",
    "im = Image.open(\"tmp-232.png\")\n",
    "source_bin = numpy.asarray(im, dtype=numpy.uint8)\n",
    "source_bin = source_bin[:, :, 0]\n",
    "\n",
    "source_bin = source_bin.flatten()[:-232]\n",
    "\n",
    "with open(\"student-teacher.ipynb.zip.bak\", \"wb\") as mynewzip:\n",
    "    mynewzip.write(source_bin)\n",
    "\n",
    "# PROUT"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 2160x1008 with 2 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"377.005398pt\" version=\"1.1\" viewBox=\"0 0 1711.303125 377.005398\" width=\"1711.303125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 377.005398 \nL 1711.303125 377.005398 \nL 1711.303125 -0 \nL 0 -0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 353.127273 \nL 522.456066 353.127273 \nL 522.456066 7.2 \nL 30.103125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m3ef3fd7c31\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.482804\" xlink:href=\"#m3ef3fd7c31\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(49.301554 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.713154\" xlink:href=\"#m3ef3fd7c31\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 25 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(102.350654 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"164.943503\" xlink:href=\"#m3ef3fd7c31\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 50 -->\n      <g transform=\"translate(158.581003 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"221.173853\" xlink:href=\"#m3ef3fd7c31\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 75 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g transform=\"translate(214.811353 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"277.404203\" xlink:href=\"#m3ef3fd7c31\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(267.860453 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"333.634552\" xlink:href=\"#m3ef3fd7c31\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 125 -->\n      <g transform=\"translate(324.090802 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"389.864902\" xlink:href=\"#m3ef3fd7c31\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 150 -->\n      <g transform=\"translate(380.321152 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"446.095251\" xlink:href=\"#m3ef3fd7c31\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 175 -->\n      <g transform=\"translate(436.551501 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"502.325601\" xlink:href=\"#m3ef3fd7c31\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 200 -->\n      <g transform=\"translate(492.781851 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m2d49fa9480\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m2d49fa9480\" y=\"327.092517\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.2 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(7.2 330.891736)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m2d49fa9480\" y=\"270.760351\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.3 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(7.2 274.55957)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m2d49fa9480\" y=\"214.428186\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.4 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(7.2 218.227405)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m2d49fa9480\" y=\"158.09602\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.5 -->\n      <g transform=\"translate(7.2 161.895239)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m2d49fa9480\" y=\"101.763855\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.6 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(7.2 105.563074)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m2d49fa9480\" y=\"45.43169\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.7 -->\n      <g transform=\"translate(7.2 49.230908)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#p4b0301d8fd)\" d=\"M 52.482804 337.403306 \nL 54.732018 326.715282 \nL 56.981232 252.024861 \nL 59.230446 252.024861 \nL 61.47966 233.037922 \nL 63.728874 142.252572 \nL 81.722586 142.252572 \nL 83.9718 123.265616 \nL 86.221014 123.265616 \nL 88.470228 112.577584 \nL 92.968656 112.577584 \nL 95.21787 81.896676 \nL 106.46394 81.896676 \nL 108.713154 75.10665 \nL 126.706866 75.10665 \nL 128.95608 71.58591 \nL 131.205294 51.593 \nL 133.454508 40.527769 \nL 146.949791 40.527769 \nL 149.199005 34.240687 \nL 176.189573 34.240687 \nL 178.438787 34.114943 \nL 194.183285 34.114943 \nL 196.432499 28.456583 \nL 324.637696 28.456583 \nL 326.88691 26.067474 \nL 333.634552 26.067474 \nL 335.883766 22.923967 \nL 500.076387 22.923967 \nL 500.076387 22.923967 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path clip-path=\"url(#p4b0301d8fd)\" d=\"M 52.482804 272.269243 \nL 54.732018 272.269243 \nL 56.981232 269.377191 \nL 59.230446 255.797106 \nL 61.47966 245.109091 \nL 63.728874 224.487493 \nL 65.978088 213.799478 \nL 68.227302 213.799478 \nL 70.476516 188.651185 \nL 72.72573 180.980949 \nL 74.974944 154.198014 \nL 81.722586 154.198014 \nL 83.9718 132.696239 \nL 86.221014 132.696239 \nL 88.470228 130.810108 \nL 92.968656 130.810108 \nL 95.21787 107.67369 \nL 99.716298 107.67369 \nL 101.965512 103.146996 \nL 104.214726 102.518274 \nL 106.46394 102.518274 \nL 108.713154 92.458963 \nL 110.962368 83.154085 \nL 113.211582 83.154085 \nL 115.460796 83.028341 \nL 117.71001 71.334422 \nL 119.959224 71.334422 \nL 122.208438 71.208677 \nL 124.457652 71.208677 \nL 126.706866 64.16713 \nL 128.95608 59.766179 \nL 131.205294 59.766179 \nL 133.454508 36.755506 \nL 144.700578 36.755506 \nL 146.949791 34.743631 \nL 149.199005 31.725868 \nL 162.694289 31.725868 \nL 164.943503 28.582327 \nL 180.688001 28.582327 \nL 182.937215 26.067474 \nL 344.880622 26.067474 \nL 347.129836 24.181377 \nL 500.076387 24.181377 \nL 500.076387 24.181377 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 353.127273 \nL 30.103125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 522.456066 353.127273 \nL 522.456066 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 353.127273 \nL 522.456066 353.127273 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 7.2 \nL 522.456066 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 271.796691 348.127273 \nL 515.456066 348.127273 \nQ 517.456066 348.127273 517.456066 346.127273 \nL 517.456066 317.214773 \nQ 517.456066 315.214773 515.456066 315.214773 \nL 271.796691 315.214773 \nQ 269.796691 315.214773 269.796691 317.214773 \nL 269.796691 346.127273 \nQ 269.796691 348.127273 271.796691 348.127273 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_18\">\n     <path d=\"M 273.796691 323.31321 \nL 293.796691 323.31321 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_19\"/>\n    <g id=\"text_16\">\n     <!-- max/student_acc = 0.7399553656578064 -->\n     <defs>\n      <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 54.890625 54.6875 \nL 35.109375 28.078125 \nL 55.90625 0 \nL 45.3125 0 \nL 29.390625 21.484375 \nL 13.484375 0 \nL 2.875 0 \nL 24.125 28.609375 \nL 4.6875 54.6875 \nL 15.28125 54.6875 \nL 29.78125 35.203125 \nL 44.28125 54.6875 \nz\n\" id=\"DejaVuSans-120\"/>\n      <path d=\"M 25.390625 72.90625 \nL 33.6875 72.90625 \nL 8.296875 -9.28125 \nL 0 -9.28125 \nz\n\" id=\"DejaVuSans-47\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path d=\"M 50.984375 -16.609375 \nL 50.984375 -23.578125 \nL -0.984375 -23.578125 \nL -0.984375 -16.609375 \nz\n\" id=\"DejaVuSans-95\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 10.59375 45.40625 \nL 73.1875 45.40625 \nL 73.1875 37.203125 \nL 10.59375 37.203125 \nz\nM 10.59375 25.484375 \nL 73.1875 25.484375 \nL 73.1875 17.1875 \nL 10.59375 17.1875 \nz\n\" id=\"DejaVuSans-61\"/>\n      <path d=\"M 10.984375 1.515625 \nL 10.984375 10.5 \nQ 14.703125 8.734375 18.5 7.8125 \nQ 22.3125 6.890625 25.984375 6.890625 \nQ 35.75 6.890625 40.890625 13.453125 \nQ 46.046875 20.015625 46.78125 33.40625 \nQ 43.953125 29.203125 39.59375 26.953125 \nQ 35.25 24.703125 29.984375 24.703125 \nQ 19.046875 24.703125 12.671875 31.3125 \nQ 6.296875 37.9375 6.296875 49.421875 \nQ 6.296875 60.640625 12.9375 67.421875 \nQ 19.578125 74.21875 30.609375 74.21875 \nQ 43.265625 74.21875 49.921875 64.515625 \nQ 56.59375 54.828125 56.59375 36.375 \nQ 56.59375 19.140625 48.40625 8.859375 \nQ 40.234375 -1.421875 26.421875 -1.421875 \nQ 22.703125 -1.421875 18.890625 -0.6875 \nQ 15.09375 0.046875 10.984375 1.515625 \nz\nM 30.609375 32.421875 \nQ 37.25 32.421875 41.125 36.953125 \nQ 45.015625 41.5 45.015625 49.421875 \nQ 45.015625 57.28125 41.125 61.84375 \nQ 37.25 66.40625 30.609375 66.40625 \nQ 23.96875 66.40625 20.09375 61.84375 \nQ 16.21875 57.28125 16.21875 49.421875 \nQ 16.21875 41.5 20.09375 36.953125 \nQ 23.96875 32.421875 30.609375 32.421875 \nz\n\" id=\"DejaVuSans-57\"/>\n      <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n     </defs>\n     <g transform=\"translate(301.796691 326.81321)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"158.691406\" xlink:href=\"#DejaVuSans-120\"/>\n      <use x=\"217.871094\" xlink:href=\"#DejaVuSans-47\"/>\n      <use x=\"251.5625\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"303.662109\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"342.871094\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"406.25\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"469.726562\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"531.25\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"594.628906\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"633.837891\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"683.837891\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"745.117188\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"800.097656\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"855.078125\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"886.865234\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"970.654297\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1002.441406\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"1066.064453\" xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"1097.851562\" xlink:href=\"#DejaVuSans-55\"/>\n      <use x=\"1161.474609\" xlink:href=\"#DejaVuSans-51\"/>\n      <use x=\"1225.097656\" xlink:href=\"#DejaVuSans-57\"/>\n      <use x=\"1288.720703\" xlink:href=\"#DejaVuSans-57\"/>\n      <use x=\"1352.34375\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1415.966797\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1479.589844\" xlink:href=\"#DejaVuSans-51\"/>\n      <use x=\"1543.212891\" xlink:href=\"#DejaVuSans-54\"/>\n      <use x=\"1606.835938\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1670.458984\" xlink:href=\"#DejaVuSans-54\"/>\n      <use x=\"1734.082031\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1797.705078\" xlink:href=\"#DejaVuSans-55\"/>\n      <use x=\"1861.328125\" xlink:href=\"#DejaVuSans-56\"/>\n      <use x=\"1924.951172\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"1988.574219\" xlink:href=\"#DejaVuSans-54\"/>\n      <use x=\"2052.197266\" xlink:href=\"#DejaVuSans-52\"/>\n     </g>\n    </g>\n    <g id=\"line2d_20\">\n     <path d=\"M 273.796691 338.26946 \nL 293.796691 338.26946 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_21\"/>\n    <g id=\"text_17\">\n     <!-- max/teacher_acc = 0.7377232313156128 -->\n     <defs>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n     </defs>\n     <g transform=\"translate(301.796691 341.76946)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"158.691406\" xlink:href=\"#DejaVuSans-120\"/>\n      <use x=\"217.871094\" xlink:href=\"#DejaVuSans-47\"/>\n      <use x=\"251.5625\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"290.771484\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"352.294922\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"413.574219\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"468.554688\" xlink:href=\"#DejaVuSans-104\"/>\n      <use x=\"531.933594\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"593.457031\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"634.570312\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"684.570312\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"745.849609\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"800.830078\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"855.810547\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"887.597656\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"971.386719\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1003.173828\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"1066.796875\" xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"1098.583984\" xlink:href=\"#DejaVuSans-55\"/>\n      <use x=\"1162.207031\" xlink:href=\"#DejaVuSans-51\"/>\n      <use x=\"1225.830078\" xlink:href=\"#DejaVuSans-55\"/>\n      <use x=\"1289.453125\" xlink:href=\"#DejaVuSans-55\"/>\n      <use x=\"1353.076172\" xlink:href=\"#DejaVuSans-50\"/>\n      <use x=\"1416.699219\" xlink:href=\"#DejaVuSans-51\"/>\n      <use x=\"1480.322266\" xlink:href=\"#DejaVuSans-50\"/>\n      <use x=\"1543.945312\" xlink:href=\"#DejaVuSans-51\"/>\n      <use x=\"1607.568359\" xlink:href=\"#DejaVuSans-49\"/>\n      <use x=\"1671.191406\" xlink:href=\"#DejaVuSans-51\"/>\n      <use x=\"1734.814453\" xlink:href=\"#DejaVuSans-49\"/>\n      <use x=\"1798.4375\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1862.060547\" xlink:href=\"#DejaVuSans-54\"/>\n      <use x=\"1925.683594\" xlink:href=\"#DejaVuSans-49\"/>\n      <use x=\"1989.306641\" xlink:href=\"#DejaVuSans-50\"/>\n      <use x=\"2052.929688\" xlink:href=\"#DejaVuSans-56\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_8\">\n    <path d=\"M 1211.750184 353.127273 \nL 1704.103125 353.127273 \nL 1704.103125 7.2 \nL 1211.750184 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_10\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1234.129863\" xlink:href=\"#m3ef3fd7c31\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 0 -->\n      <g transform=\"translate(1230.948613 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1290.360213\" xlink:href=\"#m3ef3fd7c31\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_19\">\n      <!-- 25 -->\n      <g transform=\"translate(1283.997713 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1346.590562\" xlink:href=\"#m3ef3fd7c31\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_20\">\n      <!-- 50 -->\n      <g transform=\"translate(1340.228062 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1402.820912\" xlink:href=\"#m3ef3fd7c31\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_21\">\n      <!-- 75 -->\n      <g transform=\"translate(1396.458412 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_14\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1459.051261\" xlink:href=\"#m3ef3fd7c31\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_22\">\n      <!-- 100 -->\n      <g transform=\"translate(1449.507511 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_15\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1515.281611\" xlink:href=\"#m3ef3fd7c31\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_23\">\n      <!-- 125 -->\n      <g transform=\"translate(1505.737861 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_16\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1571.511961\" xlink:href=\"#m3ef3fd7c31\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_24\">\n      <!-- 150 -->\n      <g transform=\"translate(1561.968211 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_17\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1627.74231\" xlink:href=\"#m3ef3fd7c31\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_25\">\n      <!-- 175 -->\n      <g transform=\"translate(1618.19856 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_18\">\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1683.97266\" xlink:href=\"#m3ef3fd7c31\" y=\"353.127273\"/>\n      </g>\n     </g>\n     <g id=\"text_26\">\n      <!-- 200 -->\n      <g transform=\"translate(1674.42891 367.72571)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_7\">\n     <g id=\"line2d_31\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1211.750184\" xlink:href=\"#m2d49fa9480\" y=\"337.480913\"/>\n      </g>\n     </g>\n     <g id=\"text_27\">\n      <!-- 0.0000 -->\n      <g transform=\"translate(1169.759559 341.280132)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1211.750184\" xlink:href=\"#m2d49fa9480\" y=\"285.054756\"/>\n      </g>\n     </g>\n     <g id=\"text_28\">\n      <!-- 0.0005 -->\n      <g transform=\"translate(1169.759559 288.853974)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_33\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1211.750184\" xlink:href=\"#m2d49fa9480\" y=\"232.628598\"/>\n      </g>\n     </g>\n     <g id=\"text_29\">\n      <!-- 0.0010 -->\n      <g transform=\"translate(1169.759559 236.427817)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1211.750184\" xlink:href=\"#m2d49fa9480\" y=\"180.20244\"/>\n      </g>\n     </g>\n     <g id=\"text_30\">\n      <!-- 0.0015 -->\n      <g transform=\"translate(1169.759559 184.001659)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_35\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1211.750184\" xlink:href=\"#m2d49fa9480\" y=\"127.776282\"/>\n      </g>\n     </g>\n     <g id=\"text_31\">\n      <!-- 0.0020 -->\n      <g transform=\"translate(1169.759559 131.575501)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_36\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1211.750184\" xlink:href=\"#m2d49fa9480\" y=\"75.350125\"/>\n      </g>\n     </g>\n     <g id=\"text_32\">\n      <!-- 0.0025 -->\n      <g transform=\"translate(1169.759559 79.149343)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_37\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1211.750184\" xlink:href=\"#m2d49fa9480\" y=\"22.923967\"/>\n      </g>\n     </g>\n     <g id=\"text_33\">\n      <!-- 0.0030 -->\n      <g transform=\"translate(1169.759559 26.723186)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_38\">\n    <path clip-path=\"url(#p10b6c5596c)\" d=\"M 1234.129863 22.94337 \nL 1238.628291 22.94337 \nL 1243.126719 23.098566 \nL 1247.625147 23.408804 \nL 1252.123575 23.873779 \nL 1256.622003 24.493031 \nL 1261.120431 25.265949 \nL 1265.618859 26.191772 \nL 1272.366501 27.865159 \nL 1279.114143 29.876774 \nL 1285.861785 32.222149 \nL 1292.609427 34.896078 \nL 1299.357069 37.892623 \nL 1306.10471 41.205131 \nL 1312.852352 44.826248 \nL 1319.599994 48.747934 \nL 1326.347636 52.961482 \nL 1333.095278 57.457537 \nL 1339.84292 62.226116 \nL 1346.590562 67.256632 \nL 1353.338204 72.537916 \nL 1360.085846 78.058243 \nL 1366.833488 83.805355 \nL 1375.830344 91.798825 \nL 1384.8272 100.141183 \nL 1393.824056 108.799507 \nL 1402.820912 117.739626 \nL 1414.066982 129.257203 \nL 1425.313052 141.088874 \nL 1438.808336 155.598666 \nL 1459.051261 177.732017 \nL 1486.041829 207.243186 \nL 1499.537113 221.70399 \nL 1510.783183 233.478623 \nL 1522.029253 244.92479 \nL 1531.026109 253.797727 \nL 1540.022965 262.380217 \nL 1549.019821 270.638388 \nL 1558.016677 278.53965 \nL 1564.764319 284.212561 \nL 1571.511961 289.654542 \nL 1578.259603 294.853513 \nL 1585.007245 299.797929 \nL 1591.754886 304.476814 \nL 1598.502528 308.879778 \nL 1605.25017 312.997047 \nL 1611.997812 316.819478 \nL 1618.745454 320.338586 \nL 1625.493096 323.546556 \nL 1632.240738 326.436266 \nL 1638.98838 329.001301 \nL 1645.736022 331.235965 \nL 1652.483664 333.135296 \nL 1659.231306 334.695079 \nL 1663.729734 335.544554 \nL 1668.228162 336.240725 \nL 1672.72659 336.782906 \nL 1677.225018 337.17056 \nL 1681.723446 337.403306 \nL 1681.723446 337.403306 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 1211.750184 353.127273 \nL 1211.750184 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 1704.103125 353.127273 \nL 1704.103125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 1211.750184 353.127273 \nL 1704.103125 353.127273 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path d=\"M 1211.750184 7.2 \nL 1704.103125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_2\">\n    <g id=\"patch_13\">\n     <path d=\"M 1465.215625 30.15625 \nL 1697.103125 30.15625 \nQ 1699.103125 30.15625 1699.103125 28.15625 \nL 1699.103125 14.2 \nQ 1699.103125 12.2 1697.103125 12.2 \nL 1465.215625 12.2 \nQ 1463.215625 12.2 1463.215625 14.2 \nL 1463.215625 28.15625 \nQ 1463.215625 30.15625 1465.215625 30.15625 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_39\">\n     <path d=\"M 1467.215625 20.298437 \nL 1487.215625 20.298437 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_40\"/>\n    <g id=\"text_34\">\n     <!-- hyperparameters/learning_rate = 0.003 -->\n     <defs>\n      <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n     </defs>\n     <g transform=\"translate(1495.215625 23.798437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-104\"/>\n      <use x=\"63.378906\" xlink:href=\"#DejaVuSans-121\"/>\n      <use x=\"122.558594\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"186.035156\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"247.558594\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"288.671875\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"352.148438\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"413.427734\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"454.541016\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"515.820312\" xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"613.232422\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"674.755859\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"713.964844\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"775.488281\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"816.601562\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"868.701172\" xlink:href=\"#DejaVuSans-47\"/>\n      <use x=\"902.392578\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"930.175781\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"991.699219\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"1052.978516\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"1094.076172\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"1157.455078\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"1185.238281\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"1248.617188\" xlink:href=\"#DejaVuSans-103\"/>\n      <use x=\"1312.09375\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"1362.09375\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"1403.207031\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"1464.486328\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"1503.695312\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"1565.21875\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1597.005859\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"1680.794922\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1712.582031\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"1776.205078\" xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"1807.992188\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"1871.615234\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"1935.238281\" xlink:href=\"#DejaVuSans-51\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p4b0301d8fd\">\n   <rect height=\"345.927273\" width=\"492.352941\" x=\"30.103125\" y=\"7.2\"/>\n  </clipPath>\n  <clipPath id=\"p10b6c5596c\">\n   <rect height=\"345.927273\" width=\"492.352941\" x=\"1211.750184\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = list(range(checkpoint.epoch_counter))\n",
    "sm = lambda y, w: np.convolve(y, np.ones(w)/w, mode='same')\n",
    "pp = lambda k: plt.plot(x, tensorboard.history[k], label=f\"{k} = {max(tensorboard.history[k])}\")\n",
    "spp = lambda k: plt.plot(x, sm(tensorboard.history[k], 5), label=f\"{k} = {max(tensorboard.history[k])}\")\n",
    "\n",
    "\n",
    "plt.figure(0, figsize=(30, 14))\n",
    "plt.subplot(2, 3, 1)\n",
    "pp(\"max/student_acc\")\n",
    "pp(\"max/teacher_acc\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "pp(\"hyperparameters/learning_rate\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dct",
   "language": "python",
   "name": "dct"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}