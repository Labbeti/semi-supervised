{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1703.01780.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/users/samova/lcances/.miniconda3/envs/pytorch-dev/bin/python'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import time\n",
    "import pprint\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from SSL.util.loaders import load_dataset, load_optimizer, load_callbacks, load_preprocesser\n",
    "from SSL.util.model_loader import load_model\n",
    "from SSL.util.checkpoint import CheckPoint, mSummaryWriter\n",
    "from SSL.util.mixup import MixUpBatchShuffle\n",
    "from SSL.util.utils import reset_seed, get_datetime, track_maximum, DotDict\n",
    "from SSL.ramps import Warmup, sigmoid_rampup\n",
    "from SSL.losses import JensenShanon\n",
    "\n",
    "from metric_utils.metrics import CategoricalAccuracy, FScore, ContinueAverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--from_config\", default=\"\", type=str)\n",
    "parser.add_argument(\"-d\", \"--dataset_root\", default=\"../datasets\", type=str)\n",
    "parser.add_argument(\"-D\", \"--dataset\", default=\"ubs8k\", type=str)\n",
    "\n",
    "group_t = parser.add_argument_group(\"Commun parameters\")\n",
    "group_t.add_argument(\"-m\", \"--model\", default=\"wideresnet28_2\", type=str)\n",
    "group_t.add_argument(\"--supervised_ratio\", default=0.1, type=float)\n",
    "group_t.add_argument(\"--batch_size\", default=64, type=int)\n",
    "group_t.add_argument(\"--nb_epoch\", default=200, type=int)\n",
    "group_t.add_argument(\"--learning_rate\", default=0.001, type=float)\n",
    "group_t.add_argument(\"--resume\", action=\"store_true\", default=False)\n",
    "group_t.add_argument(\"--seed\", default=1234, type=int)\n",
    "group_t.add_argument(\"--num_classes\", default=10, type=int)\n",
    "\n",
    "group_u = parser.add_argument_group(\"Datasets parameters\")\n",
    "group_u.add_argument(\"-t\", \"--train_folds\", nargs=\"+\", default=[1, 2, 3, 4, 5, 6, 7, 8, 9], type=int)\n",
    "group_u.add_argument(\"-v\", \"--val_folds\", nargs=\"+\", default=[10], type=int)\n",
    "\n",
    "group_s = parser.add_argument_group(\"Student teacher parameters\")\n",
    "group_s.add_argument(\"--ema_alpha\", default=0.999, type=float)\n",
    "group_s.add_argument(\"--warmup_length\", default=50, type=int)\n",
    "group_s.add_argument(\"--lambda_cost_max\", default=1, type=float)\n",
    "group_s.add_argument(\"--teacher_noise\", default=0, type=float)\n",
    "group_s.add_argument(\"--ccost_softmax\", action=\"store_false\", default=True)\n",
    "group_s.add_argument(\"--ccost_method\", type=str, default=\"js\")\n",
    "\n",
    "group_mixup = parser.add_argument_group(\"Mixup parameters\")\n",
    "group_mixup.add_argument(\"--mixup\", action=\"store_true\", default=False)\n",
    "group_mixup.add_argument(\"--mixup_alpha\", type=float, default=0.4)\n",
    "group_mixup.add_argument(\"--mixup_max\", action=\"store_true\", default=False)\n",
    "group_mixup.add_argument(\"--mixup_label\", action=\"store_true\", default=False)\n",
    "\n",
    "group_l = parser.add_argument_group(\"Logs\")\n",
    "group_l.add_argument(\"--checkpoint_root\", default=\"../model_save/\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_root\", default=\"../tensorboard/\", type=str)\n",
    "group_l.add_argument(\"--checkpoint_path\", default=\"mean-teacher_mixup\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_path\", default=\"mean-teacher_mixup\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_sufix\", default=\"\", type=str)\n",
    "\n",
    "args=parser.parse_args([])\n",
    "\n",
    "tensorboard_path = os.path.join(args.tensorboard_root, args.dataset, args.tensorboard_path)\n",
    "checkpoint_path = os.path.join(args.checkpoint_root, args.dataset, args.checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 64,\n",
      " 'ccost_method': 'js',\n",
      " 'ccost_softmax': True,\n",
      " 'checkpoint_path': 'mean-teacher_mixup',\n",
      " 'checkpoint_root': '../model_save/',\n",
      " 'dataset': 'ubs8k',\n",
      " 'dataset_root': '../datasets',\n",
      " 'ema_alpha': 0.999,\n",
      " 'from_config': '',\n",
      " 'lambda_cost_max': 1,\n",
      " 'learning_rate': 0.001,\n",
      " 'mixup': False,\n",
      " 'mixup_alpha': 0.4,\n",
      " 'mixup_label': False,\n",
      " 'mixup_max': False,\n",
      " 'model': 'wideresnet28_2',\n",
      " 'nb_epoch': 200,\n",
      " 'num_classes': 10,\n",
      " 'resume': False,\n",
      " 'seed': 1234,\n",
      " 'supervised_ratio': 0.1,\n",
      " 'teacher_noise': 0,\n",
      " 'tensorboard_path': 'mean-teacher_mixup',\n",
      " 'tensorboard_root': '../tensorboard/',\n",
      " 'tensorboard_sufix': '',\n",
      " 'train_folds': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      " 'val_folds': [10],\n",
      " 'warmup_length': 50}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(vars(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "reset_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/PyTorch/audio/torchaudio/extension/extension.py:14: UserWarning: torchaudio C++ extension is not available.\n",
      "  warnings.warn('torchaudio C++ extension is not available.')\n"
     ]
    }
   ],
   "source": [
    "train_transform, val_transform = load_preprocesser(args.dataset, \"mean-teacher\")\n",
    "train_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:22<00:00,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_batch_size:  6\n",
      "u_batch_size:  58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/users/samova/lcances/Datasets/UrbanSound8K/ubs8k/datasets.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.y[\"idx\"] = list(range(len(self.y)))\n"
     ]
    }
   ],
   "source": [
    "manager, train_loader, val_loader = load_dataset(\n",
    "    args.dataset,\n",
    "    \"mean-teacher\",\n",
    "    \n",
    "    dataset_root = args.dataset_root,\n",
    "    supervised_ratio = args.supervised_ratio,\n",
    "    batch_size = args.batch_size,\n",
    "    train_folds = args.train_folds,\n",
    "    val_folds = args.val_folds,\n",
    "\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    "    \n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 173)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = tuple(train_loader._iterables[0].dataset[0][0].shape)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_func = load_model(args.dataset, args.model)\n",
    "\n",
    "student = model_func(input_shape=input_shape, num_classes = args.num_classes)\n",
    "teacher = model_func(input_shape=input_shape, num_classes = args.num_classes)\n",
    "\n",
    "student = student.cuda()\n",
    "teacher = teacher.cuda()\n",
    "\n",
    "# We do not need gradient for the teacher model\n",
    "for p in teacher.parameters():\n",
    "    p.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 32, 64, 173]             288\n",
      "       BatchNorm2d-2          [-1, 32, 64, 173]              64\n",
      "              ReLU-3          [-1, 32, 64, 173]               0\n",
      "         MaxPool2d-4           [-1, 32, 32, 87]               0\n",
      "            Conv2d-5           [-1, 32, 32, 87]           9,216\n",
      "       BatchNorm2d-6           [-1, 32, 32, 87]              64\n",
      "              ReLU-7           [-1, 32, 32, 87]               0\n",
      "            Conv2d-8           [-1, 32, 32, 87]           9,216\n",
      "       BatchNorm2d-9           [-1, 32, 32, 87]              64\n",
      "             ReLU-10           [-1, 32, 32, 87]               0\n",
      "       BasicBlock-11           [-1, 32, 32, 87]               0\n",
      "           Conv2d-12           [-1, 32, 32, 87]           9,216\n",
      "      BatchNorm2d-13           [-1, 32, 32, 87]              64\n",
      "             ReLU-14           [-1, 32, 32, 87]               0\n",
      "           Conv2d-15           [-1, 32, 32, 87]           9,216\n",
      "      BatchNorm2d-16           [-1, 32, 32, 87]              64\n",
      "             ReLU-17           [-1, 32, 32, 87]               0\n",
      "       BasicBlock-18           [-1, 32, 32, 87]               0\n",
      "           Conv2d-19           [-1, 32, 32, 87]           9,216\n",
      "      BatchNorm2d-20           [-1, 32, 32, 87]              64\n",
      "             ReLU-21           [-1, 32, 32, 87]               0\n",
      "           Conv2d-22           [-1, 32, 32, 87]           9,216\n",
      "      BatchNorm2d-23           [-1, 32, 32, 87]              64\n",
      "             ReLU-24           [-1, 32, 32, 87]               0\n",
      "       BasicBlock-25           [-1, 32, 32, 87]               0\n",
      "           Conv2d-26           [-1, 32, 32, 87]           9,216\n",
      "      BatchNorm2d-27           [-1, 32, 32, 87]              64\n",
      "             ReLU-28           [-1, 32, 32, 87]               0\n",
      "           Conv2d-29           [-1, 32, 32, 87]           9,216\n",
      "      BatchNorm2d-30           [-1, 32, 32, 87]              64\n",
      "             ReLU-31           [-1, 32, 32, 87]               0\n",
      "       BasicBlock-32           [-1, 32, 32, 87]               0\n",
      "           Conv2d-33           [-1, 64, 16, 44]          18,432\n",
      "      BatchNorm2d-34           [-1, 64, 16, 44]             128\n",
      "             ReLU-35           [-1, 64, 16, 44]               0\n",
      "           Conv2d-36           [-1, 64, 16, 44]          36,864\n",
      "      BatchNorm2d-37           [-1, 64, 16, 44]             128\n",
      "           Conv2d-38           [-1, 64, 16, 44]           2,048\n",
      "      BatchNorm2d-39           [-1, 64, 16, 44]             128\n",
      "             ReLU-40           [-1, 64, 16, 44]               0\n",
      "       BasicBlock-41           [-1, 64, 16, 44]               0\n",
      "           Conv2d-42           [-1, 64, 16, 44]          36,864\n",
      "      BatchNorm2d-43           [-1, 64, 16, 44]             128\n",
      "             ReLU-44           [-1, 64, 16, 44]               0\n",
      "           Conv2d-45           [-1, 64, 16, 44]          36,864\n",
      "      BatchNorm2d-46           [-1, 64, 16, 44]             128\n",
      "             ReLU-47           [-1, 64, 16, 44]               0\n",
      "       BasicBlock-48           [-1, 64, 16, 44]               0\n",
      "           Conv2d-49           [-1, 64, 16, 44]          36,864\n",
      "      BatchNorm2d-50           [-1, 64, 16, 44]             128\n",
      "             ReLU-51           [-1, 64, 16, 44]               0\n",
      "           Conv2d-52           [-1, 64, 16, 44]          36,864\n",
      "      BatchNorm2d-53           [-1, 64, 16, 44]             128\n",
      "             ReLU-54           [-1, 64, 16, 44]               0\n",
      "       BasicBlock-55           [-1, 64, 16, 44]               0\n",
      "           Conv2d-56           [-1, 64, 16, 44]          36,864\n",
      "      BatchNorm2d-57           [-1, 64, 16, 44]             128\n",
      "             ReLU-58           [-1, 64, 16, 44]               0\n",
      "           Conv2d-59           [-1, 64, 16, 44]          36,864\n",
      "      BatchNorm2d-60           [-1, 64, 16, 44]             128\n",
      "             ReLU-61           [-1, 64, 16, 44]               0\n",
      "       BasicBlock-62           [-1, 64, 16, 44]               0\n",
      "           Conv2d-63           [-1, 128, 8, 22]          73,728\n",
      "      BatchNorm2d-64           [-1, 128, 8, 22]             256\n",
      "             ReLU-65           [-1, 128, 8, 22]               0\n",
      "           Conv2d-66           [-1, 128, 8, 22]         147,456\n",
      "      BatchNorm2d-67           [-1, 128, 8, 22]             256\n",
      "           Conv2d-68           [-1, 128, 8, 22]           8,192\n",
      "      BatchNorm2d-69           [-1, 128, 8, 22]             256\n",
      "             ReLU-70           [-1, 128, 8, 22]               0\n",
      "       BasicBlock-71           [-1, 128, 8, 22]               0\n",
      "           Conv2d-72           [-1, 128, 8, 22]         147,456\n",
      "      BatchNorm2d-73           [-1, 128, 8, 22]             256\n",
      "             ReLU-74           [-1, 128, 8, 22]               0\n",
      "           Conv2d-75           [-1, 128, 8, 22]         147,456\n",
      "      BatchNorm2d-76           [-1, 128, 8, 22]             256\n",
      "             ReLU-77           [-1, 128, 8, 22]               0\n",
      "       BasicBlock-78           [-1, 128, 8, 22]               0\n",
      "           Conv2d-79           [-1, 128, 8, 22]         147,456\n",
      "      BatchNorm2d-80           [-1, 128, 8, 22]             256\n",
      "             ReLU-81           [-1, 128, 8, 22]               0\n",
      "           Conv2d-82           [-1, 128, 8, 22]         147,456\n",
      "      BatchNorm2d-83           [-1, 128, 8, 22]             256\n",
      "             ReLU-84           [-1, 128, 8, 22]               0\n",
      "       BasicBlock-85           [-1, 128, 8, 22]               0\n",
      "           Conv2d-86           [-1, 128, 8, 22]         147,456\n",
      "      BatchNorm2d-87           [-1, 128, 8, 22]             256\n",
      "             ReLU-88           [-1, 128, 8, 22]               0\n",
      "           Conv2d-89           [-1, 128, 8, 22]         147,456\n",
      "      BatchNorm2d-90           [-1, 128, 8, 22]             256\n",
      "             ReLU-91           [-1, 128, 8, 22]               0\n",
      "       BasicBlock-92           [-1, 128, 8, 22]               0\n",
      "AdaptiveAvgPool2d-93            [-1, 128, 1, 1]               0\n",
      "           Linear-94                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 1,471,978\n",
      "Trainable params: 1,471,978\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 43.29\n",
      "Params size (MB): 5.62\n",
      "Estimated Total Size (MB): 48.95\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "s = summary(student, input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_root=f\"{args.model}/{args.supervised_ratio}/{get_datetime()}_{model_func.__name__}\"\n",
    "checkpoint_root = f\"{args.model}/{args.supervised_ratio}/{model_func.__name__}\"\n",
    "\n",
    "# mea teacher parameters\n",
    "sufix_title = f'_{args.ema_alpha}-emaa'\n",
    "sufix_title += f'_{args.warmup_length}-wl'\n",
    "sufix_title += f'_{args.lambda_cost_max}-lccm'\n",
    "\n",
    "# mixup parameters\n",
    "if args.mixup:\n",
    "    sufix_title += \"_mixup\"\n",
    "    if args.mixup_max: sufix_title += \"-max\"\n",
    "    if args.mixup_label: sufix_title += \"-label\"\n",
    "    sufix_title += f\"-{args.mixup_alpha}-a\"\n",
    "    \n",
    "# ccost function and method\n",
    "if args.ccost_method: sufix_title += \"_cc-MSE\"\n",
    "if args.ccost_softmax: sufix_title += \"-SOFTMAX\"\n",
    "    \n",
    "# normale training parameters\n",
    "sufix_title += f'_{args.learning_rate}-lr'\n",
    "sufix_title += f'_{args.supervised_ratio}-sr'\n",
    "sufix_title += f'_{args.nb_epoch}-e'\n",
    "sufix_title += f'_{args.batch_size}-bs'\n",
    "sufix_title += f'_{args.seed}-seed'\n",
    "\n",
    "tensorboard_title = tensorboard_root + sufix_title\n",
    "checkpoint_title = checkpoint_root + sufix_title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../tensorboard/ubs8k/mean-teacher_mixup/wideresnet28_2/0.1/2021-01-18_15:41:57_wideresnet28_2_0.999-emaa_50-wl_1-lccm_cc-MSE-SOFTMAX_0.001-lr_0.1-sr_200-e_64-bs_1234-seed\n"
     ]
    }
   ],
   "source": [
    "tensorboard = mSummaryWriter(log_dir=\"%s/%s\" % (tensorboard_path, tensorboard_title), comment=model_func.__name__)\n",
    "print(os.path.join(tensorboard_path, tensorboard_title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer & callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = load_optimizer(args.dataset, \"mean-teacher\", student=student, learning_rate=args.learning_rate)\n",
    "callbacks = load_callbacks(args.dataset, \"mean-teacher\", optimizer=optimizer, nb_epoch=args.nb_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint initialise at:  /users/samova/lcances/semi-supervised/model_save/ubs8k/mean-teacher_mixup/wideresnet28_2/0.1/wideresnet28_2_0.999-emaa_50-wl_1-lccm_cc-MSE-SOFTMAX_0.001-lr_0.1-sr_200-e_64-bs_1234-seed.torch\n",
      "name:  wideresnet28_2_0.999-emaa_50-wl_1-lccm_cc-MSE-SOFTMAX_0.001-lr_0.1-sr_200-e_64-bs_1234-seed.torch\n",
      "mode:  max\n"
     ]
    }
   ],
   "source": [
    "# losses\n",
    "loss_ce = nn.CrossEntropyLoss(reduction=\"mean\") # Supervised loss\n",
    "\n",
    "if args.ccost_method == \"mse\":\n",
    "    consistency_cost = nn.MSELoss(reduction=\"mean\") # Unsupervised loss\n",
    "elif args.ccost_method == \"js\":\n",
    "    consistency_cost = JensenShanon\n",
    "        \n",
    "lambda_cost = Warmup(args.lambda_cost_max, args.warmup_length, sigmoid_rampup)\n",
    "callbacks += [lambda_cost]\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint = CheckPoint(student, optimizer, mode=\"max\", name=\"%s/%s.torch\" % (checkpoint_path, checkpoint_title))\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function SSL.losses.JensenShanon(logits_1, logits_2)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consistency_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_calculator():\n",
    "    def c(logits, y):\n",
    "        with torch.no_grad():\n",
    "            y_one_hot = F.one_hot(y, num_classes=args.num_classes)\n",
    "            \n",
    "            pred = torch.softmax(logits, dim=1)\n",
    "            arg = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            acc = c.fn.acc(arg, y).mean\n",
    "            f1 = c.fn.f1(pred, y_one_hot).mean\n",
    "            \n",
    "            return acc, f1,\n",
    "            \n",
    "    c.fn = DotDict(\n",
    "        acc = CategoricalAccuracy(),\n",
    "        f1 = FScore(),\n",
    "    )\n",
    "    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_student_s_metrics = metrics_calculator()\n",
    "calc_student_u_metrics = metrics_calculator()\n",
    "calc_teacher_s_metrics = metrics_calculator()\n",
    "calc_teacher_u_metrics = metrics_calculator()\n",
    "\n",
    "avg_Sce = ContinueAverage()\n",
    "avg_Tce = ContinueAverage()\n",
    "avg_ccost = ContinueAverage()\n",
    "\n",
    "softmax_fn = lambda x: x\n",
    "if args.ccost_softmax:\n",
    "    softmax_fn = nn.Softmax(dim=1)\n",
    "\n",
    "def reset_metrics():\n",
    "    for d in [calc_student_s_metrics.fn, calc_student_u_metrics.fn, calc_teacher_s_metrics.fn, calc_teacher_u_metrics.fn]:\n",
    "        for fn in d.values():\n",
    "            fn.reset()\n",
    "\n",
    "maximum_tracker = track_maximum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Can resume previous training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if args.resume:\n",
    "    checkpoint.load_last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".        Epoch  - %      - Student:   ce       ccost    acc_s    f1_s     acc_u    f1_u     | Teacher:   ce       acc_s    f1_s     acc_u    f1_u     - Time    \n"
     ]
    }
   ],
   "source": [
    "UNDERLINE_SEQ = \"\\033[1;4m\"\n",
    "RESET_SEQ = \"\\033[0m\"\n",
    "\n",
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<10.8} {:<8.6} {:<8.6} {:<8.6} {:<8.6} {:<8.6} {:<8.6} | {:<10.8} {:<8.6} {:<8.6} {:<8.6} {:<8.6} {:<8.6} - {:<8.6}\"\n",
    "value_form  = \"{:<8.8} {:<6d} - {:<6d} - {:<10.8} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} | {:<10.8} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} - {:<8.4f}\"\n",
    "header = header_form.format(\".               \", \"Epoch\",  \"%\", \"Student:\", \"ce\", \"ccost\", \"acc_s\", \"f1_s\", \"acc_u\", \"f1_u\", \"Teacher:\", \"ce\", \"acc_s\", \"f1_s\", \"acc_u\", \"f1_u\" , \"Time\")\n",
    "\n",
    "train_form = value_form\n",
    "val_form = UNDERLINE_SEQ + value_form + RESET_SEQ\n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_teacher_model(student_model, teacher_model, alpha, epoch):\n",
    "    \n",
    "    # Use the true average until the exponential average is more correct\n",
    "    alpha = min(1 - 1 / (epoch + 1), alpha)\n",
    "    \n",
    "    for param, ema_param in zip(student_model.parameters(), teacher_model.parameters()):\n",
    "        ema_param.data.mul_(alpha).add_(param.data,  alpha = 1-alpha)\n",
    "\n",
    "\n",
    "noise_fn = lambda x: x\n",
    "if args.teacher_noise != 0:\n",
    "    n_db = args.teacher_noise\n",
    "    noise_fn = transforms.Lambda(lambda x: x + (torch.rand(x.shape).cuda() * n_db + n_db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup_fn = MixUpBatchShuffle(alpha=args.mixup_alpha, apply_max=args.mixup_max, mix_labels=args.mixup_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "    \n",
    "    nb_batch = len(train_loader)\n",
    "\n",
    "    reset_metrics()\n",
    "    student.train()\n",
    "\n",
    "    for i, (S, U) in enumerate(train_loader):        \n",
    "        x_s, y_s = S\n",
    "        x_u, y_u = U\n",
    "\n",
    "        # Apply mixup if needed, otherwise no mixup.\n",
    "        n_x_s, n_y_s, n_x_u, n_y_u = x_s, y_s, x_u, y_u\n",
    "        if args.mixup:\n",
    "            n_x_s, n_y_s = mixup_fn(x_s, y_s)\n",
    "            n_x_u, n_y_u = mixup_fn(x_u, y_u)\n",
    "        \n",
    "        n_x_s, n_x_u = n_x_s.cuda(), n_x_u.cuda()\n",
    "        x_s, x_u = x_s.cuda(), x_u.cuda()\n",
    "        y_s, y_u = y_s.cuda(), y_u.cuda()\n",
    "        \n",
    "        # Predictions\n",
    "        student_s_logits = student(x_s)        \n",
    "        student_u_logits = student(x_u)\n",
    "        teacher_s_logits = teacher(n_x_s)\n",
    "        teacher_u_logits = teacher(n_x_u)\n",
    "        \n",
    "        # Calculate supervised loss (only student on S)\n",
    "        loss = loss_ce(student_s_logits, y_s)\n",
    "        \n",
    "        # Calculate consistency cost (mse(student(x), teacher(x))) x is S + U\n",
    "        student_logits = torch.cat((student_s_logits, student_u_logits), dim=0)\n",
    "        teacher_logits = torch.cat((teacher_s_logits, teacher_u_logits), dim=0)\n",
    "        ccost = consistency_cost(softmax_fn(student_logits), softmax_fn(teacher_logits))\n",
    "#         ccost = consistency_cost(softmax_fn(student_u_logits), softmax_fn(teacher_u_logits))\n",
    "\n",
    "        total_loss = loss + lambda_cost() * ccost\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            # Teacher prediction (for metrics purpose)\n",
    "            _teacher_loss = loss_ce(teacher_s_logits, y_s)\n",
    "            \n",
    "            # Update teacher\n",
    "            update_teacher_model(student, teacher, args.ema_alpha, epoch*nb_batch + i)\n",
    "            \n",
    "            # Compute the metrics for the student\n",
    "            student_s_metrics = calc_student_s_metrics(student_s_logits, y_s)\n",
    "            student_u_metrics = calc_student_u_metrics(student_u_logits, y_u)\n",
    "            student_s_acc, student_s_f1, student_u_acc, student_u_f1 = *student_s_metrics, *student_u_metrics\n",
    "            \n",
    "            # Compute the metrics for the teacher\n",
    "            teacher_s_metrics = calc_teacher_s_metrics(teacher_s_logits, y_s)\n",
    "            teacher_u_metrics = calc_teacher_u_metrics(teacher_u_logits, y_u)\n",
    "            teacher_s_acc, teacher_s_f1, teacher_u_acc, teacher_u_f1 = *teacher_s_metrics, *teacher_u_metrics\n",
    "            \n",
    "            # Running average of the two losses\n",
    "            student_running_loss = avg_Sce(loss.item()).mean\n",
    "            teacher_running_loss = avg_Tce(_teacher_loss.item()).mean\n",
    "            running_ccost = avg_ccost(ccost.item()).mean\n",
    "\n",
    "            # logs\n",
    "            print(train_form.format(\n",
    "                \"Training: \", epoch + 1, int(100 * (i + 1) / nb_batch),\n",
    "                \"\", student_running_loss, running_ccost, *student_s_metrics, *student_u_metrics,\n",
    "                \"\", teacher_running_loss, *teacher_s_metrics, *teacher_u_metrics,\n",
    "                time.time() - start_time),\n",
    "                end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"train/student_acc_s\", student_s_acc, epoch)\n",
    "    tensorboard.add_scalar(\"train/student_acc_u\", student_u_acc, epoch)\n",
    "    tensorboard.add_scalar(\"train/student_f1_s\", student_s_f1, epoch)\n",
    "    tensorboard.add_scalar(\"train/student_f1_u\", student_u_f1, epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"train/teacher_acc_s\", teacher_s_acc, epoch)\n",
    "    tensorboard.add_scalar(\"train/teacher_acc_u\", teacher_u_acc, epoch)\n",
    "    tensorboard.add_scalar(\"train/teacher_f1_s\", teacher_s_f1, epoch)\n",
    "    tensorboard.add_scalar(\"train/teacher_f1_u\", teacher_u_f1, epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"train/student_loss\", student_running_loss, epoch)\n",
    "    tensorboard.add_scalar(\"train/teacher_loss\", teacher_running_loss, epoch)\n",
    "    tensorboard.add_scalar(\"train/consistency_cost\", running_ccost, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "    reset_metrics()\n",
    "    student.eval()\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (X, y) in enumerate(val_loader):\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            # Predictions\n",
    "            student_logits = student(X)        \n",
    "            teacher_logits = teacher(X)\n",
    "\n",
    "            # Calculate supervised loss (only student on S)\n",
    "            loss = loss_ce(student_logits, y)\n",
    "            _teacher_loss = loss_ce(teacher_logits, y) # for metrics only\n",
    "            ccost = consistency_cost(softmax_fn(student_logits), softmax_fn(teacher_logits))\n",
    "            \n",
    "            # Compute the metrics\n",
    "            y_one_hot = F.one_hot(y, num_classes=args.num_classes)\n",
    "            \n",
    "            # ---- student ----\n",
    "            student_metrics = calc_student_s_metrics(student_logits, y)\n",
    "            student_acc, student_f1 = student_metrics\n",
    "            \n",
    "            # ---- teacher ----\n",
    "            teacher_metrics = calc_teacher_s_metrics(teacher_logits, y)\n",
    "            teacher_acc, teacher_f1 = teacher_metrics\n",
    "\n",
    "            # Running average of the two losses\n",
    "            student_running_loss = avg_Sce(loss.item()).mean\n",
    "            teacher_running_loss = avg_Tce(_teacher_loss.item()).mean\n",
    "            running_ccost = avg_ccost(ccost.item()).mean\n",
    "\n",
    "            # logs\n",
    "            print(val_form.format(\n",
    "                \"Validation: \", epoch + 1, int(100 * (i + 1) / len(val_loader)),\n",
    "                \"\", student_running_loss, running_ccost, *student_metrics, 0.0, 0.0,\n",
    "                \"\", teacher_running_loss, *teacher_metrics, 0.0, 0.0,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"val/student_acc\", student_acc, epoch)\n",
    "    tensorboard.add_scalar(\"val/student_f1\", student_f1, epoch)\n",
    "    tensorboard.add_scalar(\"val/teacher_acc\", teacher_acc, epoch)\n",
    "    tensorboard.add_scalar(\"val/teacher_f1\", teacher_f1, epoch)\n",
    "    tensorboard.add_scalar(\"val/student_loss\", student_running_loss, epoch)\n",
    "    tensorboard.add_scalar(\"val/teacher_loss\", teacher_running_loss, epoch)\n",
    "    tensorboard.add_scalar(\"val/consistency_cost\", running_ccost, epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"hyperparameters/learning_rate\", get_lr(optimizer), epoch)\n",
    "    tensorboard.add_scalar(\"hyperparameters/lambda_cost_max\", lambda_cost(), epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"max/student_acc\", maximum_tracker(\"student_acc\", student_acc), epoch )\n",
    "    tensorboard.add_scalar(\"max/teacher_acc\", maximum_tracker(\"teacher_acc\", teacher_acc), epoch )\n",
    "    tensorboard.add_scalar(\"max/student_f1\", maximum_tracker(\"student_f1\", student_f1), epoch )\n",
    "    tensorboard.add_scalar(\"max/teacher_f1\", maximum_tracker(\"teacher_f1\", teacher_f1), epoch )\n",
    "\n",
    "    checkpoint.step(teacher_acc)\n",
    "    for c in callbacks:\n",
    "        c.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".        Epoch  - %      - Student:   ce       ccost    acc_s    f1_s     acc_u    f1_u     | Teacher:   ce       acc_s    f1_s     acc_u    f1_u     - Time    \n",
      "\n",
      "Training 1      - 100    -            2.0665   0.0008   0.2689   0.0593   0.3142   0.1166   |            1.9520   0.3093   0.0254   0.3509   0.0669   - 84.8131 \n",
      "\u001b[1;4mValidati 1      - 100    -            2.0424   0.0008   0.3759   0.1887   0.0000   0.0000   |            1.9230   0.4295   0.0787   0.0000   0.0000   - 7.5112  \u001b[0m\n",
      "Training 2      - 100    -            1.9312   0.0008   0.3561   0.1380   0.4036   0.2191   |            1.8019   0.4116   0.0974   0.4407   0.1354   - 20.0872 \n",
      "\u001b[1;4mValidati 2      - 100    -            1.9094   0.0008   0.4757   0.2350   0.0000   0.0000   |            1.7887   0.4848   0.1289   0.0000   0.0000   - 0.5603  \u001b[0m\n",
      "Training 3      - 100    -            1.8167   0.0009   0.4293   0.2809   0.4666   0.3301   |            1.7197   0.4621   0.1728   0.5041   0.2430   - 20.1130 \n",
      "\u001b[1;4mValidati 3      - 100    -            1.8079   0.0010   0.4413   0.3602   0.0000   0.0000   |            1.7111   0.5406   0.3002   0.0000   0.0000   - 0.5570  \u001b[0m\n",
      "Training 4      - 100    -            1.7357   0.0010   0.4634   0.3433   0.4958   0.3765   |            1.6426   0.5240   0.2889   0.5435   0.3305   - 20.1663 \n",
      "\u001b[1;4mValidati 4      - 100    -            1.7336   0.0010   0.4446   0.3212   0.0000   0.0000   |            1.6357   0.5321   0.3406   0.0000   0.0000   - 0.5707  \u001b[0m\n",
      "Training 5      - 100    -            1.6648   0.0010   0.5290   0.4055   0.5447   0.4487   |            1.5813   0.5518   0.3412   0.5635   0.3713   - 20.2355 \n",
      "\u001b[1;4mValidati 5      - 100    -            1.6592   0.0010   0.5440   0.4623   0.0000   0.0000   |            1.5758   0.5931   0.4050   0.0000   0.0000   - 0.5576  \u001b[0m\n",
      "Training 6      - 100    -            1.6136   0.0011   0.5328   0.4189   0.5422   0.4778   |            1.5343   0.5909   0.3848   0.5868   0.4110   - 20.4688 \n",
      "\u001b[1;4mValidati 6      - 100    -            1.6113   0.0011   0.4717   0.3796   0.0000   0.0000   |            1.5296   0.6096   0.4389   0.0000   0.0000   - 0.5727  \u001b[0m\n",
      "Training 7      - 100    -            1.5582   0.0011   0.5606   0.4848   0.6061   0.5374   |            1.4905   0.6010   0.4154   0.6003   0.4493   - 20.5852 \n",
      "\u001b[1;4mValidati 7      - 100    -            1.5550   0.0011   0.5821   0.4713   0.0000   0.0000   |            1.4875   0.6087   0.4372   0.0000   0.0000   - 0.5793  \u001b[0m\n",
      "Training 8      - 100    -            1.5041   0.0012   0.6174   0.5566   0.6272   0.5871   |            1.4502   0.5997   0.4517   0.6160   0.4907   - 20.4462 \n",
      "\u001b[1;4mValidati 8      - 100    -            1.5031   0.0012   0.4906   0.4473   0.0000   0.0000   |            1.4481   0.5935   0.4813   0.0000   0.0000   - 0.5651  \u001b[0m\n",
      "Training 9      - 100    -            1.4588   0.0013   0.6313   0.5820   0.6372   0.6100   |            1.4118   0.6465   0.4967   0.6368   0.5383   - 20.3253 \n",
      "\u001b[1;4mValidati 9      - 100    -            1.4551   0.0013   0.6176   0.5836   0.0000   0.0000   |            1.4094   0.6100   0.5224   0.0000   0.0000   - 0.5736  \u001b[0m\n",
      "Training 10     - 100    -            1.4135   0.0013   0.6629   0.6153   0.6562   0.6278   |            1.3766   0.6604   0.5620   0.6464   0.5653   - 20.2962 \n",
      "\u001b[1;4mValidati 10     - 100    -            1.4114   0.0013   0.5353   0.5413   0.0000   0.0000   |            1.3744   0.6368   0.5482   0.0000   0.0000   - 0.5651  \u001b[0m\n",
      "Training 11     - 100    -            1.3689   0.0013   0.6944   0.6596   0.6884   0.6703   |            1.3410   0.6780   0.5678   0.6642   0.6030   - 20.6850 \n",
      "\u001b[1;4mValidati 11     - 100    -            1.3684   0.0014   0.5498   0.5067   0.0000   0.0000   |            1.3397   0.6234   0.5549   0.0000   0.0000   - 0.5850  \u001b[0m\n",
      "Training 12     - 100    -            1.3262   0.0014   0.7096   0.6932   0.6873   0.6753   |            1.3061   0.6982   0.6384   0.6773   0.6295   - 20.3871 \n",
      "\u001b[1;4mValidati 12     - 100    -            1.3260   0.0014   0.5759   0.5659   0.0000   0.0000   |            1.3046   0.6455   0.5902   0.0000   0.0000   - 0.5727  \u001b[0m\n",
      "Training 13     - 100    -            1.2887   0.0015   0.7159   0.6949   0.6999   0.6873   |            1.2731   0.7374   0.6596   0.6947   0.6598   - 20.3893 \n",
      "\u001b[1;4mValidati 13     - 100    -            1.2878   0.0015   0.6379   0.6458   0.0000   0.0000   |            1.2716   0.6391   0.6229   0.0000   0.0000   - 0.5607  \u001b[0m\n",
      "Training 14     - 100    -            1.2490   0.0015   0.7891   0.7573   0.7321   0.7213   |            1.2383   0.7639   0.7218   0.7101   0.6836   - 20.3957 \n",
      "\u001b[1;4mValidati 14     - 100    -            1.2479   0.0015   0.6676   0.6617   0.0000   0.0000   |            1.2372   0.6480   0.6241   0.0000   0.0000   - 0.5738  \u001b[0m\n",
      "Training 15     - 100    -            1.2128   0.0015   0.7689   0.7544   0.7325   0.7256   |            1.2046   0.7917   0.7443   0.7238   0.7021   - 20.4699 \n",
      "\u001b[1;4mValidati 15     - 100    -            1.2143   0.0015   0.5431   0.5350   0.0000   0.0000   |            1.2038   0.6549   0.6504   0.0000   0.0000   - 0.5593  \u001b[0m\n",
      "Training 16     - 100    -            1.1786   0.0015   0.7942   0.7879   0.7400   0.7353   |            1.1707   0.8056   0.7859   0.7339   0.7213   - 20.4192 \n",
      "\u001b[1;4mValidati 16     - 100    -            1.1792   0.0015   0.6033   0.5864   0.0000   0.0000   |            1.1698   0.6813   0.6679   0.0000   0.0000   - 0.5741  \u001b[0m\n",
      "Training 17     - 100    -            1.1463   0.0016   0.8157   0.7953   0.7495   0.7496   |            1.1379   0.8333   0.8032   0.7465   0.7359   - 20.2630 \n",
      "\u001b[1;4mValidati 17     - 100    -            1.1464   0.0016   0.6297   0.6209   0.0000   0.0000   |            1.1370   0.6824   0.6650   0.0000   0.0000   - 0.5656  \u001b[0m\n",
      "Training 18     - 100    -            1.1189   0.0016   0.7917   0.7776   0.7373   0.7388   |            1.1076   0.8220   0.8144   0.7577   0.7507   - 20.4430 \n",
      "\u001b[1;4mValidati 18     - 100    -            1.1210   0.0016   0.5196   0.5043   0.0000   0.0000   |            1.1072   0.6692   0.6783   0.0000   0.0000   - 0.5740  \u001b[0m\n",
      "Training 19     - 100    -            1.0922   0.0016   0.8409   0.8254   0.7451   0.7439   |            1.0776   0.8598   0.8479   0.7665   0.7600   - 20.3676 \n",
      "\u001b[1;4mValidati 19     - 100    -            1.0917   0.0016   0.6710   0.6826   0.0000   0.0000   |            1.0768   0.7087   0.7103   0.0000   0.0000   - 0.5731  \u001b[0m\n",
      "Training 20     - 100    -            1.0611   0.0016   0.8674   0.8676   0.7773   0.7781   |            1.0478   0.8712   0.8622   0.7755   0.7712   - 20.4365 \n",
      "\u001b[1;4mValidati 20     - 100    -            1.0615   0.0016   0.6712   0.6584   0.0000   0.0000   |            1.0473   0.6967   0.6879   0.0000   0.0000   - 0.5747  \u001b[0m\n",
      "Training 21     - 100    -            1.0318   0.0016   0.8712   0.8735   0.7610   0.7645   |            1.0186   0.8788   0.8806   0.7832   0.7808   - 20.4433 \n",
      "\u001b[1;4mValidati 21     - 100    -            1.0350   0.0016   0.5431   0.5329   0.0000   0.0000   |            1.0188   0.6658   0.6502   0.0000   0.0000   - 0.5764  \u001b[0m\n",
      "Training 22     - 100    -            1.0076   0.0016   0.8788   0.8702   0.7669   0.7697   |            0.9908   0.9040   0.8945   0.7911   0.7898   - 20.4153 \n",
      "\u001b[1;4mValidati 22     - 100    -            1.0087   0.0016   0.6286   0.6308   0.0000   0.0000   |            0.9908   0.6558   0.6729   0.0000   0.0000   - 0.6152  \u001b[0m\n",
      "Training 23     - 100    -            0.9849   0.0016   0.8674   0.8624   0.7633   0.7585   |            0.9644   0.9129   0.9029   0.7983   0.7997   - 20.3804 \n",
      "\u001b[1;4mValidati 23     - 100    -            0.9862   0.0016   0.6375   0.6371   0.0000   0.0000   |            0.9646   0.6723   0.6795   0.0000   0.0000   - 0.5851  \u001b[0m\n",
      "Training 24     - 100    -            0.9624   0.0016   0.8838   0.8786   0.7822   0.7837   |            0.9387   0.9242   0.9175   0.8022   0.8042   - 20.3563 \n",
      "\u001b[1;4mValidati 24     - 100    -            0.9641   0.0016   0.6112   0.6008   0.0000   0.0000   |            0.9392   0.6339   0.6340   0.0000   0.0000   - 0.5716  \u001b[0m\n",
      "Training 25     - 100    -            0.9405   0.0016   0.9104   0.9017   0.7806   0.7838   |            0.9148   0.9205   0.9134   0.8126   0.8093   - 20.4028 \n",
      "\u001b[1;4mValidati 25     - 100    -            0.9423   0.0016   0.6310   0.6295   0.0000   0.0000   |            0.9150   0.6911   0.6905   0.0000   0.0000   - 0.5638  \u001b[0m\n",
      "Training 26     - 100    -            0.9187   0.0016   0.9129   0.9050   0.7896   0.7950   |            0.8909   0.9394   0.9399   0.8142   0.8165   - 20.5088 \n",
      "\u001b[1;4mValidati 26     - 100    -            0.9203   0.0016   0.6754   0.6779   0.0000   0.0000   |            0.8912   0.6844   0.6963   0.0000   0.0000   - 0.5754  \u001b[0m\n",
      "Training 27     - 100    -            0.8986   0.0016   0.9053   0.9067   0.7888   0.7900   |            0.8685   0.9407   0.9446   0.8160   0.8164   - 20.4185 \n",
      "\u001b[1;4mValidati 27     - 100    -            0.9007   0.0017   0.5984   0.6109   0.0000   0.0000   |            0.8689   0.6888   0.6983   0.0000   0.0000   - 0.5687  \u001b[0m\n",
      "Training 28     - 100    -            0.8798   0.0017   0.9179   0.9134   0.7861   0.7895   |            0.8461   0.9571   0.9530   0.8214   0.8222   - 20.4834 \n",
      "\u001b[1;4mValidati 28     - 100    -            0.8813   0.0017   0.6221   0.6264   0.0000   0.0000   |            0.8470   0.6471   0.6559   0.0000   0.0000   - 0.5745  \u001b[0m\n",
      "Training 29     - 100    -            0.8622   0.0017   0.9104   0.9070   0.7716   0.7733   |            0.8255   0.9596   0.9518   0.8237   0.8238   - 20.3883 \n",
      "\u001b[1;4mValidati 29     - 100    -            0.8640   0.0017   0.5855   0.5900   0.0000   0.0000   |            0.8261   0.6967   0.6999   0.0000   0.0000   - 0.5769  \u001b[0m\n",
      "Training 30     - 100    -            0.8451   0.0017   0.9242   0.9227   0.7881   0.7894   |            0.8048   0.9722   0.9706   0.8225   0.8243   - 20.4194 \n",
      "\u001b[1;4mValidati 30     - 100    -            0.8474   0.0017   0.6433   0.6389   0.0000   0.0000   |            0.8057   0.6681   0.6718   0.0000   0.0000   - 0.5618  \u001b[0m\n",
      "Training 31     - 100    -            0.8323   0.0017   0.9003   0.8973   0.7622   0.7650   |            0.7864   0.9583   0.9584   0.8242   0.8252   - 20.3411 \n",
      "\u001b[1;4mValidati 31     - 100    -            0.8337   0.0017   0.6422   0.6373   0.0000   0.0000   |            0.7873   0.6636   0.6738   0.0000   0.0000   - 0.5755  \u001b[0m\n",
      "Training 32     - 100    -            0.8166   0.0017   0.9242   0.9276   0.7972   0.7995   |            0.7683   0.9684   0.9711   0.8278   0.8291   - 20.4969 \n",
      "\u001b[1;4mValidati 32     - 100    -            0.8184   0.0017   0.6533   0.6557   0.0000   0.0000   |            0.7691   0.6835   0.6899   0.0000   0.0000   - 0.5601  \u001b[0m\n",
      "Training 33     - 100    -            0.7992   0.0017   0.9697   0.9692   0.8124   0.8148   |            0.7506   0.9798   0.9767   0.8313   0.8312   - 20.5170 \n",
      "\u001b[1;4mValidati 33     - 100    -            0.8005   0.0017   0.6763   0.6698   0.0000   0.0000   |            0.7515   0.6801   0.6741   0.0000   0.0000   - 0.5751  \u001b[0m\n",
      "Training 34     - 100    -            0.7822   0.0017   0.9659   0.9666   0.8220   0.8236   |            0.7341   0.9735   0.9737   0.8307   0.8316   - 20.3356 \n",
      "\u001b[1;4mValidati 34     - 100    -            0.7839   0.0017   0.6679   0.6707   0.0000   0.0000   |            0.7352   0.6681   0.6639   0.0000   0.0000   - 0.5762  \u001b[0m\n",
      "Training 35     - 100    -            0.7673   0.0017   0.9558   0.9542   0.7968   0.8004   |            0.7182   0.9798   0.9783   0.8346   0.8352   - 20.4917 \n",
      "\u001b[1;4mValidati 35     - 100    -            0.7691   0.0017   0.6333   0.6351   0.0000   0.0000   |            0.7192   0.6911   0.6959   0.0000   0.0000   - 0.5939  \u001b[0m\n",
      "Training 36     - 100    -            0.7543   0.0017   0.9381   0.9413   0.7886   0.7919   |            0.7031   0.9874   0.9841   0.8346   0.8343   - 20.3764 \n",
      "\u001b[1;4mValidati 36     - 100    -            0.7565   0.0017   0.6096   0.6136   0.0000   0.0000   |            0.7041   0.6944   0.6914   0.0000   0.0000   - 0.5733  \u001b[0m\n",
      "Training 37     - 100    -            0.7403   0.0017   0.9722   0.9743   0.8014   0.8051   |            0.6881   0.9874   0.9868   0.8349   0.8364   - 20.3796 \n",
      "\u001b[1;4mValidati 37     - 100    -            0.7424   0.0017   0.6156   0.6275   0.0000   0.0000   |            0.6892   0.6701   0.6700   0.0000   0.0000   - 0.5599  \u001b[0m\n",
      "Training 38     - 100    -            0.7277   0.0017   0.9634   0.9627   0.8016   0.8049   |            0.6743   0.9899   0.9846   0.8323   0.8351   - 20.5160 \n",
      "\u001b[1;4mValidati 38     - 100    -            0.7294   0.0017   0.6435   0.6482   0.0000   0.0000   |            0.6755   0.6592   0.6644   0.0000   0.0000   - 0.5775  \u001b[0m\n",
      "Training 39     - 100    -            0.7156   0.0017   0.9634   0.9597   0.7999   0.8027   |            0.6608   0.9924   0.9921   0.8364   0.8386   - 20.4456 \n",
      "\u001b[1;4mValidati 39     - 100    -            0.7177   0.0017   0.6208   0.6203   0.0000   0.0000   |            0.6621   0.7000   0.6992   0.0000   0.0000   - 0.5672  \u001b[0m\n",
      "Training 40     - 100    -            0.7047   0.0017   0.9558   0.9512   0.8040   0.8062   |            0.6481   0.9899   0.9916   0.8347   0.8368   - 20.3615 \n",
      "\u001b[1;4mValidati 40     - 100    -            0.7070   0.0017   0.5777   0.5769   0.0000   0.0000   |            0.6495   0.6734   0.6721   0.0000   0.0000   - 0.5754  \u001b[0m\n",
      "Training 41     - 100    -            0.6933   0.0017   0.9684   0.9681   0.8078   0.8106   |            0.6357   0.9975   0.9974   0.8395   0.8415   - 20.4686 \n",
      "\u001b[1;4mValidati 41     - 100    -            0.6952   0.0017   0.6254   0.6270   0.0000   0.0000   |            0.6368   0.6911   0.6899   0.0000   0.0000   - 0.5890  \u001b[0m\n",
      "Training 42     - 100    -            0.6831   0.0017   0.9596   0.9612   0.7896   0.7891   |            0.6237   0.9924   0.9947   0.8365   0.8378   - 20.4250 \n",
      "\u001b[1;4mValidati 42     - 100    -            0.6852   0.0017   0.6308   0.6221   0.0000   0.0000   |            0.6250   0.6779   0.6757   0.0000   0.0000   - 0.5622  \u001b[0m\n",
      "Training 43     - 100    -            0.6735   0.0017   0.9634   0.9617   0.7906   0.7914   |            0.6130   0.9886   0.9890   0.8390   0.8411   - 20.4772 \n",
      "\u001b[1;4mValidati 43     - 100    -            0.6756   0.0017   0.6330   0.6414   0.0000   0.0000   |            0.6142   0.6855   0.6919   0.0000   0.0000   - 0.5749  \u001b[0m\n",
      "Training 44     - 100    -            0.6656   0.0017   0.9343   0.9355   0.7718   0.7741   |            0.6023   0.9899   0.9898   0.8341   0.8372   - 20.3905 \n",
      "\u001b[1;4mValidati 44     - 100    -            0.6680   0.0017   0.6116   0.6022   0.0000   0.0000   |            0.6039   0.6612   0.6678   0.0000   0.0000   - 0.5619  \u001b[0m\n",
      "Training 45     - 100    -            0.6577   0.0017   0.9394   0.9374   0.7974   0.8022   |            0.5921   1.0000   0.9993   0.8343   0.8378   - 20.5565 \n",
      "\u001b[1;4mValidati 45     - 100    -            0.6598   0.0017   0.6201   0.6166   0.0000   0.0000   |            0.5934   0.6679   0.6731   0.0000   0.0000   - 0.5732  \u001b[0m\n",
      "Training 46     - 100    -            0.6493   0.0017   0.9621   0.9624   0.7955   0.7982   |            0.5824   0.9949   0.9928   0.8363   0.8390   - 20.4257 \n",
      "\u001b[1;4mValidati 46     - 100    -            0.6516   0.0017   0.6357   0.6430   0.0000   0.0000   |            0.5838   0.6712   0.6731   0.0000   0.0000   - 0.5695  \u001b[0m\n",
      "Training 47     - 100    -            0.6403   0.0017   0.9836   0.9812   0.8102   0.8126   |            0.5730   0.9949   0.9955   0.8339   0.8366   - 20.4960 \n",
      "\u001b[1;4mValidati 47     - 100    -            0.6425   0.0017   0.6442   0.6425   0.0000   0.0000   |            0.5744   0.6810   0.6828   0.0000   0.0000   - 0.5801  \u001b[0m\n",
      "Training 48     - 100    -            0.6313   0.0017   0.9886   0.9870   0.8164   0.8192   |            0.5639   0.9987   0.9987   0.8335   0.8374   - 20.3722 \n",
      "\u001b[1;4mValidati 48     - 100    -            0.6330   0.0017   0.6585   0.6662   0.0000   0.0000   |            0.5652   0.6897   0.6889   0.0000   0.0000   - 0.6188  \u001b[0m\n",
      "Training 49     - 100    -            0.6226   0.0017   0.9811   0.9777   0.8066   0.8102   |            0.5551   0.9962   0.9974   0.8356   0.8386   - 20.3535 \n",
      "\u001b[1;4mValidati 49     - 100    -            0.6250   0.0017   0.6188   0.6126   0.0000   0.0000   |            0.5566   0.6567   0.6626   0.0000   0.0000   - 0.5635  \u001b[0m\n",
      "Training 50     - 100    -            0.6145   0.0017   0.9899   0.9891   0.8198   0.8223   |            0.5468   0.9987   0.9987   0.8362   0.8389   - 20.5304 \n",
      "\u001b[1;4mValidati 50     - 100    -            0.6163   0.0017   0.6290   0.6313   0.0000   0.0000   |            0.5485   0.6569   0.6629   0.0000   0.0000   - 0.5738  \u001b[0m\n",
      "Training 51     - 100    -            0.6070   0.0017   0.9722   0.9705   0.8009   0.8032   |            0.5390   1.0000   1.0000   0.8363   0.8412   - 20.4815 \n",
      "\u001b[1;4mValidati 51     - 100    -            0.6090   0.0017   0.6107   0.6119   0.0000   0.0000   |            0.5404   0.6987   0.7031   0.0000   0.0000   - 0.5632  \u001b[0m\n",
      "Training 52     - 100    -            0.5999   0.0017   0.9735   0.9728   0.7999   0.8032   |            0.5313   0.9962   0.9962   0.8341   0.8347   - 20.5404 \n",
      "\u001b[1;4mValidati 52     - 100    -            0.6018   0.0017   0.6355   0.6390   0.0000   0.0000   |            0.5329   0.6612   0.6686   0.0000   0.0000   - 0.5968  \u001b[0m\n",
      "Training 53     - 100    -            0.5932   0.0017   0.9710   0.9712   0.8103   0.8142   |            0.5240   0.9975   0.9975   0.8329   0.8349   - 20.3814 \n",
      "\u001b[1;4mValidati 53     - 100    -            0.5956   0.0017   0.6475   0.6432   0.0000   0.0000   |            0.5255   0.6810   0.6838   0.0000   0.0000   - 0.5765  \u001b[0m\n",
      "Training 54     - 100    -            0.5880   0.0017   0.9558   0.9508   0.8094   0.8109   |            0.5170   0.9987   0.9987   0.8392   0.8401   - 20.4709 \n",
      "\u001b[1;4mValidati 54     - 100    -            0.5894   0.0017   0.7143   0.7098   0.0000   0.0000   |            0.5185   0.6525   0.6648   0.0000   0.0000   - 0.5623  \u001b[0m\n",
      "Training 55     - 100    -            0.5811   0.0017   0.9773   0.9747   0.8161   0.8195   |            0.5102   0.9987   0.9987   0.8402   0.8436   - 20.4180 \n",
      "\u001b[1;4mValidati 55     - 100    -            0.5826   0.0017   0.6531   0.6557   0.0000   0.0000   |            0.5115   0.7031   0.7058   0.0000   0.0000   - 0.5771  \u001b[0m\n",
      "Training 56     - 100    -            0.5741   0.0017   0.9785   0.9816   0.8007   0.8035   |            0.5035   0.9975   0.9960   0.8414   0.8450   - 20.4544 \n",
      "\u001b[1;4mValidati 56     - 100    -            0.5762   0.0017   0.6109   0.6166   0.0000   0.0000   |            0.5050   0.6835   0.6941   0.0000   0.0000   - 0.5646  \u001b[0m\n",
      "Training 57     - 100    -            0.5682   0.0017   0.9773   0.9750   0.7919   0.7975   |            0.4972   0.9987   0.9980   0.8408   0.8427   - 20.6295 \n",
      "\u001b[1;4mValidati 57     - 100    -            0.5705   0.0017   0.6411   0.6402   0.0000   0.0000   |            0.4988   0.6527   0.6546   0.0000   0.0000   - 0.5764  \u001b[0m\n",
      "Training 58     - 100    -            0.5631   0.0017   0.9672   0.9674   0.7889   0.7921   |            0.4912   0.9975   0.9980   0.8422   0.8446   - 20.4077 \n",
      "\u001b[1;4mValidati 58     - 100    -            0.5650   0.0017   0.5786   0.5804   0.0000   0.0000   |            0.4927   0.6757   0.6799   0.0000   0.0000   - 0.5728  \u001b[0m\n",
      "Training 59     - 100    -            0.5576   0.0017   0.9747   0.9754   0.8032   0.8054   |            0.4853   0.9987   0.9993   0.8381   0.8412   - 20.5096 \n",
      "\u001b[1;4mValidati 59     - 100    -            0.5596   0.0017   0.6056   0.6040   0.0000   0.0000   |            0.4868   0.6513   0.6576   0.0000   0.0000   - 0.5750  \u001b[0m\n",
      "Training 60     - 100    -            0.5519   0.0017   0.9861   0.9855   0.8141   0.8169   |            0.4797   0.9962   0.9968   0.8406   0.8426   - 20.4438 \n",
      "\u001b[1;4mValidati 60     - 100    -            0.5534   0.0017   0.6656   0.6640   0.0000   0.0000   |            0.4812   0.6600   0.6668   0.0000   0.0000   - 0.5780  \u001b[0m\n",
      "Training 61     - 100    -            0.5458   0.0017   0.9886   0.9883   0.8178   0.8209   |            0.4743   0.9975   0.9975   0.8379   0.8384   - 20.4642 \n",
      "\u001b[1;4mValidati 61     - 100    -            0.5473   0.0017   0.6978   0.6821   0.0000   0.0000   |            0.4756   0.6844   0.6767   0.0000   0.0000   - 0.5688  \u001b[0m\n",
      "Training 62     - 100    -            0.5408   0.0017   0.9646   0.9671   0.7999   0.8014   |            0.4688   0.9987   0.9980   0.8382   0.8414   - 20.4805 \n",
      "\u001b[1;4mValidati 62     - 100    -            0.5430   0.0017   0.6143   0.6190   0.0000   0.0000   |            0.4703   0.6790   0.6694   0.0000   0.0000   - 0.5793  \u001b[0m\n",
      "Training 63     - 100    -            0.5360   0.0017   0.9836   0.9828   0.8118   0.8162   |            0.4637   0.9987   0.9980   0.8423   0.8445   - 20.3858 \n",
      "\u001b[1;4mValidati 63     - 100    -            0.5374   0.0017   0.6813   0.6712   0.0000   0.0000   |            0.4652   0.6681   0.6758   0.0000   0.0000   - 0.5621  \u001b[0m\n",
      "Training 64     - 100    -            0.5302   0.0017   0.9937   0.9939   0.8230   0.8238   |            0.4587   1.0000   0.9993   0.8392   0.8410   - 20.5600 \n",
      "\u001b[1;4mValidati 64     - 100    -            0.5317   0.0017   0.6554   0.6452   0.0000   0.0000   |            0.4599   0.6933   0.6968   0.0000   0.0000   - 0.5804  \u001b[0m\n",
      "Training 65     - 100    -            0.5246   0.0016   0.9924   0.9929   0.8155   0.8184   |            0.4537   0.9975   0.9986   0.8389   0.8414   - 20.4412 \n",
      "\u001b[1;4mValidati 65     - 100    -            0.5262   0.0016   0.6612   0.6616   0.0000   0.0000   |            0.4551   0.6958   0.6963   0.0000   0.0000   - 0.5771  \u001b[0m\n",
      "Training 66     - 100    -            0.5192   0.0016   0.9975   0.9972   0.8205   0.8245   |            0.4490   1.0000   0.9993   0.8388   0.8420   - 20.5490 \n",
      "\u001b[1;4mValidati 66     - 100    -            0.5207   0.0016   0.6170   0.6301   0.0000   0.0000   |            0.4503   0.6781   0.6771   0.0000   0.0000   - 0.5644  \u001b[0m\n",
      "Training 67     - 100    -            0.5144   0.0016   0.9811   0.9803   0.8036   0.8061   |            0.4444   0.9975   0.9975   0.8371   0.8400   - 20.5044 \n",
      "\u001b[1;4mValidati 67     - 100    -            0.5162   0.0016   0.6297   0.6289   0.0000   0.0000   |            0.4457   0.6989   0.6974   0.0000   0.0000   - 0.5895  \u001b[0m\n",
      "Training 68     - 100    -            0.5101   0.0016   0.9798   0.9820   0.8141   0.8156   |            0.4398   0.9987   0.9979   0.8449   0.8456   - 20.4650 \n",
      "\u001b[1;4mValidati 68     - 100    -            0.5120   0.0016   0.6129   0.6033   0.0000   0.0000   |            0.4411   0.6933   0.7010   0.0000   0.0000   - 0.5631  \u001b[0m\n",
      "Training 69     - 100    -            0.5060   0.0016   0.9785   0.9801   0.8139   0.8178   |            0.4355   0.9975   0.9980   0.8442   0.8470   - 20.5735 \n",
      "\u001b[1;4mValidati 69     - 100    -            0.5079   0.0016   0.6210   0.6263   0.0000   0.0000   |            0.4368   0.6944   0.6954   0.0000   0.0000   - 0.5832  \u001b[0m\n",
      "Training 70     - 100    -            0.5025   0.0017   0.9697   0.9692   0.7990   0.8008   |            0.4312   1.0000   1.0000   0.8451   0.8482   - 20.4107 \n",
      "\u001b[1;4mValidati 70     - 100    -            0.5041   0.0017   0.6310   0.6417   0.0000   0.0000   |            0.4326   0.6723   0.6741   0.0000   0.0000   - 0.5629  \u001b[0m\n",
      "Training 71     - 100    -            0.4987   0.0017   0.9747   0.9747   0.8022   0.8041   |            0.4272   0.9987   0.9987   0.8424   0.8450   - 20.4984 \n",
      "\u001b[1;4mValidati 71     - 100    -            0.5004   0.0017   0.6408   0.6454   0.0000   0.0000   |            0.4286   0.6922   0.7010   0.0000   0.0000   - 0.5912  \u001b[0m\n",
      "Training 72     - 100    -            0.4949   0.0017   0.9811   0.9803   0.8006   0.8019   |            0.4233   1.0000   1.0000   0.8433   0.8464   - 20.5811 \n",
      "\u001b[1;4mValidati 72     - 100    -            0.4966   0.0017   0.6156   0.6223   0.0000   0.0000   |            0.4245   0.6801   0.6928   0.0000   0.0000   - 0.5911  \u001b[0m\n",
      "Training 73     - 100    -            0.4916   0.0017   0.9684   0.9677   0.8070   0.8071   |            0.4193   1.0000   0.9993   0.8451   0.8473   - 20.4906 \n",
      "\u001b[1;4mValidati 73     - 100    -            0.4934   0.0017   0.6442   0.6469   0.0000   0.0000   |            0.4206   0.6634   0.6733   0.0000   0.0000   - 0.5688  \u001b[0m\n",
      "Training 74     - 100    -            0.4879   0.0017   0.9899   0.9898   0.8136   0.8163   |            0.4155   1.0000   1.0000   0.8448   0.8488   - 20.5064 \n",
      "\u001b[1;4mValidati 74     - 100    -            0.4893   0.0017   0.6545   0.6599   0.0000   0.0000   |            0.4168   0.6723   0.6758   0.0000   0.0000   - 0.5789  \u001b[0m\n",
      "Training 75     - 100    -            0.4841   0.0017   0.9836   0.9839   0.8226   0.8241   |            0.4119   0.9975   0.9980   0.8436   0.8465   - 20.4533 \n",
      "\u001b[1;4mValidati 75     - 100    -            0.4857   0.0017   0.6145   0.6197   0.0000   0.0000   |            0.4131   0.6788   0.6737   0.0000   0.0000   - 0.5636  \u001b[0m\n",
      "Training 76     - 100    -            0.4804   0.0017   0.9886   0.9879   0.8151   0.8181   |            0.4082   1.0000   1.0000   0.8453   0.8477   - 20.5349 \n",
      "\u001b[1;4mValidati 76     - 100    -            0.4820   0.0017   0.6422   0.6432   0.0000   0.0000   |            0.4096   0.6723   0.6765   0.0000   0.0000   - 0.5771  \u001b[0m\n",
      "Training 77     - 100    -            0.4765   0.0017   0.9987   0.9987   0.8270   0.8292   |            0.4048   1.0000   1.0000   0.8443   0.8474   - 20.5846 \n",
      "\u001b[1;4mValidati 77     - 100    -            0.4782   0.0017   0.6431   0.6408   0.0000   0.0000   |            0.4061   0.6877   0.6896   0.0000   0.0000   - 0.5765  \u001b[0m\n",
      "Training 78     - 100    -            0.4728   0.0016   0.9987   0.9974   0.8234   0.8255   |            0.4014   1.0000   1.0000   0.8441   0.8473   - 20.5320 \n",
      "\u001b[1;4mValidati 78     - 100    -            0.4744   0.0016   0.6578   0.6594   0.0000   0.0000   |            0.4028   0.6754   0.6836   0.0000   0.0000   - 0.5696  \u001b[0m\n",
      "Training 79     - 100    -            0.4691   0.0016   0.9962   0.9968   0.8244   0.8264   |            0.3982   1.0000   0.9993   0.8456   0.8478   - 20.4462 \n",
      "\u001b[1;4mValidati 79     - 100    -            0.4706   0.0016   0.6752   0.6778   0.0000   0.0000   |            0.3995   0.6942   0.6990   0.0000   0.0000   - 0.5782  \u001b[0m\n",
      "Training 80     - 100    -            0.4655   0.0016   0.9962   0.9955   0.8236   0.8261   |            0.3950   0.9987   0.9993   0.8432   0.8458   - 20.4506 \n",
      "\u001b[1;4mValidati 80     - 100    -            0.4672   0.0016   0.6219   0.6215   0.0000   0.0000   |            0.3964   0.6788   0.6828   0.0000   0.0000   - 0.5628  \u001b[0m\n",
      "Training 81     - 100    -            0.4625   0.0016   0.9848   0.9854   0.8026   0.8060   |            0.3919   1.0000   1.0000   0.8435   0.8462   - 20.5344 \n",
      "\u001b[1;4mValidati 81     - 100    -            0.4644   0.0016   0.6042   0.6127   0.0000   0.0000   |            0.3933   0.6578   0.6623   0.0000   0.0000   - 0.5789  \u001b[0m\n",
      "Training 82     - 100    -            0.4597   0.0016   0.9861   0.9840   0.8123   0.8157   |            0.3890   1.0000   1.0000   0.8413   0.8443   - 20.6662 \n",
      "\u001b[1;4mValidati 82     - 100    -            0.4613   0.0016   0.6777   0.6806   0.0000   0.0000   |            0.3903   0.6922   0.7028   0.0000   0.0000   - 0.5789  \u001b[0m\n",
      "Training 83     - 100    -            0.4565   0.0016   0.9899   0.9898   0.8142   0.8183   |            0.3861   1.0000   1.0000   0.8437   0.8463   - 20.5319 \n",
      "\u001b[1;4mValidati 83     - 100    -            0.4580   0.0016   0.6489   0.6565   0.0000   0.0000   |            0.3874   0.6712   0.6763   0.0000   0.0000   - 0.5774  \u001b[0m\n",
      "Training 84     - 100    -            0.4532   0.0016   0.9949   0.9948   0.8302   0.8310   |            0.3832   1.0000   1.0000   0.8437   0.8447   - 20.4339 \n",
      "\u001b[1;4mValidati 84     - 100    -            0.4547   0.0016   0.6558   0.6574   0.0000   0.0000   |            0.3846   0.6725   0.6753   0.0000   0.0000   - 0.6126  \u001b[0m\n",
      "Training 85     - 100    -            0.4500   0.0016   0.9987   0.9980   0.8250   0.8271   |            0.3805   1.0000   1.0000   0.8422   0.8439   - 20.5025 \n",
      "\u001b[1;4mValidati 85     - 100    -            0.4515   0.0016   0.6935   0.6942   0.0000   0.0000   |            0.3819   0.6779   0.6804   0.0000   0.0000   - 0.5653  \u001b[0m\n",
      "Training 86     - 100    -            0.4468   0.0016   1.0000   1.0000   0.8303   0.8322   |            0.3779   1.0000   1.0000   0.8444   0.8471   - 20.6013 \n",
      "\u001b[1;4mValidati 86     - 100    -            0.4479   0.0016   0.7201   0.7215   0.0000   0.0000   |            0.3791   0.7045   0.7086   0.0000   0.0000   - 0.6249  \u001b[0m\n",
      "Training 87     - 100    -            0.4434   0.0016   0.9962   0.9971   0.8363   0.8383   |            0.3753   0.9987   0.9980   0.8478   0.8499   - 20.4473 \n",
      "\u001b[1;4mValidati 87     - 100    -            0.4444   0.0016   0.6960   0.7039   0.0000   0.0000   |            0.3766   0.7002   0.6897   0.0000   0.0000   - 0.5676  \u001b[0m\n",
      "Training 88     - 100    -            0.4399   0.0016   1.0000   1.0000   0.8291   0.8323   |            0.3727   1.0000   1.0000   0.8440   0.8457   - 20.5547 \n",
      "\u001b[1;4mValidati 88     - 100    -            0.4411   0.0016   0.7031   0.7061   0.0000   0.0000   |            0.3741   0.7069   0.7036   0.0000   0.0000   - 0.5775  \u001b[0m\n",
      "Training 89     - 100    -            0.4366   0.0016   1.0000   0.9993   0.8422   0.8445   |            0.3703   1.0000   1.0000   0.8480   0.8502   - 20.5938 \n",
      "\u001b[1;4mValidati 89     - 100    -            0.4377   0.0016   0.7234   0.7254   0.0000   0.0000   |            0.3717   0.7047   0.7055   0.0000   0.0000   - 0.5847  \u001b[0m\n",
      "Training 90     - 100    -            0.4333   0.0016   1.0000   1.0000   0.8444   0.8474   |            0.3679   1.0000   1.0000   0.8451   0.8476   - 20.4154 \n",
      "\u001b[1;4mValidati 90     - 100    -            0.4346   0.0016   0.7092   0.7115   0.0000   0.0000   |            0.3692   0.6960   0.6972   0.0000   0.0000   - 0.5643  \u001b[0m\n",
      "Training 91     - 100    -            0.4304   0.0015   0.9987   0.9987   0.8437   0.8447   |            0.3655   1.0000   1.0000   0.8483   0.8505   - 20.6052 \n",
      "\u001b[1;4mValidati 91     - 100    -            0.4314   0.0015   0.7266   0.7295   0.0000   0.0000   |            0.3666   0.7310   0.7343   0.0000   0.0000   - 0.5846  \u001b[0m\n",
      "Training 92     - 100    -            0.4272   0.0015   1.0000   1.0000   0.8359   0.8392   |            0.3630   1.0000   1.0000   0.8483   0.8500   - 20.4468 \n",
      "\u001b[1;4mValidati 92     - 100    -            0.4283   0.0015   0.7036   0.7074   0.0000   0.0000   |            0.3641   0.7279   0.7187   0.0000   0.0000   - 0.6065  \u001b[0m\n",
      "Training 93     - 100    -            0.4241   0.0015   1.0000   1.0000   0.8403   0.8422   |            0.3606   1.0000   1.0000   0.8470   0.8487   - 20.7459 \n",
      "\u001b[1;4mValidati 93     - 100    -            0.4252   0.0015   0.6946   0.6982   0.0000   0.0000   |            0.3616   0.7344   0.7363   0.0000   0.0000   - 0.5786  \u001b[0m\n",
      "Training 94     - 100    -            0.4211   0.0015   1.0000   1.0000   0.8352   0.8366   |            0.3582   1.0000   1.0000   0.8463   0.8465   - 20.5458 \n",
      "\u001b[1;4mValidati 94     - 100    -            0.4223   0.0015   0.6779   0.6845   0.0000   0.0000   |            0.3594   0.7058   0.7007   0.0000   0.0000   - 0.5831  \u001b[0m\n",
      "Training 95     - 100    -            0.4183   0.0015   1.0000   1.0000   0.8451   0.8468   |            0.3560   1.0000   1.0000   0.8473   0.8490   - 20.5119 \n",
      "\u001b[1;4mValidati 95     - 100    -            0.4195   0.0015   0.7000   0.7026   0.0000   0.0000   |            0.3571   0.6991   0.7046   0.0000   0.0000   - 0.5661  \u001b[0m\n",
      "Training 96     - 100    -            0.4156   0.0015   0.9962   0.9962   0.8296   0.8316   |            0.3538   0.9987   0.9987   0.8453   0.8479   - 20.6145 \n",
      "\u001b[1;4mValidati 96     - 100    -            0.4174   0.0015   0.6143   0.6149   0.0000   0.0000   |            0.3549   0.7201   0.7239   0.0000   0.0000   - 0.6018  \u001b[0m\n",
      "Training 97     - 100    -            0.4144   0.0015   0.9710   0.9736   0.7875   0.7903   |            0.3516   1.0000   1.0000   0.8447   0.8482   - 20.3877 \n",
      "\u001b[1;4mValidati 97     - 100    -            0.4162   0.0015   0.5924   0.5951   0.0000   0.0000   |            0.3528   0.7147   0.7105   0.0000   0.0000   - 0.5803  \u001b[0m\n",
      "Training 98     - 100    -            0.4130   0.0015   0.9823   0.9799   0.7960   0.7976   |            0.3496   0.9987   0.9993   0.8470   0.8491   - 20.5080 \n",
      "\u001b[1;4mValidati 98     - 100    -            0.4144   0.0015   0.6567   0.6576   0.0000   0.0000   |            0.3508   0.7058   0.7114   0.0000   0.0000   - 0.5707  \u001b[0m\n",
      "Training 99     - 100    -            0.4110   0.0015   0.9874   0.9874   0.8054   0.8074   |            0.3476   1.0000   1.0000   0.8478   0.8490   - 20.6555 \n",
      "\u001b[1;4mValidati 99     - 100    -            0.4125   0.0015   0.6199   0.6187   0.0000   0.0000   |            0.3486   0.6868   0.6866   0.0000   0.0000   - 0.5778  \u001b[0m\n",
      "Training 100    - 100    -            0.4091   0.0015   0.9924   0.9924   0.8110   0.8133   |            0.3455   1.0000   1.0000   0.8443   0.8465   - 20.5713 \n",
      "\u001b[1;4mValidati 100    - 100    -            0.4108   0.0015   0.5743   0.5751   0.0000   0.0000   |            0.3467   0.6813   0.6849   0.0000   0.0000   - 0.5842  \u001b[0m\n",
      "Training 101    - 100    -            0.4075   0.0015   0.9899   0.9905   0.8097   0.8117   |            0.3436   1.0000   1.0000   0.8451   0.8479   - 20.4408 \n",
      "\u001b[1;4mValidati 101    - 100    -            0.4088   0.0015   0.6531   0.6549   0.0000   0.0000   |            0.3446   0.7054   0.7062   0.0000   0.0000   - 0.5800  \u001b[0m\n",
      "Training 102    - 100    -            0.4055   0.0015   0.9886   0.9890   0.8186   0.8185   |            0.3416   1.0000   1.0000   0.8463   0.8474   - 20.5169 \n",
      "\u001b[1;4mValidati 102    - 100    -            0.4072   0.0015   0.5757   0.5763   0.0000   0.0000   |            0.3428   0.6866   0.6742   0.0000   0.0000   - 0.5797  \u001b[0m\n",
      "Training 103    - 100    -            0.4037   0.0015   0.9937   0.9953   0.8219   0.8232   |            0.3398   1.0000   1.0000   0.8427   0.8446   - 20.4911 \n",
      "\u001b[1;4mValidati 103    - 100    -            0.4050   0.0015   0.6344   0.6454   0.0000   0.0000   |            0.3410   0.6536   0.6540   0.0000   0.0000   - 0.5666  \u001b[0m\n",
      "Training 104    - 100    -            0.4016   0.0015   0.9962   0.9962   0.8184   0.8203   |            0.3381   1.0000   1.0000   0.8417   0.8440   - 20.4885 \n",
      "\u001b[1;4mValidati 104    - 100    -            0.4030   0.0015   0.6598   0.6618   0.0000   0.0000   |            0.3391   0.6942   0.6935   0.0000   0.0000   - 0.5927  \u001b[0m\n",
      "Training 105    - 100    -            0.3998   0.0015   0.9937   0.9930   0.8194   0.8203   |            0.3362   1.0000   1.0000   0.8410   0.8438   - 20.3784 \n",
      "\u001b[1;4mValidati 105    - 100    -            0.4012   0.0015   0.6587   0.6597   0.0000   0.0000   |            0.3374   0.6721   0.6754   0.0000   0.0000   - 0.5710  \u001b[0m\n",
      "Training 106    - 100    -            0.3979   0.0015   0.9962   0.9962   0.8215   0.8236   |            0.3345   1.0000   1.0000   0.8416   0.8433   - 20.5425 \n",
      "\u001b[1;4mValidati 106    - 100    -            0.3992   0.0015   0.6489   0.6497   0.0000   0.0000   |            0.3356   0.6679   0.6710   0.0000   0.0000   - 0.5774  \u001b[0m\n",
      "Training 107    - 100    -            0.3959   0.0015   0.9987   0.9987   0.8295   0.8311   |            0.3328   0.9987   0.9993   0.8387   0.8411   - 20.5427 \n",
      "\u001b[1;4mValidati 107    - 100    -            0.3974   0.0015   0.6520   0.6559   0.0000   0.0000   |            0.3340   0.6688   0.6724   0.0000   0.0000   - 0.5637  \u001b[0m\n",
      "Training 108    - 100    -            0.3941   0.0015   0.9987   0.9993   0.8321   0.8337   |            0.3312   1.0000   1.0000   0.8404   0.8433   - 20.5395 \n",
      "\u001b[1;4mValidati 108    - 100    -            0.3955   0.0015   0.6464   0.6529   0.0000   0.0000   |            0.3324   0.6612   0.6597   0.0000   0.0000   - 0.5813  \u001b[0m\n",
      "Training 109    - 100    -            0.3923   0.0015   0.9975   0.9985   0.8309   0.8340   |            0.3297   0.9987   0.9993   0.8417   0.8445   - 20.4820 \n",
      "\u001b[1;4mValidati 109    - 100    -            0.3937   0.0015   0.6375   0.6460   0.0000   0.0000   |            0.3309   0.6754   0.6779   0.0000   0.0000   - 0.5720  \u001b[0m\n",
      "Training 110    - 100    -            0.3905   0.0015   1.0000   1.0000   0.8338   0.8372   |            0.3281   1.0000   1.0000   0.8391   0.8424   - 20.4820 \n",
      "\u001b[1;4mValidati 110    - 100    -            0.3918   0.0015   0.6632   0.6646   0.0000   0.0000   |            0.3294   0.6688   0.6707   0.0000   0.0000   - 0.5783  \u001b[0m\n",
      "Training 111    - 100    -            0.3886   0.0015   1.0000   1.0000   0.8390   0.8413   |            0.3267   1.0000   1.0000   0.8409   0.8431   - 20.5096 \n",
      "\u001b[1;4mValidati 111    - 100    -            0.3898   0.0015   0.6897   0.6899   0.0000   0.0000   |            0.3279   0.6897   0.6920   0.0000   0.0000   - 0.5964  \u001b[0m\n",
      "Training 112    - 100    -            0.3866   0.0015   0.9987   0.9987   0.8334   0.8353   |            0.3252   0.9987   0.9987   0.8372   0.8397   - 20.4573 \n",
      "\u001b[1;4mValidati 112    - 100    -            0.3879   0.0015   0.6674   0.6666   0.0000   0.0000   |            0.3264   0.6931   0.6948   0.0000   0.0000   - 0.5697  \u001b[0m\n",
      "Training 113    - 100    -            0.3848   0.0015   1.0000   1.0000   0.8332   0.8353   |            0.3238   1.0000   1.0000   0.8410   0.8438   - 20.5481 \n",
      "\u001b[1;4mValidati 113    - 100    -            0.3861   0.0015   0.6467   0.6558   0.0000   0.0000   |            0.3250   0.6547   0.6607   0.0000   0.0000   - 0.5790  \u001b[0m\n",
      "Training 114    - 100    -            0.3830   0.0015   0.9987   0.9987   0.8314   0.8337   |            0.3225   1.0000   1.0000   0.8417   0.8438   - 20.4762 \n",
      "\u001b[1;4mValidati 114    - 100    -            0.3842   0.0015   0.6201   0.6241   0.0000   0.0000   |            0.3236   0.6502   0.6581   0.0000   0.0000   - 0.5699  \u001b[0m\n",
      "Training 115    - 100    -            0.3812   0.0014   0.9987   0.9987   0.8395   0.8422   |            0.3211   1.0000   1.0000   0.8448   0.8470   - 20.4843 \n",
      "\u001b[1;4mValidati 115    - 100    -            0.3824   0.0014   0.6402   0.6423   0.0000   0.0000   |            0.3223   0.6645   0.6632   0.0000   0.0000   - 0.5803  \u001b[0m\n",
      "Training 116    - 100    -            0.3796   0.0014   0.9962   0.9962   0.8313   0.8344   |            0.3198   1.0000   1.0000   0.8401   0.8441   - 20.3415 \n",
      "\u001b[1;4mValidati 116    - 100    -            0.3809   0.0014   0.6377   0.6378   0.0000   0.0000   |            0.3209   0.6623   0.6689   0.0000   0.0000   - 0.5655  \u001b[0m\n",
      "Training 117    - 100    -            0.3780   0.0014   1.0000   1.0000   0.8324   0.8336   |            0.3185   1.0000   1.0000   0.8393   0.8420   - 20.8903 \n",
      "\u001b[1;4mValidati 117    - 100    -            0.3795   0.0014   0.6556   0.6592   0.0000   0.0000   |            0.3197   0.6589   0.6638   0.0000   0.0000   - 0.5822  \u001b[0m\n",
      "Training 118    - 100    -            0.3766   0.0014   0.9962   0.9962   0.8323   0.8340   |            0.3173   1.0000   1.0000   0.8451   0.8476   - 20.6256 \n",
      "\u001b[1;4mValidati 118    - 100    -            0.3780   0.0014   0.6199   0.6232   0.0000   0.0000   |            0.3185   0.6600   0.6603   0.0000   0.0000   - 0.5948  \u001b[0m\n",
      "Training 119    - 100    -            0.3752   0.0014   0.9975   0.9975   0.8251   0.8274   |            0.3161   1.0000   1.0000   0.8437   0.8455   - 20.7449 \n",
      "\u001b[1;4mValidati 119    - 100    -            0.3763   0.0014   0.6824   0.6922   0.0000   0.0000   |            0.3173   0.6545   0.6466   0.0000   0.0000   - 0.5747  \u001b[0m\n",
      "Training 120    - 100    -            0.3736   0.0014   0.9949   0.9955   0.8244   0.8276   |            0.3149   1.0000   0.9993   0.8407   0.8430   - 20.5832 \n",
      "\u001b[1;4mValidati 120    - 100    -            0.3749   0.0014   0.6478   0.6466   0.0000   0.0000   |            0.3160   0.6645   0.6729   0.0000   0.0000   - 0.5872  \u001b[0m\n",
      "Training 121    - 100    -            0.3724   0.0014   0.9937   0.9943   0.8211   0.8242   |            0.3137   1.0000   1.0000   0.8406   0.8438   - 20.7387 \n",
      "\u001b[1;4mValidati 121    - 100    -            0.3737   0.0014   0.6257   0.6278   0.0000   0.0000   |            0.3148   0.6513   0.6531   0.0000   0.0000   - 0.5827  \u001b[0m\n",
      "Training 122    - 100    -            0.3713   0.0014   0.9912   0.9889   0.8152   0.8183   |            0.3125   0.9987   0.9987   0.8450   0.8468   - 20.5518 \n",
      "\u001b[1;4mValidati 122    - 100    -            0.3729   0.0014   0.5574   0.5602   0.0000   0.0000   |            0.3137   0.6600   0.6519   0.0000   0.0000   - 0.5774  \u001b[0m\n",
      "Training 123    - 100    -            0.3702   0.0014   0.9975   0.9975   0.8215   0.8237   |            0.3114   1.0000   1.0000   0.8430   0.8453   - 20.4594 \n",
      "\u001b[1;4mValidati 123    - 100    -            0.3716   0.0014   0.6355   0.6384   0.0000   0.0000   |            0.3126   0.6645   0.6647   0.0000   0.0000   - 0.5726  \u001b[0m\n",
      "Training 124    - 100    -            0.3689   0.0014   1.0000   1.0000   0.8250   0.8275   |            0.3103   1.0000   1.0000   0.8410   0.8425   - 20.6138 \n",
      "\u001b[1;4mValidati 124    - 100    -            0.3702   0.0014   0.6266   0.6323   0.0000   0.0000   |            0.3115   0.6600   0.6621   0.0000   0.0000   - 0.5913  \u001b[0m\n",
      "Training 125    - 100    -            0.3676   0.0014   0.9962   0.9966   0.8269   0.8289   |            0.3093   0.9975   0.9975   0.8404   0.8426   - 20.6447 \n",
      "\u001b[1;4mValidati 125    - 100    -            0.3689   0.0014   0.6496   0.6486   0.0000   0.0000   |            0.3104   0.6797   0.6807   0.0000   0.0000   - 0.5758  \u001b[0m\n",
      "Training 126    - 100    -            0.3663   0.0014   1.0000   1.0000   0.8285   0.8311   |            0.3082   0.9987   0.9987   0.8378   0.8396   - 20.7302 \n",
      "\u001b[1;4mValidati 126    - 100    -            0.3675   0.0014   0.6433   0.6476   0.0000   0.0000   |            0.3094   0.6667   0.6653   0.0000   0.0000   - 0.5860  \u001b[0m\n",
      "Training 127    - 100    -            0.3649   0.0014   1.0000   1.0000   0.8346   0.8368   |            0.3072   1.0000   1.0000   0.8394   0.8408   - 20.6697 \n",
      "\u001b[1;4mValidati 127    - 100    -            0.3661   0.0014   0.6368   0.6442   0.0000   0.0000   |            0.3084   0.6656   0.6676   0.0000   0.0000   - 0.6230  \u001b[0m\n",
      "Training 128    - 100    -            0.3637   0.0014   0.9912   0.9921   0.8192   0.8207   |            0.3062   0.9975   0.9975   0.8381   0.8404   - 20.6703 \n",
      "\u001b[1;4mValidati 128    - 100    -            0.3652   0.0014   0.6074   0.6053   0.0000   0.0000   |            0.3074   0.6533   0.6567   0.0000   0.0000   - 0.5723  \u001b[0m\n",
      "Training 129    - 100    -            0.3627   0.0014   1.0000   0.9993   0.8208   0.8233   |            0.3052   1.0000   1.0000   0.8398   0.8418   - 20.7133 \n",
      "\u001b[1;4mValidati 129    - 100    -            0.3641   0.0014   0.6121   0.6136   0.0000   0.0000   |            0.3064   0.6754   0.6692   0.0000   0.0000   - 0.5950  \u001b[0m\n",
      "Training 130    - 100    -            0.3616   0.0014   0.9987   0.9987   0.8192   0.8217   |            0.3042   0.9987   0.9987   0.8389   0.8415   - 20.6773 \n",
      "\u001b[1;4mValidati 130    - 100    -            0.3627   0.0014   0.6366   0.6362   0.0000   0.0000   |            0.3054   0.6545   0.6592   0.0000   0.0000   - 0.5715  \u001b[0m\n",
      "Training 131    - 100    -            0.3602   0.0014   1.0000   1.0000   0.8293   0.8309   |            0.3033   1.0000   1.0000   0.8377   0.8389   - 20.7324 \n",
      "\u001b[1;4mValidati 131    - 100    -            0.3616   0.0014   0.6225   0.6247   0.0000   0.0000   |            0.3046   0.6270   0.6315   0.0000   0.0000   - 0.5872  \u001b[0m\n",
      "Training 132    - 100    -            0.3591   0.0014   1.0000   1.0000   0.8305   0.8313   |            0.3025   1.0000   1.0000   0.8340   0.8363   - 20.7812 \n",
      "\u001b[1;4mValidati 132    - 100    -            0.3603   0.0014   0.6487   0.6436   0.0000   0.0000   |            0.3036   0.6567   0.6585   0.0000   0.0000   - 0.5827  \u001b[0m\n",
      "Training 133    - 100    -            0.3579   0.0014   0.9987   0.9987   0.8289   0.8302   |            0.3016   1.0000   0.9993   0.8342   0.8356   - 20.7379 \n",
      "\u001b[1;4mValidati 133    - 100    -            0.3590   0.0014   0.6475   0.6515   0.0000   0.0000   |            0.3026   0.6699   0.6747   0.0000   0.0000   - 0.5989  \u001b[0m\n",
      "Training 134    - 100    -            0.3566   0.0014   0.9975   0.9975   0.8293   0.8300   |            0.3006   1.0000   1.0000   0.8352   0.8372   - 20.7449 \n",
      "\u001b[1;4mValidati 134    - 100    -            0.3580   0.0014   0.6112   0.6101   0.0000   0.0000   |            0.3017   0.6623   0.6658   0.0000   0.0000   - 0.5890  \u001b[0m\n",
      "Training 135    - 100    -            0.3557   0.0014   0.9949   0.9955   0.8218   0.8241   |            0.2997   0.9987   0.9987   0.8344   0.8362   - 20.6505 \n",
      "\u001b[1;4mValidati 135    - 100    -            0.3569   0.0014   0.6357   0.6382   0.0000   0.0000   |            0.3009   0.6533   0.6554   0.0000   0.0000   - 0.5722  \u001b[0m\n",
      "Training 136    - 100    -            0.3545   0.0014   0.9987   0.9987   0.8251   0.8278   |            0.2989   0.9987   0.9987   0.8370   0.8384   - 20.7941 \n",
      "\u001b[1;4mValidati 136    - 100    -            0.3556   0.0014   0.6600   0.6653   0.0000   0.0000   |            0.3000   0.6567   0.6553   0.0000   0.0000   - 0.6128  \u001b[0m\n",
      "Training 137    - 100    -            0.3533   0.0014   0.9975   0.9974   0.8330   0.8350   |            0.2981   1.0000   0.9993   0.8356   0.8377   - 20.7034 \n",
      "\u001b[1;4mValidati 137    - 100    -            0.3543   0.0014   0.6589   0.6612   0.0000   0.0000   |            0.2992   0.6656   0.6683   0.0000   0.0000   - 0.5862  \u001b[0m\n",
      "Training 138    - 100    -            0.3520   0.0013   1.0000   1.0000   0.8323   0.8342   |            0.2973   1.0000   1.0000   0.8373   0.8395   - 20.7208 \n",
      "\u001b[1;4mValidati 138    - 100    -            0.3529   0.0013   0.6589   0.6671   0.0000   0.0000   |            0.2983   0.6877   0.6868   0.0000   0.0000   - 0.5855  \u001b[0m\n",
      "Training 139    - 100    -            0.3506   0.0013   1.0000   1.0000   0.8370   0.8382   |            0.2964   1.0000   1.0000   0.8383   0.8410   - 20.8341 \n",
      "\u001b[1;4mValidati 139    - 100    -            0.3517   0.0013   0.6614   0.6642   0.0000   0.0000   |            0.2975   0.6513   0.6606   0.0000   0.0000   - 0.5871  \u001b[0m\n",
      "Training 140    - 100    -            0.3494   0.0013   1.0000   1.0000   0.8402   0.8410   |            0.2956   1.0000   1.0000   0.8404   0.8426   - 20.7757 \n",
      "\u001b[1;4mValidati 140    - 100    -            0.3503   0.0013   0.6875   0.6907   0.0000   0.0000   |            0.2966   0.7042   0.7021   0.0000   0.0000   - 0.5727  \u001b[0m\n",
      "Training 141    - 100    -            0.3481   0.0013   1.0000   0.9993   0.8386   0.8407   |            0.2947   1.0000   1.0000   0.8403   0.8414   - 20.7439 \n",
      "\u001b[1;4mValidati 141    - 100    -            0.3492   0.0013   0.6614   0.6599   0.0000   0.0000   |            0.2959   0.6625   0.6710   0.0000   0.0000   - 0.5918  \u001b[0m\n",
      "Training 142    - 100    -            0.3470   0.0013   0.9987   0.9993   0.8396   0.8419   |            0.2940   1.0000   1.0000   0.8421   0.8425   - 20.7555 \n",
      "\u001b[1;4mValidati 142    - 100    -            0.3480   0.0013   0.6542   0.6584   0.0000   0.0000   |            0.2950   0.6855   0.6904   0.0000   0.0000   - 0.5675  \u001b[0m\n",
      "Training 143    - 100    -            0.3459   0.0013   0.9987   0.9993   0.8401   0.8420   |            0.2931   1.0000   0.9993   0.8399   0.8424   - 20.7510 \n",
      "\u001b[1;4mValidati 143    - 100    -            0.3468   0.0013   0.6621   0.6611   0.0000   0.0000   |            0.2941   0.6855   0.6908   0.0000   0.0000   - 0.5895  \u001b[0m\n",
      "Training 144    - 100    -            0.3446   0.0013   1.0000   1.0000   0.8389   0.8412   |            0.2922   1.0000   1.0000   0.8402   0.8415   - 20.7672 \n",
      "\u001b[1;4mValidati 144    - 100    -            0.3456   0.0013   0.6500   0.6533   0.0000   0.0000   |            0.2933   0.6667   0.6734   0.0000   0.0000   - 0.5722  \u001b[0m\n",
      "Training 145    - 100    -            0.3435   0.0013   0.9987   0.9987   0.8392   0.8414   |            0.2915   0.9987   0.9987   0.8409   0.8433   - 20.7600 \n",
      "\u001b[1;4mValidati 145    - 100    -            0.3446   0.0013   0.6377   0.6383   0.0000   0.0000   |            0.2925   0.6701   0.6689   0.0000   0.0000   - 0.6152  \u001b[0m\n",
      "Training 146    - 100    -            0.3425   0.0013   0.9975   0.9975   0.8327   0.8336   |            0.2907   1.0000   1.0000   0.8429   0.8444   - 20.6154 \n",
      "\u001b[1;4mValidati 146    - 100    -            0.3436   0.0013   0.6507   0.6562   0.0000   0.0000   |            0.2916   0.6931   0.6918   0.0000   0.0000   - 0.5932  \u001b[0m\n",
      "Training 147    - 100    -            0.3415   0.0013   1.0000   1.0000   0.8329   0.8344   |            0.2898   1.0000   1.0000   0.8401   0.8428   - 20.7533 \n",
      "\u001b[1;4mValidati 147    - 100    -            0.3426   0.0013   0.6531   0.6514   0.0000   0.0000   |            0.2908   0.6788   0.6717   0.0000   0.0000   - 0.5938  \u001b[0m\n",
      "Training 148    - 100    -            0.3405   0.0013   1.0000   1.0000   0.8373   0.8391   |            0.2891   1.0000   1.0000   0.8423   0.8424   - 20.7674 \n",
      "\u001b[1;4mValidati 148    - 100    -            0.3415   0.0013   0.6453   0.6491   0.0000   0.0000   |            0.2901   0.6688   0.6731   0.0000   0.0000   - 0.5943  \u001b[0m\n",
      "Training 149    - 100    -            0.3395   0.0013   0.9975   0.9975   0.8312   0.8330   |            0.2884   0.9975   0.9975   0.8406   0.8423   - 20.8472 \n",
      "\u001b[1;4mValidati 149    - 100    -            0.3406   0.0013   0.6422   0.6450   0.0000   0.0000   |            0.2894   0.6567   0.6576   0.0000   0.0000   - 0.5730  \u001b[0m\n",
      "Training 150    - 100    -            0.3386   0.0013   1.0000   1.0000   0.8374   0.8395   |            0.2876   1.0000   1.0000   0.8419   0.8433   - 20.8352 \n",
      "\u001b[1;4mValidati 150    - 100    -            0.3398   0.0013   0.6246   0.6270   0.0000   0.0000   |            0.2888   0.6435   0.6508   0.0000   0.0000   - 0.5872  \u001b[0m\n",
      "Training 151    - 100    -            0.3377   0.0013   1.0000   1.0000   0.8386   0.8402   |            0.2871   1.0000   1.0000   0.8410   0.8430   - 20.6757 \n",
      "\u001b[1;4mValidati 151    - 100    -            0.3387   0.0013   0.6587   0.6636   0.0000   0.0000   |            0.2881   0.6489   0.6529   0.0000   0.0000   - 0.5814  \u001b[0m\n",
      "Training 152    - 100    -            0.3367   0.0013   1.0000   1.0000   0.8383   0.8398   |            0.2863   1.0000   1.0000   0.8413   0.8425   - 20.7471 \n",
      "\u001b[1;4mValidati 152    - 100    -            0.3378   0.0013   0.6455   0.6508   0.0000   0.0000   |            0.2874   0.6413   0.6470   0.0000   0.0000   - 0.5854  \u001b[0m\n",
      "Training 153    - 100    -            0.3358   0.0013   1.0000   1.0000   0.8396   0.8410   |            0.2858   0.9987   0.9987   0.8390   0.8408   - 20.6782 \n",
      "\u001b[1;4mValidati 153    - 100    -            0.3367   0.0013   0.6576   0.6608   0.0000   0.0000   |            0.2867   0.6710   0.6698   0.0000   0.0000   - 0.5883  \u001b[0m\n",
      "Training 154    - 100    -            0.3347   0.0012   1.0000   1.0000   0.8390   0.8415   |            0.2850   1.0000   1.0000   0.8411   0.8432   - 20.7366 \n",
      "\u001b[1;4mValidati 154    - 100    -            0.3356   0.0012   0.6565   0.6633   0.0000   0.0000   |            0.2860   0.6533   0.6561   0.0000   0.0000   - 0.5795  \u001b[0m\n",
      "Training 155    - 100    -            0.3337   0.0012   1.0000   1.0000   0.8372   0.8394   |            0.2843   1.0000   1.0000   0.8389   0.8409   - 20.6861 \n",
      "\u001b[1;4mValidati 155    - 100    -            0.3346   0.0012   0.6489   0.6504   0.0000   0.0000   |            0.2853   0.6500   0.6588   0.0000   0.0000   - 0.6194  \u001b[0m\n",
      "Training 156    - 100    -            0.3326   0.0012   1.0000   1.0000   0.8362   0.8369   |            0.2837   1.0000   1.0000   0.8386   0.8397   - 20.7119 \n",
      "\u001b[1;4mValidati 156    - 100    -            0.3336   0.0012   0.6500   0.6576   0.0000   0.0000   |            0.2846   0.6467   0.6489   0.0000   0.0000   - 0.5736  \u001b[0m\n",
      "Training 157    - 100    -            0.3317   0.0012   1.0000   0.9993   0.8370   0.8384   |            0.2830   1.0000   1.0000   0.8396   0.8405   - 20.6705 \n",
      "\u001b[1;4mValidati 157    - 100    -            0.3326   0.0012   0.6402   0.6433   0.0000   0.0000   |            0.2839   0.6435   0.6516   0.0000   0.0000   - 0.6017  \u001b[0m\n",
      "Training 158    - 100    -            0.3307   0.0012   1.0000   1.0000   0.8383   0.8410   |            0.2823   1.0000   1.0000   0.8397   0.8409   - 20.8695 \n",
      "\u001b[1;4mValidati 158    - 100    -            0.3317   0.0012   0.6467   0.6515   0.0000   0.0000   |            0.2833   0.6567   0.6520   0.0000   0.0000   - 0.5760  \u001b[0m\n",
      "Training 159    - 100    -            0.3298   0.0012   1.0000   1.0000   0.8410   0.8426   |            0.2817   1.0000   1.0000   0.8403   0.8420   - 20.7968 \n",
      "\u001b[1;4mValidati 159    - 100    -            0.3307   0.0012   0.6545   0.6564   0.0000   0.0000   |            0.2827   0.6600   0.6636   0.0000   0.0000   - 0.6061  \u001b[0m\n",
      "Training 160    - 100    -            0.3289   0.0012   1.0000   1.0000   0.8385   0.8407   |            0.2811   1.0000   1.0000   0.8399   0.8425   - 20.6809 \n",
      "\u001b[1;4mValidati 160    - 100    -            0.3298   0.0012   0.6368   0.6426   0.0000   0.0000   |            0.2822   0.6304   0.6360   0.0000   0.0000   - 0.5775  \u001b[0m\n",
      "Training 161    - 100    -            0.3279   0.0012   1.0000   1.0000   0.8401   0.8422   |            0.2806   1.0000   1.0000   0.8401   0.8424   - 20.7492 \n",
      "\u001b[1;4mValidati 161    - 100    -            0.3288   0.0012   0.6567   0.6591   0.0000   0.0000   |            0.2816   0.6522   0.6577   0.0000   0.0000   - 0.5901  \u001b[0m\n",
      "Training 162    - 100    -            0.3270   0.0012   1.0000   1.0000   0.8398   0.8415   |            0.2800   1.0000   1.0000   0.8405   0.8421   - 20.6937 \n",
      "\u001b[1;4mValidati 162    - 100    -            0.3279   0.0012   0.6556   0.6565   0.0000   0.0000   |            0.2809   0.6598   0.6566   0.0000   0.0000   - 0.5905  \u001b[0m\n",
      "Training 163    - 100    -            0.3261   0.0012   1.0000   1.0000   0.8416   0.8423   |            0.2793   1.0000   1.0000   0.8416   0.8432   - 20.7407 \n",
      "\u001b[1;4mValidati 163    - 100    -            0.3269   0.0012   0.6732   0.6766   0.0000   0.0000   |            0.2802   0.6953   0.6929   0.0000   0.0000   - 0.5748  \u001b[0m\n",
      "Training 164    - 100    -            0.3251   0.0012   1.0000   1.0000   0.8409   0.8434   |            0.2786   1.0000   1.0000   0.8404   0.8431   - 20.7707 \n",
      "\u001b[1;4mValidati 164    - 100    -            0.3260   0.0012   0.6281   0.6336   0.0000   0.0000   |            0.2796   0.6359   0.6408   0.0000   0.0000   - 0.5894  \u001b[0m\n",
      "Training 165    - 100    -            0.3243   0.0012   1.0000   1.0000   0.8396   0.8421   |            0.2781   1.0000   1.0000   0.8422   0.8443   - 20.7942 \n",
      "\u001b[1;4mValidati 165    - 100    -            0.3251   0.0012   0.6424   0.6445   0.0000   0.0000   |            0.2790   0.6600   0.6601   0.0000   0.0000   - 0.5776  \u001b[0m\n",
      "Training 166    - 100    -            0.3234   0.0012   1.0000   1.0000   0.8413   0.8434   |            0.2775   1.0000   1.0000   0.8401   0.8416   - 20.7322 \n",
      "\u001b[1;4mValidati 166    - 100    -            0.3241   0.0012   0.6699   0.6689   0.0000   0.0000   |            0.2783   0.6699   0.6710   0.0000   0.0000   - 0.6108  \u001b[0m\n",
      "Training 167    - 100    -            0.3224   0.0012   1.0000   1.0000   0.8408   0.8415   |            0.2768   1.0000   1.0000   0.8404   0.8429   - 20.6312 \n",
      "\u001b[1;4mValidati 167    - 100    -            0.3232   0.0012   0.6730   0.6757   0.0000   0.0000   |            0.2776   0.6830   0.6862   0.0000   0.0000   - 0.5726  \u001b[0m\n",
      "Training 168    - 100    -            0.3215   0.0012   0.9987   0.9979   0.8399   0.8414   |            0.2762   0.9987   0.9987   0.8389   0.8414   - 20.8372 \n",
      "\u001b[1;4mValidati 168    - 100    -            0.3223   0.0012   0.6400   0.6467   0.0000   0.0000   |            0.2771   0.6536   0.6569   0.0000   0.0000   - 0.6135  \u001b[0m\n",
      "Training 169    - 100    -            0.3206   0.0011   1.0000   1.0000   0.8404   0.8420   |            0.2756   1.0000   1.0000   0.8421   0.8436   - 20.6329 \n",
      "\u001b[1;4mValidati 169    - 100    -            0.3216   0.0011   0.6500   0.6542   0.0000   0.0000   |            0.2765   0.6545   0.6588   0.0000   0.0000   - 0.5887  \u001b[0m\n",
      "Training 170    - 100    -            0.3199   0.0011   1.0000   1.0000   0.8422   0.8441   |            0.2750   1.0000   1.0000   0.8408   0.8425   - 20.8197 \n",
      "\u001b[1;4mValidati 170    - 100    -            0.3207   0.0011   0.6522   0.6429   0.0000   0.0000   |            0.2759   0.6545   0.6576   0.0000   0.0000   - 0.5884  \u001b[0m\n",
      "Training 171    - 100    -            0.3190   0.0011   1.0000   1.0000   0.8445   0.8460   |            0.2744   1.0000   1.0000   0.8448   0.8460   - 20.7329 \n",
      "\u001b[1;4mValidati 171    - 100    -            0.3198   0.0011   0.6379   0.6407   0.0000   0.0000   |            0.2753   0.6688   0.6697   0.0000   0.0000   - 0.5981  \u001b[0m\n",
      "Training 172    - 100    -            0.3181   0.0011   1.0000   1.0000   0.8432   0.8452   |            0.2738   1.0000   0.9993   0.8428   0.8449   - 20.7325 \n",
      "\u001b[1;4mValidati 172    - 100    -            0.3191   0.0011   0.6368   0.6391   0.0000   0.0000   |            0.2747   0.6438   0.6432   0.0000   0.0000   - 0.5771  \u001b[0m\n",
      "Training 173    - 100    -            0.3174   0.0011   1.0000   1.0000   0.8432   0.8447   |            0.2733   1.0000   1.0000   0.8428   0.8451   - 20.8264 \n",
      "\u001b[1;4mValidati 173    - 100    -            0.3182   0.0011   0.6609   0.6594   0.0000   0.0000   |            0.2741   0.6810   0.6808   0.0000   0.0000   - 0.5886  \u001b[0m\n",
      "Training 174    - 100    -            0.3166   0.0011   1.0000   1.0000   0.8393   0.8417   |            0.2727   1.0000   1.0000   0.8389   0.8417   - 20.7094 \n",
      "\u001b[1;4mValidati 174    - 100    -            0.3174   0.0011   0.6520   0.6588   0.0000   0.0000   |            0.2735   0.6578   0.6623   0.0000   0.0000   - 0.5846  \u001b[0m\n",
      "Training 175    - 100    -            0.3158   0.0011   1.0000   1.0000   0.8424   0.8435   |            0.2721   1.0000   1.0000   0.8416   0.8428   - 20.8083 \n",
      "\u001b[1;4mValidati 175    - 100    -            0.3166   0.0011   0.6621   0.6625   0.0000   0.0000   |            0.2729   0.6433   0.6496   0.0000   0.0000   - 0.5927  \u001b[0m\n",
      "Training 176    - 100    -            0.3150   0.0011   1.0000   1.0000   0.8417   0.8437   |            0.2715   1.0000   1.0000   0.8418   0.8433   - 20.7622 \n",
      "\u001b[1;4mValidati 176    - 100    -            0.3158   0.0011   0.6500   0.6502   0.0000   0.0000   |            0.2724   0.6567   0.6626   0.0000   0.0000   - 0.5945  \u001b[0m\n",
      "Training 177    - 100    -            0.3142   0.0011   1.0000   1.0000   0.8428   0.8462   |            0.2710   1.0000   1.0000   0.8443   0.8469   - 20.7532 \n",
      "\u001b[1;4mValidati 177    - 100    -            0.3149   0.0011   0.6688   0.6666   0.0000   0.0000   |            0.2718   0.6788   0.6785   0.0000   0.0000   - 0.6284  \u001b[0m\n",
      "Training 178    - 100    -            0.3133   0.0011   1.0000   1.0000   0.8403   0.8429   |            0.2704   1.0000   1.0000   0.8408   0.8428   - 20.7831 \n",
      "\u001b[1;4mValidati 178    - 100    -            0.3142   0.0011   0.6598   0.6598   0.0000   0.0000   |            0.2713   0.6743   0.6762   0.0000   0.0000   - 0.5910  \u001b[0m\n",
      "Training 179    - 100    -            0.3126   0.0011   1.0000   1.0000   0.8424   0.8437   |            0.2699   1.0000   1.0000   0.8416   0.8432   - 20.8576 \n",
      "\u001b[1;4mValidati 179    - 100    -            0.3134   0.0011   0.6752   0.6770   0.0000   0.0000   |            0.2707   0.6830   0.6826   0.0000   0.0000   - 0.5759  \u001b[0m\n",
      "Training 180    - 100    -            0.3118   0.0011   1.0000   1.0000   0.8413   0.8419   |            0.2693   1.0000   1.0000   0.8416   0.8421   - 20.8846 \n",
      "\u001b[1;4mValidati 180    - 100    -            0.3126   0.0011   0.6621   0.6672   0.0000   0.0000   |            0.2701   0.6732   0.6738   0.0000   0.0000   - 0.5906  \u001b[0m\n",
      "Training 181    - 100    -            0.3110   0.0011   1.0000   0.9993   0.8403   0.8419   |            0.2688   0.9987   0.9987   0.8417   0.8432   - 20.6689 \n",
      "\u001b[1;4mValidati 181    - 100    -            0.3119   0.0011   0.6391   0.6460   0.0000   0.0000   |            0.2696   0.6435   0.6540   0.0000   0.0000   - 0.5754  \u001b[0m\n",
      "Training 182    - 100    -            0.3104   0.0011   0.9987   0.9987   0.8384   0.8404   |            0.2683   0.9987   0.9987   0.8408   0.8433   - 20.8827 \n",
      "\u001b[1;4mValidati 182    - 100    -            0.3113   0.0011   0.6446   0.6451   0.0000   0.0000   |            0.2692   0.6304   0.6343   0.0000   0.0000   - 0.6268  \u001b[0m\n",
      "Training 183    - 100    -            0.3098   0.0011   1.0000   1.0000   0.8385   0.8402   |            0.2679   1.0000   1.0000   0.8403   0.8417   - 20.7363 \n",
      "\u001b[1;4mValidati 183    - 100    -            0.3106   0.0011   0.6589   0.6569   0.0000   0.0000   |            0.2687   0.6710   0.6682   0.0000   0.0000   - 0.5910  \u001b[0m\n",
      "Training 184    - 100    -            0.3090   0.0011   1.0000   1.0000   0.8405   0.8426   |            0.2673   1.0000   1.0000   0.8407   0.8430   - 20.8141 \n",
      "\u001b[1;4mValidati 184    - 100    -            0.3099   0.0011   0.6444   0.6539   0.0000   0.0000   |            0.2682   0.6699   0.6703   0.0000   0.0000   - 0.5741  \u001b[0m\n",
      "Training 185    - 100    -            0.3084   0.0011   1.0000   1.0000   0.8406   0.8421   |            0.2669   1.0000   1.0000   0.8407   0.8420   - 20.8301 \n",
      "\u001b[1;4mValidati 185    - 100    -            0.3091   0.0011   0.6732   0.6764   0.0000   0.0000   |            0.2677   0.6357   0.6355   0.0000   0.0000   - 0.5906  \u001b[0m\n",
      "Training 186    - 100    -            0.3076   0.0011   1.0000   1.0000   0.8376   0.8405   |            0.2664   1.0000   1.0000   0.8379   0.8389   - 20.6846 \n",
      "\u001b[1;4mValidati 186    - 100    -            0.3084   0.0011   0.6797   0.6826   0.0000   0.0000   |            0.2672   0.6589   0.6605   0.0000   0.0000   - 0.5871  \u001b[0m\n",
      "Training 187    - 100    -            0.3069   0.0010   1.0000   1.0000   0.8382   0.8408   |            0.2659   1.0000   1.0000   0.8384   0.8398   - 20.8158 \n",
      "\u001b[1;4mValidati 187    - 100    -            0.3077   0.0010   0.6478   0.6500   0.0000   0.0000   |            0.2666   0.6864   0.6888   0.0000   0.0000   - 0.6008  \u001b[0m\n",
      "Training 188    - 100    -            0.3062   0.0010   1.0000   1.0000   0.8399   0.8417   |            0.2653   1.0000   1.0000   0.8411   0.8427   - 20.7396 \n",
      "\u001b[1;4mValidati 188    - 100    -            0.3070   0.0010   0.6688   0.6690   0.0000   0.0000   |            0.2661   0.6612   0.6522   0.0000   0.0000   - 0.5832  \u001b[0m\n",
      "Training 189    - 100    -            0.3056   0.0010   1.0000   1.0000   0.8437   0.8455   |            0.2649   1.0000   1.0000   0.8444   0.8455   - 20.9981 \n",
      "\u001b[1;4mValidati 189    - 100    -            0.3063   0.0010   0.6741   0.6776   0.0000   0.0000   |            0.2656   0.6766   0.6859   0.0000   0.0000   - 0.5883  \u001b[0m\n",
      "Training 190    - 100    -            0.3049   0.0010   0.9987   0.9993   0.8406   0.8427   |            0.2644   0.9987   0.9993   0.8424   0.8443   - 20.7119 \n",
      "\u001b[1;4mValidati 190    - 100    -            0.3057   0.0010   0.6632   0.6627   0.0000   0.0000   |            0.2652   0.6743   0.6741   0.0000   0.0000   - 0.5883  \u001b[0m\n",
      "Training 191    - 100    -            0.3042   0.0010   1.0000   1.0000   0.8379   0.8403   |            0.2639   1.0000   1.0000   0.8388   0.8412   - 20.8173 \n",
      "\u001b[1;4mValidati 191    - 100    -            0.3050   0.0010   0.6743   0.6732   0.0000   0.0000   |            0.2647   0.6688   0.6753   0.0000   0.0000   - 0.5750  \u001b[0m\n",
      "Training 192    - 100    -            0.3036   0.0010   1.0000   1.0000   0.8366   0.8393   |            0.2635   1.0000   1.0000   0.8372   0.8403   - 20.7531 \n",
      "\u001b[1;4mValidati 192    - 100    -            0.3043   0.0010   0.6556   0.6555   0.0000   0.0000   |            0.2642   0.6721   0.6758   0.0000   0.0000   - 0.6335  \u001b[0m\n",
      "Training 193    - 100    -            0.3029   0.0010   1.0000   1.0000   0.8412   0.8427   |            0.2630   1.0000   1.0000   0.8406   0.8424   - 20.7335 \n",
      "\u001b[1;4mValidati 193    - 100    -            0.3037   0.0010   0.6301   0.6420   0.0000   0.0000   |            0.2638   0.6721   0.6713   0.0000   0.0000   - 0.5759  \u001b[0m\n",
      "Training 194    - 100    -            0.3023   0.0010   1.0000   1.0000   0.8392   0.8427   |            0.2625   1.0000   1.0000   0.8397   0.8429   - 20.7612 \n",
      "\u001b[1;4mValidati 194    - 100    -            0.3031   0.0010   0.6467   0.6461   0.0000   0.0000   |            0.2633   0.6710   0.6732   0.0000   0.0000   - 0.5903  \u001b[0m\n",
      "Training 195    - 100    -            0.3017   0.0010   1.0000   1.0000   0.8412   0.8426   |            0.2621   1.0000   1.0000   0.8413   0.8426   - 20.5727 \n",
      "\u001b[1;4mValidati 195    - 100    -            0.3024   0.0010   0.6688   0.6750   0.0000   0.0000   |            0.2628   0.6732   0.6724   0.0000   0.0000   - 0.5765  \u001b[0m\n",
      "Training 196    - 100    -            0.3011   0.0010   1.0000   1.0000   0.8388   0.8408   |            0.2616   1.0000   1.0000   0.8388   0.8411   - 20.7862 \n",
      "\u001b[1;4mValidati 196    - 100    -            0.3018   0.0010   0.6621   0.6648   0.0000   0.0000   |            0.2624   0.6821   0.6801   0.0000   0.0000   - 0.6403  \u001b[0m\n",
      "Training 197    - 100    -            0.3004   0.0010   1.0000   1.0000   0.8377   0.8399   |            0.2612   1.0000   1.0000   0.8378   0.8409   - 20.7779 \n",
      "\u001b[1;4mValidati 197    - 100    -            0.3013   0.0010   0.6458   0.6440   0.0000   0.0000   |            0.2620   0.6589   0.6580   0.0000   0.0000   - 0.5908  \u001b[0m\n",
      "Training 198    - 100    -            0.2999   0.0010   1.0000   1.0000   0.8414   0.8421   |            0.2608   1.0000   1.0000   0.8415   0.8431   - 20.7485 \n",
      "\u001b[1;4mValidati 198    - 100    -            0.3006   0.0010   0.6853   0.6848   0.0000   0.0000   |            0.2615   0.6897   0.6880   0.0000   0.0000   - 0.5806  \u001b[0m\n",
      "Training 199    - 100    -            0.2992   0.0010   1.0000   1.0000   0.8406   0.8425   |            0.2603   1.0000   1.0000   0.8412   0.8425   - 20.7159 \n",
      "\u001b[1;4mValidati 199    - 100    -            0.3000   0.0010   0.6413   0.6456   0.0000   0.0000   |            0.2612   0.6491   0.6476   0.0000   0.0000   - 0.6320  \u001b[0m\n",
      "Training 200    - 100    -            0.2987   0.0010   1.0000   1.0000   0.8425   0.8435   |            0.2600   1.0000   1.0000   0.8425   0.8436   - 20.7164 \n",
      "\u001b[1;4mValidati 200    - 100    -            0.2994   0.0010   0.6654   0.6671   0.0000   0.0000   |            0.2608   0.6612   0.6597   0.0000   0.0000   - 0.5771  \u001b[0m\r"
     ]
    }
   ],
   "source": [
    "print(header)\n",
    "\n",
    "start_epoch = checkpoint.epoch_counter\n",
    "end_epoch = args.nb_epoch\n",
    "\n",
    "for e in range(start_epoch, args.nb_epoch):\n",
    "    train(e)\n",
    "    val(e)\n",
    "    \n",
    "    tensorboard.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the hyper parameters and the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {}\n",
    "for key, value in args.__dict__.items():\n",
    "    hparams[key] = str(value)\n",
    "\n",
    "final_metrics = {\n",
    "    \"max_acc_student\": maximum_tracker.max[\"student_acc\"],\n",
    "    \"max_f1_student\": maximum_tracker.max[\"student_f1\"],\n",
    "    \"max_acc_teacher\": maximum_tracker.max[\"teacher_acc\"],\n",
    "    \"max_f1_teacher\": maximum_tracker.max[\"teacher_f1\"],\n",
    "}\n",
    "\n",
    "tensorboard.add_hparams(hparams, final_metrics)\n",
    "\n",
    "tensorboard.flush()\n",
    "tensorboard.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x1008 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = list(range(checkpoint.epoch_counter))\n",
    "sm = lambda y, w: np.convolve(y, np.ones(w)/w, mode='same')\n",
    "pp = lambda k: plt.plot(x, tensorboard.history[k], label=f\"{k} = {max(tensorboard.history[k])}\")\n",
    "spp = lambda k: plt.plot(x, sm(tensorboard.history[k], 5), label=f\"{k} = {max(tensorboard.history[k])}\")\n",
    "\n",
    "\n",
    "plt.figure(0, figsize=(30, 14))\n",
    "plt.subplot(2, 3, 1)\n",
    "pp(\"max/student_acc\")\n",
    "pp(\"max/teacher_acc\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "pp(\"hyperparameters/learning_rate\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleAttributeError",
     "evalue": "'MixUpBatchShuffle' object has no attribute 'lambda_history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-27c4ea1a1ca9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixup_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambda_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/pytorch-dev/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    937\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m    940\u001b[0m             type(self).__name__, name))\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'MixUpBatchShuffle' object has no attribute 'lambda_history'"
     ]
    }
   ],
   "source": [
    "plt.hist(mixup_fn.lambda_history, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "320000*2/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "64*500*4/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "625/125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-dev",
   "language": "python",
   "name": "pytorch-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
