{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1703.01780.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/users/samova/lcances/.miniconda3/envs/pytorch-dev/bin/python'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import time\n",
    "import pprint\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from SSL.util.loaders import (\n",
    "    load_dataset,\n",
    "    load_optimizer,\n",
    "    load_callbacks,\n",
    "    load_preprocesser,\n",
    ")\n",
    "from SSL.util.model_loader import load_model\n",
    "from SSL.util.checkpoint import CheckPoint, mSummaryWriter\n",
    "from SSL.util.mixup import MixUpBatchShuffle\n",
    "from SSL.util.utils import reset_seed, get_datetime, track_maximum, DotDict\n",
    "from SSL.ramps import Warmup, sigmoid_rampup\n",
    "from SSL.losses import JensenShanon\n",
    "\n",
    "from metric_utils.metrics import CategoricalAccuracy, FScore, ContinueAverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--from_config\", default=\"\", type=str)\n",
    "parser.add_argument(\"-d\", \"--dataset_root\", default=\"../../datasets\", type=str)\n",
    "parser.add_argument(\"-D\", \"--dataset\", default=\"ubs8k\", type=str)\n",
    "\n",
    "group_t = parser.add_argument_group(\"Commun parameters\")\n",
    "group_t.add_argument(\"-m\", \"--model\", default=\"wideresnet28_2\", type=str)\n",
    "group_t.add_argument(\"--supervised_ratio\", default=0.1, type=float)\n",
    "group_t.add_argument(\"--batch_size\", default=64, type=int)\n",
    "group_t.add_argument(\"--nb_epoch\", default=200, type=int)\n",
    "group_t.add_argument(\"--learning_rate\", default=0.001, type=float)\n",
    "group_t.add_argument(\"--resume\", action=\"store_true\", default=False)\n",
    "group_t.add_argument(\"--seed\", default=1234, type=int)\n",
    "group_t.add_argument(\"--num_classes\", default=10, type=int)\n",
    "\n",
    "group_u = parser.add_argument_group(\"Datasets parameters\")\n",
    "group_u.add_argument(\n",
    "    \"-t\", \"--train_folds\", nargs=\"+\", default=[1, 2, 3, 4, 5, 6, 7, 8, 9], type=int\n",
    ")\n",
    "group_u.add_argument(\"-v\", \"--val_folds\", nargs=\"+\", default=[10], type=int)\n",
    "\n",
    "group_s = parser.add_argument_group(\"Student teacher parameters\")\n",
    "group_s.add_argument(\"--ema_alpha\", default=0.999, type=float)\n",
    "group_s.add_argument(\"--warmup_length\", default=50, type=int)\n",
    "group_s.add_argument(\"--lambda_cost_max\", default=1, type=float)\n",
    "group_s.add_argument(\"--teacher_noise\", default=0, type=float)\n",
    "group_s.add_argument(\"--ccost_softmax\", action=\"store_true\", default=False)\n",
    "group_s.add_argument(\"--ccost_method\", type=str, default=\"mse\")\n",
    "\n",
    "group_mixup = parser.add_argument_group(\"Mixup parameters\")\n",
    "group_mixup.add_argument(\"--mixup\", action=\"store_true\", default=False)\n",
    "group_mixup.add_argument(\"--mixup_alpha\", type=float, default=0.4)\n",
    "group_mixup.add_argument(\"--mixup_max\", action=\"store_true\", default=False)\n",
    "group_mixup.add_argument(\"--mixup_label\", action=\"store_true\", default=False)\n",
    "\n",
    "group_l = parser.add_argument_group(\"Logs\")\n",
    "group_l.add_argument(\"--checkpoint_root\", default=\"../../model_save/\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_root\", default=\"../../tensorboard/\", type=str)\n",
    "group_l.add_argument(\"--checkpoint_path\", default=\"mean-teacher_mixup\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_path\", default=\"mean-teacher_mixup\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_sufix\", default=\"\", type=str)\n",
    "\n",
    "args = parser.parse_args([\"--mixup\", \"--mixup_max\", \"--ccost_softmax\"])\n",
    "\n",
    "tensorboard_path = os.path.join(\n",
    "    args.tensorboard_root, args.dataset, args.tensorboard_path\n",
    ")\n",
    "checkpoint_path = os.path.join(args.checkpoint_root, args.dataset, args.checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 64,\n",
      " 'ccost_method': 'mse',\n",
      " 'ccost_softmax': True,\n",
      " 'checkpoint_path': 'mean-teacher_mixup',\n",
      " 'checkpoint_root': '../model_save/',\n",
      " 'dataset': 'ubs8k',\n",
      " 'dataset_root': '../datasets',\n",
      " 'ema_alpha': 0.999,\n",
      " 'from_config': '',\n",
      " 'lambda_cost_max': 1,\n",
      " 'learning_rate': 0.001,\n",
      " 'mixup': True,\n",
      " 'mixup_alpha': 0.4,\n",
      " 'mixup_label': False,\n",
      " 'mixup_max': True,\n",
      " 'model': 'wideresnet28_2',\n",
      " 'nb_epoch': 200,\n",
      " 'num_classes': 10,\n",
      " 'resume': False,\n",
      " 'seed': 1234,\n",
      " 'supervised_ratio': 0.1,\n",
      " 'teacher_noise': 0,\n",
      " 'tensorboard_path': 'mean-teacher_mixup',\n",
      " 'tensorboard_root': '../tensorboard/',\n",
      " 'tensorboard_sufix': '',\n",
      " 'train_folds': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      " 'val_folds': [10],\n",
      " 'warmup_length': 50}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(vars(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "reset_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/PyTorch/audio/torchaudio/extension/extension.py:14: UserWarning: torchaudio C++ extension is not available.\n",
      "  warnings.warn('torchaudio C++ extension is not available.')\n"
     ]
    }
   ],
   "source": [
    "train_transform, val_transform = load_preprocesser(args.dataset, \"mean-teacher\")\n",
    "train_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:16<00:00,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_batch_size:  6\n",
      "u_batch_size:  58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/users/samova/lcances/Datasets/UrbanSound8K/ubs8k/datasets.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.y[\"idx\"] = list(range(len(self.y)))\n"
     ]
    }
   ],
   "source": [
    "manager, train_loader, val_loader = load_dataset(\n",
    "    args.dataset,\n",
    "    \"mean-teacher\",\n",
    "    dataset_root=args.dataset_root,\n",
    "    supervised_ratio=args.supervised_ratio,\n",
    "    batch_size=args.batch_size,\n",
    "    train_folds=args.train_folds,\n",
    "    val_folds=args.val_folds,\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 173)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = tuple(train_loader._iterables[0].dataset[0][0].shape)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_func = load_model(args.dataset, args.model)\n",
    "\n",
    "student = model_func(input_shape=input_shape, num_classes=args.num_classes)\n",
    "teacher = model_func(input_shape=input_shape, num_classes=args.num_classes)\n",
    "\n",
    "student = student.cuda()\n",
    "teacher = teacher.cuda()\n",
    "\n",
    "# We do not need gradient for the teacher model\n",
    "for p in teacher.parameters():\n",
    "    p.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 32, 64, 173]             288\n",
      "       BatchNorm2d-2          [-1, 32, 64, 173]              64\n",
      "              ReLU-3          [-1, 32, 64, 173]               0\n",
      "         MaxPool2d-4           [-1, 32, 32, 87]               0\n",
      "            Conv2d-5           [-1, 32, 32, 87]           9,216\n",
      "       BatchNorm2d-6           [-1, 32, 32, 87]              64\n",
      "              ReLU-7           [-1, 32, 32, 87]               0\n",
      "            Conv2d-8           [-1, 32, 32, 87]           9,216\n",
      "       BatchNorm2d-9           [-1, 32, 32, 87]              64\n",
      "             ReLU-10           [-1, 32, 32, 87]               0\n",
      "       BasicBlock-11           [-1, 32, 32, 87]               0\n",
      "           Conv2d-12           [-1, 32, 32, 87]           9,216\n",
      "      BatchNorm2d-13           [-1, 32, 32, 87]              64\n",
      "             ReLU-14           [-1, 32, 32, 87]               0\n",
      "           Conv2d-15           [-1, 32, 32, 87]           9,216\n",
      "      BatchNorm2d-16           [-1, 32, 32, 87]              64\n",
      "             ReLU-17           [-1, 32, 32, 87]               0\n",
      "       BasicBlock-18           [-1, 32, 32, 87]               0\n",
      "           Conv2d-19           [-1, 32, 32, 87]           9,216\n",
      "      BatchNorm2d-20           [-1, 32, 32, 87]              64\n",
      "             ReLU-21           [-1, 32, 32, 87]               0\n",
      "           Conv2d-22           [-1, 32, 32, 87]           9,216\n",
      "      BatchNorm2d-23           [-1, 32, 32, 87]              64\n",
      "             ReLU-24           [-1, 32, 32, 87]               0\n",
      "       BasicBlock-25           [-1, 32, 32, 87]               0\n",
      "           Conv2d-26           [-1, 32, 32, 87]           9,216\n",
      "      BatchNorm2d-27           [-1, 32, 32, 87]              64\n",
      "             ReLU-28           [-1, 32, 32, 87]               0\n",
      "           Conv2d-29           [-1, 32, 32, 87]           9,216\n",
      "      BatchNorm2d-30           [-1, 32, 32, 87]              64\n",
      "             ReLU-31           [-1, 32, 32, 87]               0\n",
      "       BasicBlock-32           [-1, 32, 32, 87]               0\n",
      "           Conv2d-33           [-1, 64, 16, 44]          18,432\n",
      "      BatchNorm2d-34           [-1, 64, 16, 44]             128\n",
      "             ReLU-35           [-1, 64, 16, 44]               0\n",
      "           Conv2d-36           [-1, 64, 16, 44]          36,864\n",
      "      BatchNorm2d-37           [-1, 64, 16, 44]             128\n",
      "           Conv2d-38           [-1, 64, 16, 44]           2,048\n",
      "      BatchNorm2d-39           [-1, 64, 16, 44]             128\n",
      "             ReLU-40           [-1, 64, 16, 44]               0\n",
      "       BasicBlock-41           [-1, 64, 16, 44]               0\n",
      "           Conv2d-42           [-1, 64, 16, 44]          36,864\n",
      "      BatchNorm2d-43           [-1, 64, 16, 44]             128\n",
      "             ReLU-44           [-1, 64, 16, 44]               0\n",
      "           Conv2d-45           [-1, 64, 16, 44]          36,864\n",
      "      BatchNorm2d-46           [-1, 64, 16, 44]             128\n",
      "             ReLU-47           [-1, 64, 16, 44]               0\n",
      "       BasicBlock-48           [-1, 64, 16, 44]               0\n",
      "           Conv2d-49           [-1, 64, 16, 44]          36,864\n",
      "      BatchNorm2d-50           [-1, 64, 16, 44]             128\n",
      "             ReLU-51           [-1, 64, 16, 44]               0\n",
      "           Conv2d-52           [-1, 64, 16, 44]          36,864\n",
      "      BatchNorm2d-53           [-1, 64, 16, 44]             128\n",
      "             ReLU-54           [-1, 64, 16, 44]               0\n",
      "       BasicBlock-55           [-1, 64, 16, 44]               0\n",
      "           Conv2d-56           [-1, 64, 16, 44]          36,864\n",
      "      BatchNorm2d-57           [-1, 64, 16, 44]             128\n",
      "             ReLU-58           [-1, 64, 16, 44]               0\n",
      "           Conv2d-59           [-1, 64, 16, 44]          36,864\n",
      "      BatchNorm2d-60           [-1, 64, 16, 44]             128\n",
      "             ReLU-61           [-1, 64, 16, 44]               0\n",
      "       BasicBlock-62           [-1, 64, 16, 44]               0\n",
      "           Conv2d-63           [-1, 128, 8, 22]          73,728\n",
      "      BatchNorm2d-64           [-1, 128, 8, 22]             256\n",
      "             ReLU-65           [-1, 128, 8, 22]               0\n",
      "           Conv2d-66           [-1, 128, 8, 22]         147,456\n",
      "      BatchNorm2d-67           [-1, 128, 8, 22]             256\n",
      "           Conv2d-68           [-1, 128, 8, 22]           8,192\n",
      "      BatchNorm2d-69           [-1, 128, 8, 22]             256\n",
      "             ReLU-70           [-1, 128, 8, 22]               0\n",
      "       BasicBlock-71           [-1, 128, 8, 22]               0\n",
      "           Conv2d-72           [-1, 128, 8, 22]         147,456\n",
      "      BatchNorm2d-73           [-1, 128, 8, 22]             256\n",
      "             ReLU-74           [-1, 128, 8, 22]               0\n",
      "           Conv2d-75           [-1, 128, 8, 22]         147,456\n",
      "      BatchNorm2d-76           [-1, 128, 8, 22]             256\n",
      "             ReLU-77           [-1, 128, 8, 22]               0\n",
      "       BasicBlock-78           [-1, 128, 8, 22]               0\n",
      "           Conv2d-79           [-1, 128, 8, 22]         147,456\n",
      "      BatchNorm2d-80           [-1, 128, 8, 22]             256\n",
      "             ReLU-81           [-1, 128, 8, 22]               0\n",
      "           Conv2d-82           [-1, 128, 8, 22]         147,456\n",
      "      BatchNorm2d-83           [-1, 128, 8, 22]             256\n",
      "             ReLU-84           [-1, 128, 8, 22]               0\n",
      "       BasicBlock-85           [-1, 128, 8, 22]               0\n",
      "           Conv2d-86           [-1, 128, 8, 22]         147,456\n",
      "      BatchNorm2d-87           [-1, 128, 8, 22]             256\n",
      "             ReLU-88           [-1, 128, 8, 22]               0\n",
      "           Conv2d-89           [-1, 128, 8, 22]         147,456\n",
      "      BatchNorm2d-90           [-1, 128, 8, 22]             256\n",
      "             ReLU-91           [-1, 128, 8, 22]               0\n",
      "       BasicBlock-92           [-1, 128, 8, 22]               0\n",
      "AdaptiveAvgPool2d-93            [-1, 128, 1, 1]               0\n",
      "           Linear-94                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 1,471,978\n",
      "Trainable params: 1,471,978\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 43.29\n",
      "Params size (MB): 5.62\n",
      "Estimated Total Size (MB): 48.95\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "s = summary(student, input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_root = (\n",
    "    f\"{args.model}/{args.supervised_ratio}/{get_datetime()}_{model_func.__name__}\"\n",
    ")\n",
    "checkpoint_root = f\"{args.model}/{args.supervised_ratio}/{model_func.__name__}\"\n",
    "\n",
    "# mea teacher parameters\n",
    "sufix_title = f\"_{args.ema_alpha}-emaa\"\n",
    "sufix_title += f\"_{args.warmup_length}-wl\"\n",
    "sufix_title += f\"_{args.lambda_cost_max}-lccm\"\n",
    "\n",
    "# mixup parameters\n",
    "if args.mixup:\n",
    "    sufix_title += \"_mixup\"\n",
    "    if args.mixup_max:\n",
    "        sufix_title += \"-max\"\n",
    "    if args.mixup_label:\n",
    "        sufix_title += \"-label\"\n",
    "    sufix_title += f\"-{args.mixup_alpha}-a\"\n",
    "\n",
    "# ccost function and method\n",
    "if args.ccost_method:\n",
    "    sufix_title += \"_cc-MSE\"\n",
    "if args.ccost_softmax:\n",
    "    sufix_title += \"-SOFTMAX\"\n",
    "\n",
    "# normale training parameters\n",
    "sufix_title += f\"_{args.learning_rate}-lr\"\n",
    "sufix_title += f\"_{args.supervised_ratio}-sr\"\n",
    "sufix_title += f\"_{args.nb_epoch}-e\"\n",
    "sufix_title += f\"_{args.batch_size}-bs\"\n",
    "sufix_title += f\"_{args.seed}-seed\"\n",
    "\n",
    "tensorboard_title = tensorboard_root + sufix_title\n",
    "checkpoint_title = checkpoint_root + sufix_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../tensorboard/ubs8k/mean-teacher_mixup/wideresnet28_2/0.1/2021-01-15_22:13:40_wideresnet28_2_0.999-emaa_50-wl_1-lccm_mixup-max-0.4-a_cc-MSE-SOFTMAX_0.001-lr_0.1-sr_200-e_64-bs_1234-seed\n"
     ]
    }
   ],
   "source": [
    "tensorboard = mSummaryWriter(\n",
    "    log_dir=\"%s/%s\" % (tensorboard_path, tensorboard_title), comment=model_func.__name__\n",
    ")\n",
    "print(os.path.join(tensorboard_path, tensorboard_title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer & callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = load_optimizer(\n",
    "    args.dataset, \"mean-teacher\", student=student, learning_rate=args.learning_rate\n",
    ")\n",
    "callbacks = load_callbacks(\n",
    "    args.dataset, \"mean-teacher\", optimizer=optimizer, nb_epoch=args.nb_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint initialise at:  /users/samova/lcances/semi-supervised/model_save/ubs8k/mean-teacher_mixup/wideresnet28_2/0.1/wideresnet28_2_0.999-emaa_50-wl_1-lccm_mixup-max-0.4-a_cc-MSE-SOFTMAX_0.001-lr_0.1-sr_200-e_64-bs_1234-seed.torch\n",
      "name:  wideresnet28_2_0.999-emaa_50-wl_1-lccm_mixup-max-0.4-a_cc-MSE-SOFTMAX_0.001-lr_0.1-sr_200-e_64-bs_1234-seed.torch\n",
      "mode:  max\n"
     ]
    }
   ],
   "source": [
    "# losses\n",
    "loss_ce = nn.CrossEntropyLoss(reduction=\"mean\")  # Supervised loss\n",
    "\n",
    "if args.ccost_method == \"mse\":\n",
    "    consistency_cost = nn.MSELoss(reduction=\"mean\")  # Unsupervised loss\n",
    "elif args.ccost_method == \"js\":\n",
    "    consistency_cost = JensenShanon\n",
    "\n",
    "lambda_cost = Warmup(args.lambda_cost_max, args.warmup_length, sigmoid_rampup)\n",
    "callbacks += [lambda_cost]\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint = CheckPoint(\n",
    "    student,\n",
    "    optimizer,\n",
    "    mode=\"max\",\n",
    "    name=\"%s/%s.torch\" % (checkpoint_path, checkpoint_title),\n",
    ")\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSELoss()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consistency_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_calculator():\n",
    "    def c(logits, y):\n",
    "        with torch.no_grad():\n",
    "            y_one_hot = F.one_hot(y, num_classes=args.num_classes)\n",
    "\n",
    "            pred = torch.softmax(logits, dim=1)\n",
    "            arg = torch.argmax(logits, dim=1)\n",
    "\n",
    "            acc = c.fn.acc(arg, y).mean\n",
    "            f1 = c.fn.f1(pred, y_one_hot).mean\n",
    "\n",
    "            return (\n",
    "                acc,\n",
    "                f1,\n",
    "            )\n",
    "\n",
    "    c.fn = DotDict(\n",
    "        acc=CategoricalAccuracy(),\n",
    "        f1=FScore(),\n",
    "    )\n",
    "\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_student_s_metrics = metrics_calculator()\n",
    "calc_student_u_metrics = metrics_calculator()\n",
    "calc_teacher_s_metrics = metrics_calculator()\n",
    "calc_teacher_u_metrics = metrics_calculator()\n",
    "\n",
    "avg_Sce = ContinueAverage()\n",
    "avg_Tce = ContinueAverage()\n",
    "avg_ccost = ContinueAverage()\n",
    "\n",
    "softmax_fn = lambda x: x\n",
    "if args.ccost_softmax:\n",
    "    softmax_fn = nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "def reset_metrics():\n",
    "    for d in [\n",
    "        calc_student_s_metrics.fn,\n",
    "        calc_student_u_metrics.fn,\n",
    "        calc_teacher_s_metrics.fn,\n",
    "        calc_teacher_u_metrics.fn,\n",
    "    ]:\n",
    "        for fn in d.values():\n",
    "            fn.reset()\n",
    "\n",
    "\n",
    "maximum_tracker = track_maximum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Can resume previous training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if args.resume:\n",
    "    checkpoint.load_last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".        Epoch  - %      - Student:   ce       ccost    acc_s    f1_s     acc_u    f1_u     | Teacher:   ce       acc_s    f1_s     acc_u    f1_u     - Time    \n"
     ]
    }
   ],
   "source": [
    "UNDERLINE_SEQ = \"\\033[1;4m\"\n",
    "RESET_SEQ = \"\\033[0m\"\n",
    "\n",
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<10.8} {:<8.6} {:<8.6} {:<8.6} {:<8.6} {:<8.6} {:<8.6} | {:<10.8} {:<8.6} {:<8.6} {:<8.6} {:<8.6} {:<8.6} - {:<8.6}\"\n",
    "value_form = \"{:<8.8} {:<6d} - {:<6d} - {:<10.8} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} | {:<10.8} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f} - {:<8.4f}\"\n",
    "header = header_form.format(\n",
    "    \".               \",\n",
    "    \"Epoch\",\n",
    "    \"%\",\n",
    "    \"Student:\",\n",
    "    \"ce\",\n",
    "    \"ccost\",\n",
    "    \"acc_s\",\n",
    "    \"f1_s\",\n",
    "    \"acc_u\",\n",
    "    \"f1_u\",\n",
    "    \"Teacher:\",\n",
    "    \"ce\",\n",
    "    \"acc_s\",\n",
    "    \"f1_s\",\n",
    "    \"acc_u\",\n",
    "    \"f1_u\",\n",
    "    \"Time\",\n",
    ")\n",
    "\n",
    "train_form = value_form\n",
    "val_form = UNDERLINE_SEQ + value_form + RESET_SEQ\n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_teacher_model(student_model, teacher_model, alpha, epoch):\n",
    "\n",
    "    # Use the true average until the exponential average is more correct\n",
    "    alpha = min(1 - 1 / (epoch + 1), alpha)\n",
    "\n",
    "    for param, ema_param in zip(student_model.parameters(), teacher_model.parameters()):\n",
    "        ema_param.data.mul_(alpha).add_(param.data, alpha=1 - alpha)\n",
    "\n",
    "\n",
    "noise_fn = lambda x: x\n",
    "if args.teacher_noise != 0:\n",
    "    n_db = args.teacher_noise\n",
    "    noise_fn = transforms.Lambda(\n",
    "        lambda x: x + (torch.rand(x.shape).cuda() * n_db + n_db)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup_fn = MixUpBatchShuffle(\n",
    "    alpha=args.mixup_alpha, apply_max=args.mixup_max, mix_labels=args.mixup_label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "\n",
    "    nb_batch = len(train_loader)\n",
    "\n",
    "    reset_metrics()\n",
    "    student.train()\n",
    "\n",
    "    for i, (S, U) in enumerate(train_loader):\n",
    "        x_s, y_s = S\n",
    "        x_u, y_u = U\n",
    "\n",
    "        # Apply mixup if needed, otherwise no mixup.\n",
    "        n_x_s, n_y_s, n_x_u, n_y_u = x_s, y_s, x_u, y_u\n",
    "        if args.mixup:\n",
    "            n_x_s, n_y_s = mixup_fn(x_s, y_s)\n",
    "            n_x_u, n_y_u = mixup_fn(x_u, y_u)\n",
    "\n",
    "        n_x_s, n_x_u = n_x_s.cuda(), n_x_u.cuda()\n",
    "        x_s, x_u = x_s.cuda(), x_u.cuda()\n",
    "        y_s, y_u = y_s.cuda(), y_u.cuda()\n",
    "\n",
    "        # Predictions\n",
    "        student_s_logits = student(x_s)\n",
    "        student_u_logits = student(x_u)\n",
    "        teacher_s_logits = teacher(n_x_s)\n",
    "        teacher_u_logits = teacher(n_x_u)\n",
    "\n",
    "        # Calculate supervised loss (only student on S)\n",
    "        loss = loss_ce(student_s_logits, y_s)\n",
    "\n",
    "        # Calculate consistency cost (mse(student(x), teacher(x))) x is S + U\n",
    "        student_logits = torch.cat((student_s_logits, student_u_logits), dim=0)\n",
    "        teacher_logits = torch.cat((teacher_s_logits, teacher_u_logits), dim=0)\n",
    "        ccost = consistency_cost(softmax_fn(student_logits), softmax_fn(teacher_logits))\n",
    "\n",
    "        total_loss = loss + lambda_cost() * ccost\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            # Teacher prediction (for metrics purpose)\n",
    "            _teacher_loss = loss_ce(teacher_s_logits, y_s)\n",
    "\n",
    "            # Update teacher\n",
    "            update_teacher_model(student, teacher, args.ema_alpha, epoch * nb_batch + i)\n",
    "\n",
    "            # Compute the metrics for the student\n",
    "            student_s_metrics = calc_student_s_metrics(student_s_logits, y_s)\n",
    "            student_u_metrics = calc_student_u_metrics(student_u_logits, y_u)\n",
    "            student_s_acc, student_s_f1, student_u_acc, student_u_f1 = (\n",
    "                *student_s_metrics,\n",
    "                *student_u_metrics,\n",
    "            )\n",
    "\n",
    "            # Compute the metrics for the teacher\n",
    "            teacher_s_metrics = calc_teacher_s_metrics(teacher_s_logits, y_s)\n",
    "            teacher_u_metrics = calc_teacher_u_metrics(teacher_u_logits, y_u)\n",
    "            teacher_s_acc, teacher_s_f1, teacher_u_acc, teacher_u_f1 = (\n",
    "                *teacher_s_metrics,\n",
    "                *teacher_u_metrics,\n",
    "            )\n",
    "\n",
    "            # Running average of the two losses\n",
    "            student_running_loss = avg_Sce(loss.item()).mean\n",
    "            teacher_running_loss = avg_Tce(_teacher_loss.item()).mean\n",
    "            running_ccost = avg_ccost(ccost.item()).mean\n",
    "\n",
    "            # logs\n",
    "            print(\n",
    "                train_form.format(\n",
    "                    \"Training: \",\n",
    "                    epoch + 1,\n",
    "                    int(100 * (i + 1) / nb_batch),\n",
    "                    \"\",\n",
    "                    student_running_loss,\n",
    "                    running_ccost,\n",
    "                    *student_s_metrics,\n",
    "                    *student_u_metrics,\n",
    "                    \"\",\n",
    "                    teacher_running_loss,\n",
    "                    *teacher_s_metrics,\n",
    "                    *teacher_u_metrics,\n",
    "                    time.time() - start_time\n",
    "                ),\n",
    "                end=\"\\r\",\n",
    "            )\n",
    "\n",
    "    tensorboard.add_scalar(\"train/student_acc_s\", student_s_acc, epoch)\n",
    "    tensorboard.add_scalar(\"train/student_acc_u\", student_u_acc, epoch)\n",
    "    tensorboard.add_scalar(\"train/student_f1_s\", student_s_f1, epoch)\n",
    "    tensorboard.add_scalar(\"train/student_f1_u\", student_u_f1, epoch)\n",
    "\n",
    "    tensorboard.add_scalar(\"train/teacher_acc_s\", teacher_s_acc, epoch)\n",
    "    tensorboard.add_scalar(\"train/teacher_acc_u\", teacher_u_acc, epoch)\n",
    "    tensorboard.add_scalar(\"train/teacher_f1_s\", teacher_s_f1, epoch)\n",
    "    tensorboard.add_scalar(\"train/teacher_f1_u\", teacher_u_f1, epoch)\n",
    "\n",
    "    tensorboard.add_scalar(\"train/student_loss\", student_running_loss, epoch)\n",
    "    tensorboard.add_scalar(\"train/teacher_loss\", teacher_running_loss, epoch)\n",
    "    tensorboard.add_scalar(\"train/consistency_cost\", running_ccost, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "    reset_metrics()\n",
    "    student.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (X, y) in enumerate(val_loader):\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            # Predictions\n",
    "            student_logits = student(X)\n",
    "            teacher_logits = teacher(X)\n",
    "\n",
    "            # Calculate supervised loss (only student on S)\n",
    "            loss = loss_ce(student_logits, y)\n",
    "            _teacher_loss = loss_ce(teacher_logits, y)  # for metrics only\n",
    "            ccost = consistency_cost(\n",
    "                softmax_fn(student_logits), softmax_fn(teacher_logits)\n",
    "            )\n",
    "\n",
    "            # Compute the metrics\n",
    "            y_one_hot = F.one_hot(y, num_classes=args.num_classes)\n",
    "\n",
    "            # ---- student ----\n",
    "            student_metrics = calc_student_s_metrics(student_logits, y)\n",
    "            student_acc, student_f1 = student_metrics\n",
    "\n",
    "            # ---- teacher ----\n",
    "            teacher_metrics = calc_teacher_s_metrics(teacher_logits, y)\n",
    "            teacher_acc, teacher_f1 = teacher_metrics\n",
    "\n",
    "            # Running average of the two losses\n",
    "            student_running_loss = avg_Sce(loss.item()).mean\n",
    "            teacher_running_loss = avg_Tce(_teacher_loss.item()).mean\n",
    "            running_ccost = avg_ccost(ccost.item()).mean\n",
    "\n",
    "            # logs\n",
    "            print(\n",
    "                val_form.format(\n",
    "                    \"Validation: \",\n",
    "                    epoch + 1,\n",
    "                    int(100 * (i + 1) / len(val_loader)),\n",
    "                    \"\",\n",
    "                    student_running_loss,\n",
    "                    running_ccost,\n",
    "                    *student_metrics,\n",
    "                    0.0,\n",
    "                    0.0,\n",
    "                    \"\",\n",
    "                    teacher_running_loss,\n",
    "                    *teacher_metrics,\n",
    "                    0.0,\n",
    "                    0.0,\n",
    "                    time.time() - start_time\n",
    "                ),\n",
    "                end=\"\\r\",\n",
    "            )\n",
    "\n",
    "    tensorboard.add_scalar(\"val/student_acc\", student_acc, epoch)\n",
    "    tensorboard.add_scalar(\"val/student_f1\", student_f1, epoch)\n",
    "    tensorboard.add_scalar(\"val/teacher_acc\", teacher_acc, epoch)\n",
    "    tensorboard.add_scalar(\"val/teacher_f1\", teacher_f1, epoch)\n",
    "    tensorboard.add_scalar(\"val/student_loss\", student_running_loss, epoch)\n",
    "    tensorboard.add_scalar(\"val/teacher_loss\", teacher_running_loss, epoch)\n",
    "    tensorboard.add_scalar(\"val/consistency_cost\", running_ccost, epoch)\n",
    "\n",
    "    tensorboard.add_scalar(\"hyperparameters/learning_rate\", get_lr(optimizer), epoch)\n",
    "    tensorboard.add_scalar(\"hyperparameters/lambda_cost_max\", lambda_cost(), epoch)\n",
    "\n",
    "    tensorboard.add_scalar(\n",
    "        \"max/student_acc\", maximum_tracker(\"student_acc\", student_acc), epoch\n",
    "    )\n",
    "    tensorboard.add_scalar(\n",
    "        \"max/teacher_acc\", maximum_tracker(\"teacher_acc\", teacher_acc), epoch\n",
    "    )\n",
    "    tensorboard.add_scalar(\n",
    "        \"max/student_f1\", maximum_tracker(\"student_f1\", student_f1), epoch\n",
    "    )\n",
    "    tensorboard.add_scalar(\n",
    "        \"max/teacher_f1\", maximum_tracker(\"teacher_f1\", teacher_f1), epoch\n",
    "    )\n",
    "\n",
    "    checkpoint.step(teacher_acc)\n",
    "    for c in callbacks:\n",
    "        c.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".        Epoch  - %      - Student:   ce       ccost    acc_s    f1_s     acc_u    f1_u     | Teacher:   ce       acc_s    f1_s     acc_u    f1_u     - Time    \n",
      "\n",
      "Training 1      - 100    -            2.1103   0.0053   0.2437   0.0381   0.2876   0.0936   |            2.0233   0.2790   0.0233   0.3080   0.0624   - 84.7137 \n",
      "\u001b[1;4mValidati 1      - 100    -            2.0715   0.0053   0.3857   0.1994   0.0000   0.0000   |            1.9893   0.3614   0.0798   0.0000   0.0000   - 7.4430  \u001b[0m\n",
      "Training 2      - 100    -            1.9369   0.0068   0.3649   0.1807   0.3991   0.2351   |            1.8923   0.3813   0.0660   0.3679   0.1245   - 20.4059 \n",
      "\u001b[1;4mValidati 2      - 100    -            1.9186   0.0068   0.3438   0.3062   0.0000   0.0000   |            1.8746   0.4493   0.1801   0.0000   0.0000   - 0.5574  \u001b[0m\n",
      "Training 3      - 100    -            1.8310   0.0075   0.4104   0.2605   0.4500   0.3135   |            1.8023   0.4217   0.1503   0.4465   0.2139   - 20.6622 \n",
      "\u001b[1;4mValidati 3      - 100    -            1.8213   0.0075   0.4080   0.3476   0.0000   0.0000   |            1.7919   0.5408   0.3371   0.0000   0.0000   - 0.5718  \u001b[0m\n",
      "Training 4      - 100    -            1.7459   0.0079   0.4508   0.3117   0.5004   0.3873   |            1.7358   0.4571   0.2213   0.4909   0.2859   - 20.6577 \n",
      "\u001b[1;4mValidati 4      - 100    -            1.7395   0.0079   0.4136   0.3885   0.0000   0.0000   |            1.7264   0.5252   0.3712   0.0000   0.0000   - 0.5630  \u001b[0m\n",
      "Training 5      - 100    -            1.6760   0.0085   0.5076   0.3892   0.5348   0.4429   |            1.6730   0.4899   0.3151   0.5068   0.3360   - 20.7784 \n",
      "\u001b[1;4mValidati 5      - 100    -            1.6711   0.0085   0.5183   0.4398   0.0000   0.0000   |            1.6671   0.5241   0.3907   0.0000   0.0000   - 0.5735  \u001b[0m\n",
      "Training 6      - 100    -            1.6065   0.0091   0.5644   0.4668   0.5592   0.4914   |            1.6219   0.5492   0.3502   0.5392   0.3732   - 20.8474 \n",
      "\u001b[1;4mValidati 6      - 100    -            1.6083   0.0092   0.3759   0.3784   0.0000   0.0000   |            1.6164   0.5763   0.4157   0.0000   0.0000   - 0.5892  \u001b[0m\n",
      "Training 7      - 100    -            1.5592   0.0096   0.5644   0.4994   0.5804   0.5245   |            1.5817   0.5568   0.3648   0.5660   0.4175   - 20.7327 \n",
      "\u001b[1;4mValidati 7      - 100    -            1.5582   0.0097   0.4542   0.4024   0.0000   0.0000   |            1.5780   0.5592   0.4011   0.0000   0.0000   - 0.5826  \u001b[0m\n",
      "Training 8      - 100    -            1.5023   0.0102   0.6338   0.5763   0.6221   0.5923   |            1.5372   0.6023   0.4121   0.5794   0.4351   - 21.1008 \n",
      "\u001b[1;4mValidati 8      - 100    -            1.5016   0.0102   0.5080   0.4921   0.0000   0.0000   |            1.5332   0.5897   0.4539   0.0000   0.0000   - 0.6159  \u001b[0m\n",
      "Training 9      - 100    -            1.4567   0.0108   0.6465   0.5806   0.6356   0.6087   |            1.5006   0.6098   0.4472   0.5772   0.4603   - 20.9830 \n",
      "\u001b[1;4mValidati 9      - 100    -            1.4553   0.0108   0.5188   0.4935   0.0000   0.0000   |            1.4980   0.5790   0.4457   0.0000   0.0000   - 0.5853  \u001b[0m\n",
      "Training 10     - 100    -            1.4085   0.0114   0.6604   0.6330   0.6550   0.6273   |            1.4643   0.6275   0.5101   0.6036   0.5094   - 21.1090 \n",
      "\u001b[1;4mValidati 10     - 100    -            1.4077   0.0115   0.5741   0.5516   0.0000   0.0000   |            1.4613   0.6022   0.5076   0.0000   0.0000   - 0.5631  \u001b[0m\n",
      "Training 11     - 100    -            1.3660   0.0120   0.6995   0.6718   0.6699   0.6569   |            1.4328   0.6313   0.5189   0.6000   0.5188   - 20.7632 \n",
      "\u001b[1;4mValidati 11     - 100    -            1.3647   0.0120   0.5621   0.5453   0.0000   0.0000   |            1.4300   0.6464   0.5567   0.0000   0.0000   - 0.5771  \u001b[0m\n",
      "Training 12     - 100    -            1.3183   0.0124   0.7525   0.7241   0.6807   0.6723   |            1.4009   0.6692   0.5759   0.6260   0.5669   - 20.7825 \n",
      "\u001b[1;4mValidati 12     - 100    -            1.3201   0.0125   0.5560   0.5213   0.0000   0.0000   |            1.3985   0.6431   0.5882   0.0000   0.0000   - 0.5800  \u001b[0m\n",
      "Training 13     - 100    -            1.2812   0.0129   0.7437   0.7300   0.6850   0.6801   |            1.3721   0.6553   0.5985   0.6423   0.5941   - 20.8432 \n",
      "\u001b[1;4mValidati 13     - 100    -            1.2816   0.0130   0.5705   0.5353   0.0000   0.0000   |            1.3705   0.5935   0.5541   0.0000   0.0000   - 0.5938  \u001b[0m\n",
      "Training 14     - 100    -            1.2414   0.0132   0.7854   0.7563   0.7057   0.6961   |            1.3401   0.7121   0.6553   0.6634   0.6345   - 20.9561 \n",
      "\u001b[1;4mValidati 14     - 100    -            1.2411   0.0132   0.5830   0.5449   0.0000   0.0000   |            1.3384   0.6132   0.5886   0.0000   0.0000   - 0.5756  \u001b[0m\n",
      "Training 15     - 100    -            1.2065   0.0136   0.7664   0.7461   0.7078   0.7031   |            1.3126   0.6982   0.6653   0.6463   0.6229   - 21.3968 \n",
      "\u001b[1;4mValidati 15     - 100    -            1.2080   0.0136   0.5145   0.5139   0.0000   0.0000   |            1.3115   0.5969   0.5876   0.0000   0.0000   - 0.5739  \u001b[0m\n",
      "Training 16     - 100    -            1.1728   0.0140   0.8093   0.7973   0.7167   0.7111   |            1.2885   0.7336   0.6873   0.6488   0.6217   - 20.8410 \n",
      "\u001b[1;4mValidati 16     - 100    -            1.1732   0.0140   0.5996   0.5816   0.0000   0.0000   |            1.2873   0.6067   0.6146   0.0000   0.0000   - 0.5752  \u001b[0m\n",
      "Training 17     - 100    -            1.1410   0.0142   0.8194   0.7991   0.7228   0.7239   |            1.2612   0.7601   0.7168   0.6850   0.6724   - 20.8275 \n",
      "\u001b[1;4mValidati 17     - 100    -            1.1419   0.0142   0.6042   0.6065   0.0000   0.0000   |            1.2604   0.6036   0.5887   0.0000   0.0000   - 0.5654  \u001b[0m\n",
      "Training 18     - 100    -            1.1076   0.0145   0.8396   0.8379   0.7500   0.7504   |            1.2288   0.8106   0.7771   0.6903   0.6782   - 20.8401 \n",
      "\u001b[1;4mValidati 18     - 100    -            1.1086   0.0145   0.6257   0.6360   0.0000   0.0000   |            1.2280   0.6022   0.6110   0.0000   0.0000   - 0.5872  \u001b[0m\n",
      "Training 19     - 100    -            1.0786   0.0147   0.8346   0.8319   0.7369   0.7393   |            1.2034   0.7765   0.7684   0.6908   0.6867   - 21.0707 \n",
      "\u001b[1;4mValidati 19     - 100    -            1.0807   0.0147   0.5556   0.5601   0.0000   0.0000   |            1.2034   0.6112   0.6068   0.0000   0.0000   - 0.5753  \u001b[0m\n",
      "Training 20     - 100    -            1.0519   0.0149   0.8460   0.8470   0.7387   0.7379   |            1.1779   0.8056   0.7820   0.6990   0.6943   - 20.9040 \n",
      "\u001b[1;4mValidati 20     - 100    -            1.0551   0.0150   0.5641   0.5571   0.0000   0.0000   |            1.1777   0.6308   0.6251   0.0000   0.0000   - 0.5751  \u001b[0m\n",
      "Training 21     - 100    -            1.0283   0.0151   0.8586   0.8513   0.7439   0.7407   |            1.1520   0.8157   0.8071   0.7217   0.7162   - 20.8740 \n",
      "\u001b[1;4mValidati 21     - 100    -            1.0304   0.0151   0.5908   0.5868   0.0000   0.0000   |            1.1517   0.6188   0.6249   0.0000   0.0000   - 0.5943  \u001b[0m\n",
      "Training 22     - 100    -            1.0048   0.0152   0.8649   0.8579   0.7408   0.7432   |            1.1299   0.8081   0.8016   0.7144   0.7153   - 21.0201 \n",
      "\u001b[1;4mValidati 22     - 100    -            1.0071   0.0152   0.6058   0.6130   0.0000   0.0000   |            1.1302   0.6011   0.6068   0.0000   0.0000   - 0.6040  \u001b[0m\n",
      "Training 23     - 100    -            0.9805   0.0153   0.8788   0.8797   0.7643   0.7679   |            1.1074   0.8295   0.8148   0.7252   0.7271   - 21.0353 \n",
      "\u001b[1;4mValidati 23     - 100    -            0.9819   0.0153   0.6344   0.6349   0.0000   0.0000   |            1.1078   0.6000   0.6018   0.0000   0.0000   - 0.6139  \u001b[0m\n",
      "Training 24     - 100    -            0.9566   0.0154   0.9028   0.8959   0.7558   0.7579   |            1.0817   0.8775   0.8662   0.7203   0.7246   - 21.3337 \n",
      "\u001b[1;4mValidati 24     - 100    -            0.9590   0.0154   0.5967   0.5990   0.0000   0.0000   |            1.0822   0.6201   0.6187   0.0000   0.0000   - 0.5652  \u001b[0m\n",
      "Training 25     - 100    -            0.9364   0.0156   0.8889   0.8786   0.7685   0.7709   |            1.0610   0.8548   0.8436   0.7145   0.7139   - 20.6825 \n",
      "\u001b[1;4mValidati 25     - 100    -            0.9388   0.0156   0.5795   0.5898   0.0000   0.0000   |            1.0613   0.6364   0.6358   0.0000   0.0000   - 0.5804  \u001b[0m\n",
      "Training 26     - 100    -            0.9157   0.0158   0.9066   0.8971   0.7547   0.7599   |            1.0396   0.8737   0.8673   0.7257   0.7270   - 20.8722 \n",
      "\u001b[1;4mValidati 26     - 100    -            0.9181   0.0158   0.6049   0.6041   0.0000   0.0000   |            1.0401   0.6364   0.6424   0.0000   0.0000   - 0.5764  \u001b[0m\n",
      "Training 27     - 100    -            0.8954   0.0159   0.9230   0.9168   0.7703   0.7728   |            1.0199   0.8775   0.8725   0.7223   0.7241   - 20.8612 \n",
      "\u001b[1;4mValidati 27     - 100    -            0.8980   0.0159   0.5944   0.6031   0.0000   0.0000   |            1.0208   0.6134   0.6264   0.0000   0.0000   - 0.5774  \u001b[0m\n",
      "Training 28     - 100    -            0.8780   0.0161   0.9154   0.9089   0.7675   0.7686   |            1.0003   0.8902   0.8887   0.7283   0.7296   - 20.8023 \n",
      "\u001b[1;4mValidati 28     - 100    -            0.8812   0.0161   0.5645   0.5776   0.0000   0.0000   |            1.0013   0.5879   0.5910   0.0000   0.0000   - 0.5746  \u001b[0m\n",
      "Training 29     - 100    -            0.8608   0.0163   0.9167   0.9136   0.7658   0.7697   |            0.9828   0.8801   0.8784   0.7264   0.7267   - 20.8066 \n",
      "\u001b[1;4mValidati 29     - 100    -            0.8635   0.0163   0.5886   0.5793   0.0000   0.0000   |            0.9836   0.6297   0.6305   0.0000   0.0000   - 0.5750  \u001b[0m\n",
      "Training 30     - 100    -            0.8448   0.0164   0.9179   0.9190   0.7640   0.7669   |            0.9672   0.8725   0.8701   0.7313   0.7333   - 20.7379 \n",
      "\u001b[1;4mValidati 30     - 100    -            0.8467   0.0164   0.6364   0.6419   0.0000   0.0000   |            0.9680   0.6364   0.6284   0.0000   0.0000   - 0.5930  \u001b[0m\n",
      "Training 31     - 100    -            0.8283   0.0166   0.9331   0.9302   0.7581   0.7611   |            0.9518   0.8864   0.8889   0.7384   0.7424   - 20.8563 \n",
      "\u001b[1;4mValidati 31     - 100    -            0.8302   0.0166   0.6386   0.6413   0.0000   0.0000   |            0.9528   0.6475   0.6446   0.0000   0.0000   - 0.5782  \u001b[0m\n",
      "Training 32     - 100    -            0.8118   0.0167   0.9470   0.9424   0.7813   0.7835   |            0.9367   0.8914   0.8907   0.7277   0.7291   - 20.8233 \n",
      "\u001b[1;4mValidati 32     - 100    -            0.8137   0.0166   0.6420   0.6510   0.0000   0.0000   |            0.9376   0.6364   0.6394   0.0000   0.0000   - 0.6171  \u001b[0m\n",
      "Training 33     - 100    -            0.7950   0.0167   0.9596   0.9587   0.7854   0.7892   |            0.9222   0.9066   0.9097   0.7431   0.7470   - 20.9446 \n",
      "\u001b[1;4mValidati 33     - 100    -            0.7973   0.0167   0.6520   0.6486   0.0000   0.0000   |            0.9231   0.6297   0.6321   0.0000   0.0000   - 0.5802  \u001b[0m\n",
      "Training 34     - 100    -            0.7793   0.0167   0.9659   0.9613   0.7864   0.7874   |            0.9065   0.9104   0.9068   0.7309   0.7340   - 20.9514 \n",
      "\u001b[1;4mValidati 34     - 100    -            0.7823   0.0168   0.5641   0.5582   0.0000   0.0000   |            0.9078   0.6509   0.6536   0.0000   0.0000   - 0.5838  \u001b[0m\n",
      "Training 35     - 100    -            0.7672   0.0170   0.9318   0.9331   0.7686   0.7719   |            0.8939   0.9078   0.9060   0.7242   0.7265   - 20.7989 \n",
      "\u001b[1;4mValidati 35     - 100    -            0.7697   0.0170   0.5808   0.5895   0.0000   0.0000   |            0.8949   0.6542   0.6538   0.0000   0.0000   - 0.5754  \u001b[0m\n",
      "Training 36     - 100    -            0.7536   0.0171   0.9545   0.9547   0.7687   0.7715   |            0.8797   0.9192   0.9140   0.7433   0.7451   - 20.7545 \n",
      "\u001b[1;4mValidati 36     - 100    -            0.7567   0.0171   0.5897   0.5890   0.0000   0.0000   |            0.8808   0.6464   0.6455   0.0000   0.0000   - 0.6198  \u001b[0m\n",
      "Training 37     - 100    -            0.7407   0.0172   0.9697   0.9682   0.7871   0.7909   |            0.8695   0.8952   0.8915   0.7442   0.7485   - 20.8824 \n",
      "\u001b[1;4mValidati 37     - 100    -            0.7431   0.0172   0.6299   0.6332   0.0000   0.0000   |            0.8709   0.6254   0.6270   0.0000   0.0000   - 0.5813  \u001b[0m\n",
      "Training 38     - 100    -            0.7288   0.0172   0.9545   0.9512   0.7720   0.7756   |            0.8586   0.9078   0.9101   0.7576   0.7594   - 20.9845 \n",
      "\u001b[1;4mValidati 38     - 100    -            0.7316   0.0172   0.5911   0.5948   0.0000   0.0000   |            0.8599   0.6290   0.6402   0.0000   0.0000   - 0.5635  \u001b[0m\n",
      "Training 39     - 100    -            0.7198   0.0173   0.9280   0.9257   0.7655   0.7668   |            0.8467   0.9167   0.9144   0.7533   0.7551   - 20.9109 \n",
      "\u001b[1;4mValidati 39     - 100    -            0.7227   0.0173   0.6000   0.5968   0.0000   0.0000   |            0.8480   0.6377   0.6437   0.0000   0.0000   - 0.5953  \u001b[0m\n",
      "Training 40     - 100    -            0.7107   0.0175   0.9407   0.9386   0.7670   0.7687   |            0.8369   0.9015   0.9024   0.7411   0.7441   - 20.8640 \n",
      "\u001b[1;4mValidati 40     - 100    -            0.7140   0.0175   0.5554   0.5587   0.0000   0.0000   |            0.8383   0.6400   0.6478   0.0000   0.0000   - 0.5788  \u001b[0m\n",
      "Training 41     - 100    -            0.7016   0.0175   0.9495   0.9493   0.7745   0.7768   |            0.8268   0.9205   0.9203   0.7562   0.7595   - 20.8439 \n",
      "\u001b[1;4mValidati 41     - 100    -            0.7034   0.0175   0.6078   0.6148   0.0000   0.0000   |            0.8281   0.6377   0.6385   0.0000   0.0000   - 0.5764  \u001b[0m\n",
      "Training 42     - 100    -            0.6908   0.0176   0.9545   0.9525   0.7792   0.7816   |            0.8166   0.9192   0.9188   0.7451   0.7488   - 20.8451 \n",
      "\u001b[1;4mValidati 42     - 100    -            0.6929   0.0176   0.6277   0.6326   0.0000   0.0000   |            0.8178   0.6520   0.6640   0.0000   0.0000   - 0.5890  \u001b[0m\n",
      "Training 43     - 100    -            0.6799   0.0177   0.9773   0.9759   0.7937   0.7963   |            0.8079   0.9179   0.9166   0.7491   0.7536   - 20.9243 \n",
      "\u001b[1;4mValidati 43     - 100    -            0.6819   0.0177   0.6134   0.6199   0.0000   0.0000   |            0.8095   0.6234   0.6353   0.0000   0.0000   - 0.6198  \u001b[0m\n",
      "Training 44     - 100    -            0.6693   0.0177   0.9811   0.9806   0.8010   0.8031   |            0.7997   0.9078   0.9143   0.7425   0.7474   - 21.1436 \n",
      "\u001b[1;4mValidati 44     - 100    -            0.6713   0.0177   0.6263   0.6226   0.0000   0.0000   |            0.8009   0.6400   0.6400   0.0000   0.0000   - 0.5875  \u001b[0m\n",
      "Training 45     - 100    -            0.6594   0.0177   0.9811   0.9782   0.7890   0.7933   |            0.7915   0.9217   0.9242   0.7696   0.7710   - 21.2131 \n",
      "\u001b[1;4mValidati 45     - 100    -            0.6628   0.0177   0.5866   0.5734   0.0000   0.0000   |            0.7929   0.6279   0.6341   0.0000   0.0000   - 0.6039  \u001b[0m\n",
      "Training 46     - 100    -            0.6511   0.0178   0.9785   0.9797   0.7889   0.7895   |            0.7833   0.9268   0.9260   0.7367   0.7393   - 21.1030 \n",
      "\u001b[1;4mValidati 46     - 100    -            0.6534   0.0178   0.6031   0.6016   0.0000   0.0000   |            0.7846   0.6344   0.6445   0.0000   0.0000   - 0.5765  \u001b[0m\n",
      "Training 47     - 100    -            0.6419   0.0179   0.9861   0.9850   0.8007   0.8010   |            0.7758   0.9192   0.9208   0.7524   0.7550   - 20.8268 \n",
      "\u001b[1;4mValidati 47     - 100    -            0.6442   0.0179   0.6513   0.6551   0.0000   0.0000   |            0.7771   0.6621   0.6526   0.0000   0.0000   - 0.6056  \u001b[0m\n",
      "Training 48     - 100    -            0.6337   0.0179   0.9735   0.9735   0.7808   0.7832   |            0.7697   0.9078   0.9078   0.7467   0.7521   - 20.8224 \n",
      "\u001b[1;4mValidati 48     - 100    -            0.6357   0.0179   0.6232   0.6311   0.0000   0.0000   |            0.7711   0.6335   0.6453   0.0000   0.0000   - 0.5784  \u001b[0m\n",
      "Training 49     - 100    -            0.6249   0.0179   0.9874   0.9866   0.7986   0.8010   |            0.7634   0.9268   0.9298   0.7634   0.7666   - 20.9172 \n",
      "\u001b[1;4mValidati 49     - 100    -            0.6267   0.0179   0.6609   0.6573   0.0000   0.0000   |            0.7647   0.6643   0.6663   0.0000   0.0000   - 0.6034  \u001b[0m\n",
      "Training 50     - 100    -            0.6173   0.0180   0.9684   0.9676   0.7859   0.7883   |            0.7553   0.9381   0.9385   0.7473   0.7491   - 20.9027 \n",
      "\u001b[1;4mValidati 50     - 100    -            0.6194   0.0180   0.6261   0.6300   0.0000   0.0000   |            0.7565   0.6632   0.6722   0.0000   0.0000   - 0.5771  \u001b[0m\n",
      "Training 51     - 100    -            0.6108   0.0181   0.9558   0.9576   0.7628   0.7673   |            0.7511   0.8990   0.9017   0.7689   0.7716   - 20.9973 \n",
      "\u001b[1;4mValidati 51     - 100    -            0.6139   0.0181   0.6297   0.6283   0.0000   0.0000   |            0.7524   0.6643   0.6679   0.0000   0.0000   - 0.5784  \u001b[0m\n",
      "Training 52     - 100    -            0.6046   0.0181   0.9722   0.9721   0.7868   0.7885   |            0.7449   0.9318   0.9290   0.7525   0.7546   - 20.8521 \n",
      "\u001b[1;4mValidati 52     - 100    -            0.6064   0.0181   0.6353   0.6397   0.0000   0.0000   |            0.7461   0.6719   0.6703   0.0000   0.0000   - 0.5638  \u001b[0m\n",
      "Training 53     - 100    -            0.5984   0.0182   0.9646   0.9652   0.7820   0.7834   |            0.7378   0.9432   0.9401   0.7544   0.7561   - 20.9718 \n",
      "\u001b[1;4mValidati 53     - 100    -            0.6008   0.0182   0.6487   0.6442   0.0000   0.0000   |            0.7391   0.6489   0.6554   0.0000   0.0000   - 0.5768  \u001b[0m\n",
      "Training 54     - 100    -            0.5925   0.0182   0.9634   0.9665   0.7791   0.7837   |            0.7337   0.9167   0.9181   0.7625   0.7666   - 20.8915 \n",
      "\u001b[1;4mValidati 54     - 100    -            0.5946   0.0182   0.6319   0.6363   0.0000   0.0000   |            0.7350   0.6632   0.6654   0.0000   0.0000   - 0.5939  \u001b[0m\n",
      "Training 55     - 100    -            0.5861   0.0183   0.9735   0.9735   0.7803   0.7819   |            0.7264   0.9432   0.9398   0.7508   0.7518   - 20.8443 \n",
      "\u001b[1;4mValidati 55     - 100    -            0.5886   0.0183   0.5967   0.6031   0.0000   0.0000   |            0.7280   0.6489   0.6525   0.0000   0.0000   - 0.5807  \u001b[0m\n",
      "Training 56     - 100    -            0.5797   0.0183   0.9912   0.9907   0.7981   0.8020   |            0.7193   0.9508   0.9505   0.7487   0.7518   - 20.8727 \n",
      "\u001b[1;4mValidati 56     - 100    -            0.5821   0.0183   0.6007   0.5905   0.0000   0.0000   |            0.7209   0.6257   0.6260   0.0000   0.0000   - 0.6089  \u001b[0m\n",
      "Training 57     - 100    -            0.5736   0.0183   0.9848   0.9859   0.7908   0.7944   |            0.7141   0.9242   0.9213   0.7527   0.7560   - 21.1002 \n",
      "\u001b[1;4mValidati 57     - 100    -            0.5756   0.0183   0.6250   0.6280   0.0000   0.0000   |            0.7155   0.6685   0.6654   0.0000   0.0000   - 0.5842  \u001b[0m\n",
      "Training 58     - 100    -            0.5676   0.0184   0.9823   0.9801   0.7955   0.7973   |            0.7087   0.9280   0.9306   0.7457   0.7484   - 20.7905 \n",
      "\u001b[1;4mValidati 58     - 100    -            0.5702   0.0184   0.6337   0.6263   0.0000   0.0000   |            0.7106   0.6279   0.6357   0.0000   0.0000   - 0.5765  \u001b[0m\n",
      "Training 59     - 100    -            0.5626   0.0184   0.9697   0.9724   0.7907   0.7941   |            0.7028   0.9495   0.9509   0.7593   0.7612   - 20.8417 \n",
      "\u001b[1;4mValidati 59     - 100    -            0.5648   0.0184   0.6154   0.6161   0.0000   0.0000   |            0.7044   0.6310   0.6366   0.0000   0.0000   - 0.5783  \u001b[0m\n",
      "Training 60     - 100    -            0.5570   0.0184   0.9874   0.9857   0.7906   0.7937   |            0.7001   0.9141   0.9183   0.7450   0.7473   - 21.1589 \n",
      "\u001b[1;4mValidati 60     - 100    -            0.5588   0.0184   0.6431   0.6522   0.0000   0.0000   |            0.7017   0.6388   0.6500   0.0000   0.0000   - 0.5641  \u001b[0m\n",
      "Training 61     - 100    -            0.5510   0.0184   0.9924   0.9914   0.8067   0.8107   |            0.6958   0.9369   0.9362   0.7545   0.7558   - 20.9584 \n",
      "\u001b[1;4mValidati 61     - 100    -            0.5529   0.0184   0.6333   0.6361   0.0000   0.0000   |            0.6972   0.6478   0.6487   0.0000   0.0000   - 0.5888  \u001b[0m\n",
      "Training 62     - 100    -            0.5458   0.0185   0.9785   0.9757   0.7904   0.7913   |            0.6920   0.9242   0.9230   0.7586   0.7614   - 21.1138 \n",
      "\u001b[1;4mValidati 62     - 100    -            0.5477   0.0185   0.6333   0.6353   0.0000   0.0000   |            0.6935   0.6643   0.6549   0.0000   0.0000   - 0.5889  \u001b[0m\n",
      "Training 63     - 100    -            0.5406   0.0185   0.9874   0.9871   0.7921   0.7942   |            0.6879   0.9318   0.9342   0.7588   0.7605   - 21.2037 \n",
      "\u001b[1;4mValidati 63     - 100    -            0.5425   0.0185   0.6330   0.6420   0.0000   0.0000   |            0.6894   0.6621   0.6654   0.0000   0.0000   - 0.5780  \u001b[0m\n",
      "Training 64     - 100    -            0.5354   0.0185   0.9848   0.9848   0.8013   0.8043   |            0.6847   0.9268   0.9271   0.7527   0.7539   - 20.8278 \n",
      "\u001b[1;4mValidati 64     - 100    -            0.5374   0.0185   0.6132   0.6140   0.0000   0.0000   |            0.6863   0.6411   0.6486   0.0000   0.0000   - 0.5772  \u001b[0m\n",
      "Training 65     - 100    -            0.5310   0.0186   0.9760   0.9754   0.7840   0.7878   |            0.6818   0.9242   0.9237   0.7361   0.7381   - 20.9442 \n",
      "\u001b[1;4mValidati 65     - 100    -            0.5329   0.0186   0.6565   0.6559   0.0000   0.0000   |            0.6833   0.6422   0.6466   0.0000   0.0000   - 0.5782  \u001b[0m\n",
      "Training 66     - 100    -            0.5261   0.0186   0.9899   0.9905   0.7900   0.7930   |            0.6798   0.9066   0.9078   0.7615   0.7653   - 21.0475 \n",
      "\u001b[1;4mValidati 66     - 100    -            0.5278   0.0186   0.6520   0.6578   0.0000   0.0000   |            0.6812   0.6391   0.6398   0.0000   0.0000   - 0.5918  \u001b[0m\n",
      "Training 67     - 100    -            0.5219   0.0186   0.9722   0.9719   0.7787   0.7821   |            0.6757   0.9419   0.9394   0.7516   0.7533   - 20.8811 \n",
      "\u001b[1;4mValidati 67     - 100    -            0.5237   0.0186   0.6324   0.6355   0.0000   0.0000   |            0.6772   0.6435   0.6438   0.0000   0.0000   - 0.5674  \u001b[0m\n",
      "Training 68     - 100    -            0.5175   0.0187   0.9861   0.9845   0.7947   0.7969   |            0.6726   0.9293   0.9288   0.7505   0.7526   - 21.1523 \n",
      "\u001b[1;4mValidati 68     - 100    -            0.5196   0.0187   0.6208   0.6275   0.0000   0.0000   |            0.6739   0.6699   0.6691   0.0000   0.0000   - 0.5787  \u001b[0m\n",
      "Training 69     - 100    -            0.5138   0.0187   0.9659   0.9667   0.7939   0.7964   |            0.6689   0.9306   0.9272   0.7541   0.7550   - 21.0136 \n",
      "\u001b[1;4mValidati 69     - 100    -            0.5157   0.0187   0.5944   0.5984   0.0000   0.0000   |            0.6705   0.6379   0.6485   0.0000   0.0000   - 0.5792  \u001b[0m\n",
      "Training 70     - 100    -            0.5102   0.0187   0.9735   0.9747   0.7883   0.7909   |            0.6656   0.9356   0.9369   0.7711   0.7735   - 20.8911 \n",
      "\u001b[1;4mValidati 70     - 100    -            0.5121   0.0187   0.6357   0.6402   0.0000   0.0000   |            0.6670   0.6667   0.6566   0.0000   0.0000   - 0.5964  \u001b[0m\n",
      "Training 71     - 100    -            0.5064   0.0187   0.9773   0.9778   0.7936   0.7980   |            0.6613   0.9520   0.9519   0.7495   0.7529   - 20.8658 \n",
      "\u001b[1;4mValidati 71     - 100    -            0.5082   0.0187   0.6232   0.6246   0.0000   0.0000   |            0.6627   0.6513   0.6515   0.0000   0.0000   - 0.5789  \u001b[0m\n",
      "Training 72     - 100    -            0.5022   0.0188   0.9912   0.9905   0.8074   0.8104   |            0.6588   0.9205   0.9218   0.7457   0.7486   - 21.4214 \n",
      "\u001b[1;4mValidati 72     - 100    -            0.5040   0.0187   0.6500   0.6481   0.0000   0.0000   |            0.6601   0.6522   0.6543   0.0000   0.0000   - 0.5776  \u001b[0m\n",
      "Training 73     - 100    -            0.4980   0.0188   0.9949   0.9943   0.8078   0.8108   |            0.6554   0.9331   0.9341   0.7425   0.7443   - 21.1905 \n",
      "\u001b[1;4mValidati 73     - 100    -            0.5001   0.0188   0.6134   0.6247   0.0000   0.0000   |            0.6571   0.6270   0.6322   0.0000   0.0000   - 0.5922  \u001b[0m\n",
      "Training 74     - 100    -            0.4942   0.0188   0.9975   0.9961   0.8009   0.8041   |            0.6525   0.9470   0.9469   0.7498   0.7518   - 20.8452 \n",
      "\u001b[1;4mValidati 74     - 100    -            0.4958   0.0188   0.6609   0.6665   0.0000   0.0000   |            0.6538   0.6665   0.6668   0.0000   0.0000   - 0.5778  \u001b[0m\n",
      "Training 75     - 100    -            0.4902   0.0188   0.9949   0.9955   0.7983   0.8006   |            0.6495   0.9255   0.9281   0.7513   0.7524   - 21.0533 \n",
      "\u001b[1;4mValidati 75     - 100    -            0.4922   0.0188   0.6286   0.6278   0.0000   0.0000   |            0.6508   0.6576   0.6617   0.0000   0.0000   - 0.5993  \u001b[0m\n",
      "Training 76     - 100    -            0.4865   0.0188   0.9975   0.9974   0.7993   0.8013   |            0.6473   0.9331   0.9326   0.7526   0.7541   - 21.0808 \n",
      "\u001b[1;4mValidati 76     - 100    -            0.4884   0.0188   0.6299   0.6333   0.0000   0.0000   |            0.6488   0.6313   0.6374   0.0000   0.0000   - 0.5783  \u001b[0m\n",
      "Training 77     - 100    -            0.4828   0.0188   1.0000   0.9993   0.8085   0.8118   |            0.6447   0.9444   0.9441   0.7367   0.7392   - 20.9464 \n",
      "\u001b[1;4mValidati 77     - 100    -            0.4842   0.0188   0.6587   0.6633   0.0000   0.0000   |            0.6460   0.6621   0.6657   0.0000   0.0000   - 0.6148  \u001b[0m\n",
      "Training 78     - 100    -            0.4786   0.0187   1.0000   1.0000   0.8174   0.8200   |            0.6435   0.9179   0.9165   0.7483   0.7488   - 21.0219 \n",
      "\u001b[1;4mValidati 78     - 100    -            0.4801   0.0187   0.6310   0.6357   0.0000   0.0000   |            0.6448   0.6500   0.6600   0.0000   0.0000   - 0.5783  \u001b[0m\n",
      "Training 79     - 100    -            0.4746   0.0187   1.0000   0.9993   0.8152   0.8177   |            0.6416   0.9343   0.9355   0.7441   0.7453   - 20.7711 \n",
      "\u001b[1;4mValidati 79     - 100    -            0.4763   0.0187   0.6254   0.6317   0.0000   0.0000   |            0.6429   0.6621   0.6607   0.0000   0.0000   - 0.5790  \u001b[0m\n",
      "Training 80     - 100    -            0.4710   0.0187   1.0000   1.0000   0.8165   0.8187   |            0.6385   0.9369   0.9395   0.7372   0.7392   - 20.8712 \n",
      "\u001b[1;4mValidati 80     - 100    -            0.4725   0.0187   0.6386   0.6405   0.0000   0.0000   |            0.6397   0.6400   0.6469   0.0000   0.0000   - 0.5783  \u001b[0m\n",
      "Training 81     - 100    -            0.4673   0.0187   1.0000   0.9993   0.8141   0.8179   |            0.6358   0.9369   0.9370   0.7525   0.7545   - 20.9326 \n",
      "\u001b[1;4mValidati 81     - 100    -            0.4692   0.0187   0.6268   0.6282   0.0000   0.0000   |            0.6372   0.6313   0.6309   0.0000   0.0000   - 0.5778  \u001b[0m\n",
      "Training 82     - 100    -            0.4641   0.0187   0.9987   0.9993   0.8163   0.8189   |            0.6331   0.9470   0.9483   0.7302   0.7309   - 20.9271 \n",
      "\u001b[1;4mValidati 82     - 100    -            0.4657   0.0187   0.6612   0.6580   0.0000   0.0000   |            0.6345   0.6368   0.6394   0.0000   0.0000   - 0.5854  \u001b[0m\n",
      "Training 83     - 100    -            0.4607   0.0187   1.0000   1.0000   0.8201   0.8227   |            0.6322   0.9078   0.9080   0.7613   0.7639   - 20.9705 \n",
      "\u001b[1;4mValidati 83     - 100    -            0.4622   0.0186   0.6489   0.6496   0.0000   0.0000   |            0.6335   0.6522   0.6586   0.0000   0.0000   - 0.5778  \u001b[0m\n",
      "Training 84     - 100    -            0.4573   0.0186   1.0000   1.0000   0.8206   0.8231   |            0.6304   0.9318   0.9300   0.7374   0.7387   - 21.1159 \n",
      "\u001b[1;4mValidati 84     - 100    -            0.4590   0.0186   0.6444   0.6453   0.0000   0.0000   |            0.6318   0.6391   0.6401   0.0000   0.0000   - 0.5936  \u001b[0m\n",
      "Training 85     - 100    -            0.4542   0.0187   1.0000   1.0000   0.8180   0.8197   |            0.6281   0.9356   0.9393   0.7259   0.7284   - 21.2219 \n",
      "\u001b[1;4mValidati 85     - 100    -            0.4560   0.0186   0.6147   0.6169   0.0000   0.0000   |            0.6296   0.6181   0.6205   0.0000   0.0000   - 0.5803  \u001b[0m\n",
      "Training 86     - 100    -            0.4513   0.0186   1.0000   1.0000   0.8150   0.8177   |            0.6266   0.9381   0.9413   0.7550   0.7553   - 21.2255 \n",
      "\u001b[1;4mValidati 86     - 100    -            0.4527   0.0186   0.6629   0.6677   0.0000   0.0000   |            0.6278   0.6598   0.6578   0.0000   0.0000   - 0.6062  \u001b[0m\n",
      "Training 87     - 100    -            0.4483   0.0186   0.9912   0.9917   0.8024   0.8050   |            0.6238   0.9457   0.9471   0.7426   0.7443   - 21.1621 \n",
      "\u001b[1;4mValidati 87     - 100    -            0.4499   0.0186   0.6234   0.6283   0.0000   0.0000   |            0.6252   0.6268   0.6320   0.0000   0.0000   - 0.6106  \u001b[0m\n",
      "Training 88     - 100    -            0.4464   0.0187   0.9621   0.9632   0.7736   0.7751   |            0.6226   0.9192   0.9224   0.7396   0.7418   - 20.8364 \n",
      "\u001b[1;4mValidati 88     - 100    -            0.4484   0.0187   0.5721   0.5727   0.0000   0.0000   |            0.6242   0.6268   0.6171   0.0000   0.0000   - 0.6145  \u001b[0m\n",
      "Training 89     - 100    -            0.4442   0.0188   0.9899   0.9916   0.7935   0.7953   |            0.6208   0.9343   0.9368   0.7611   0.7631   - 21.0866 \n",
      "\u001b[1;4mValidati 89     - 100    -            0.4458   0.0188   0.6420   0.6428   0.0000   0.0000   |            0.6220   0.6621   0.6650   0.0000   0.0000   - 0.5936  \u001b[0m\n",
      "Training 90     - 100    -            0.4416   0.0188   0.9924   0.9917   0.8024   0.8051   |            0.6208   0.9053   0.9048   0.7401   0.7418   - 20.9520 \n",
      "\u001b[1;4mValidati 90     - 100    -            0.4431   0.0188   0.6308   0.6359   0.0000   0.0000   |            0.6220   0.6576   0.6562   0.0000   0.0000   - 0.6223  \u001b[0m\n",
      "Training 91     - 100    -            0.4390   0.0188   0.9912   0.9915   0.7958   0.7993   |            0.6185   0.9457   0.9448   0.7547   0.7559   - 21.0064 \n",
      "\u001b[1;4mValidati 91     - 100    -            0.4410   0.0188   0.6183   0.6205   0.0000   0.0000   |            0.6197   0.6411   0.6444   0.0000   0.0000   - 0.6168  \u001b[0m\n",
      "Training 92     - 100    -            0.4375   0.0189   0.9747   0.9747   0.7870   0.7899   |            0.6168   0.9419   0.9433   0.7485   0.7525   - 20.9490 \n",
      "\u001b[1;4mValidati 92     - 100    -            0.4391   0.0189   0.6107   0.6152   0.0000   0.0000   |            0.6180   0.6531   0.6486   0.0000   0.0000   - 0.6225  \u001b[0m\n",
      "Training 93     - 100    -            0.4352   0.0189   0.9874   0.9878   0.7980   0.8014   |            0.6153   0.9381   0.9387   0.7540   0.7570   - 21.0902 \n",
      "\u001b[1;4mValidati 93     - 100    -            0.4370   0.0189   0.5766   0.5804   0.0000   0.0000   |            0.6165   0.6123   0.6136   0.0000   0.0000   - 0.5788  \u001b[0m\n",
      "Training 94     - 100    -            0.4331   0.0189   0.9912   0.9922   0.8079   0.8110   |            0.6135   0.9369   0.9365   0.7374   0.7393   - 21.1619 \n",
      "\u001b[1;4mValidati 94     - 100    -            0.4348   0.0189   0.6031   0.6040   0.0000   0.0000   |            0.6149   0.6333   0.6320   0.0000   0.0000   - 0.5817  \u001b[0m\n",
      "Training 95     - 100    -            0.4308   0.0189   0.9975   0.9975   0.8125   0.8163   |            0.6133   0.9306   0.9317   0.7490   0.7511   - 20.8094 \n",
      "\u001b[1;4mValidati 95     - 100    -            0.4329   0.0189   0.5612   0.5622   0.0000   0.0000   |            0.6147   0.5991   0.5913   0.0000   0.0000   - 0.5798  \u001b[0m\n",
      "Training 96     - 100    -            0.4290   0.0189   0.9937   0.9922   0.7963   0.7993   |            0.6115   0.9432   0.9475   0.7320   0.7333   - 20.8810 \n",
      "\u001b[1;4mValidati 96     - 100    -            0.4308   0.0189   0.6031   0.6030   0.0000   0.0000   |            0.6128   0.6375   0.6468   0.0000   0.0000   - 0.5794  \u001b[0m\n",
      "Training 97     - 100    -            0.4271   0.0190   0.9886   0.9879   0.7935   0.7955   |            0.6109   0.9343   0.9339   0.7473   0.7494   - 20.8734 \n",
      "\u001b[1;4mValidati 97     - 100    -            0.4289   0.0190   0.6042   0.6014   0.0000   0.0000   |            0.6123   0.6266   0.6303   0.0000   0.0000   - 0.5816  \u001b[0m\n",
      "Training 98     - 100    -            0.4252   0.0190   0.9912   0.9912   0.7931   0.7952   |            0.6103   0.9343   0.9364   0.7584   0.7601   - 20.8903 \n",
      "\u001b[1;4mValidati 98     - 100    -            0.4269   0.0190   0.6199   0.6232   0.0000   0.0000   |            0.6117   0.5893   0.5949   0.0000   0.0000   - 0.5790  \u001b[0m\n",
      "Training 99     - 100    -            0.4232   0.0190   0.9899   0.9897   0.8032   0.8061   |            0.6085   0.9470   0.9494   0.7442   0.7468   - 20.9581 \n",
      "\u001b[1;4mValidati 99     - 100    -            0.4249   0.0190   0.6319   0.6318   0.0000   0.0000   |            0.6097   0.6431   0.6436   0.0000   0.0000   - 0.5893  \u001b[0m\n",
      "Training 100    - 100    -            0.4216   0.0190   0.9836   0.9829   0.8020   0.8050   |            0.6078   0.9280   0.9281   0.7644   0.7633   - 20.8542 \n",
      "\u001b[1;4mValidati 100    - 100    -            0.4231   0.0190   0.6098   0.6132   0.0000   0.0000   |            0.6091   0.6277   0.6382   0.0000   0.0000   - 0.5846  \u001b[0m\n",
      "Training 101    - 100    -            0.4197   0.0190   0.9899   0.9910   0.8010   0.8037   |            0.6062   0.9318   0.9328   0.7404   0.7437   - 20.8859 \n",
      "\u001b[1;4mValidati 101    - 100    -            0.4209   0.0190   0.6665   0.6678   0.0000   0.0000   |            0.6075   0.6254   0.6321   0.0000   0.0000   - 0.5648  \u001b[0m\n",
      "Training 102    - 100    -            0.4173   0.0190   0.9962   0.9955   0.8084   0.8119   |            0.6050   0.9318   0.9325   0.7563   0.7587   - 21.3937 \n",
      "\u001b[1;4mValidati 102    - 100    -            0.4188   0.0190   0.6313   0.6437   0.0000   0.0000   |            0.6063   0.6366   0.6411   0.0000   0.0000   - 0.5785  \u001b[0m\n",
      "Training 103    - 100    -            0.4152   0.0190   0.9987   0.9980   0.8121   0.8138   |            0.6035   0.9343   0.9385   0.7728   0.7757   - 21.0977 \n",
      "\u001b[1;4mValidati 103    - 100    -            0.4167   0.0190   0.6522   0.6563   0.0000   0.0000   |            0.6048   0.6377   0.6436   0.0000   0.0000   - 0.6031  \u001b[0m\n",
      "Training 104    - 100    -            0.4132   0.0190   0.9962   0.9955   0.8144   0.8163   |            0.6027   0.9230   0.9240   0.7627   0.7661   - 20.9152 \n",
      "\u001b[1;4mValidati 104    - 100    -            0.4147   0.0189   0.6386   0.6429   0.0000   0.0000   |            0.6039   0.6520   0.6589   0.0000   0.0000   - 0.5981  \u001b[0m\n",
      "Training 105    - 100    -            0.4113   0.0189   0.9949   0.9949   0.8110   0.8137   |            0.6019   0.9129   0.9133   0.7533   0.7559   - 20.8132 \n",
      "\u001b[1;4mValidati 105    - 100    -            0.4128   0.0189   0.6266   0.6292   0.0000   0.0000   |            0.6032   0.6290   0.6306   0.0000   0.0000   - 0.5798  \u001b[0m\n",
      "Training 106    - 100    -            0.4093   0.0189   0.9987   0.9987   0.8166   0.8182   |            0.6004   0.9331   0.9323   0.7485   0.7515   - 20.9718 \n",
      "\u001b[1;4mValidati 106    - 100    -            0.4112   0.0189   0.5980   0.6096   0.0000   0.0000   |            0.6019   0.6036   0.6077   0.0000   0.0000   - 0.5805  \u001b[0m\n",
      "Training 107    - 100    -            0.4078   0.0189   0.9975   0.9974   0.8200   0.8228   |            0.6002   0.9230   0.9249   0.7396   0.7420   - 21.0470 \n",
      "\u001b[1;4mValidati 107    - 100    -            0.4095   0.0189   0.6089   0.6138   0.0000   0.0000   |            0.6016   0.6313   0.6361   0.0000   0.0000   - 0.6013  \u001b[0m\n",
      "Training 108    - 100    -            0.4061   0.0189   1.0000   1.0000   0.8251   0.8272   |            0.6001   0.9179   0.9197   0.7669   0.7691   - 20.9470 \n",
      "\u001b[1;4mValidati 108    - 100    -            0.4076   0.0189   0.6246   0.6268   0.0000   0.0000   |            0.6013   0.6212   0.6280   0.0000   0.0000   - 0.5798  \u001b[0m\n",
      "Training 109    - 100    -            0.4042   0.0189   1.0000   1.0000   0.8241   0.8263   |            0.5992   0.9343   0.9347   0.7440   0.7470   - 20.9087 \n",
      "\u001b[1;4mValidati 109    - 100    -            0.4059   0.0189   0.6112   0.6132   0.0000   0.0000   |            0.6006   0.6223   0.6295   0.0000   0.0000   - 0.6196  \u001b[0m\n",
      "Training 110    - 100    -            0.4026   0.0188   1.0000   1.0000   0.8263   0.8289   |            0.5986   0.9369   0.9351   0.7680   0.7711   - 20.9208 \n",
      "\u001b[1;4mValidati 110    - 100    -            0.4042   0.0188   0.6145   0.6250   0.0000   0.0000   |            0.6000   0.6167   0.6245   0.0000   0.0000   - 0.5830  \u001b[0m\n",
      "Training 111    - 100    -            0.4010   0.0188   0.9975   0.9980   0.8188   0.8215   |            0.5969   0.9533   0.9533   0.7518   0.7545   - 21.1087 \n",
      "\u001b[1;4mValidati 111    - 100    -            0.4025   0.0188   0.6498   0.6494   0.0000   0.0000   |            0.5981   0.6442   0.6522   0.0000   0.0000   - 0.5795  \u001b[0m\n",
      "Training 112    - 100    -            0.3994   0.0188   0.9975   0.9960   0.8089   0.8122   |            0.5956   0.9482   0.9498   0.7428   0.7465   - 20.8643 \n",
      "\u001b[1;4mValidati 112    - 100    -            0.4007   0.0188   0.6268   0.6313   0.0000   0.0000   |            0.5968   0.6388   0.6408   0.0000   0.0000   - 0.5798  \u001b[0m\n",
      "Training 113    - 100    -            0.3976   0.0188   0.9949   0.9958   0.8120   0.8153   |            0.5940   0.9457   0.9478   0.7399   0.7405   - 20.8684 \n",
      "\u001b[1;4mValidati 113    - 100    -            0.3993   0.0188   0.6009   0.5989   0.0000   0.0000   |            0.5953   0.6366   0.6398   0.0000   0.0000   - 0.5803  \u001b[0m\n",
      "Training 114    - 100    -            0.3962   0.0188   1.0000   1.0000   0.8178   0.8203   |            0.5925   0.9470   0.9449   0.7420   0.7424   - 20.9499 \n",
      "\u001b[1;4mValidati 114    - 100    -            0.3977   0.0188   0.6239   0.6246   0.0000   0.0000   |            0.5936   0.6629   0.6637   0.0000   0.0000   - 0.5924  \u001b[0m\n",
      "Training 115    - 100    -            0.3946   0.0188   1.0000   1.0000   0.8205   0.8220   |            0.5920   0.9343   0.9370   0.7536   0.7565   - 20.8756 \n",
      "\u001b[1;4mValidati 115    - 100    -            0.3964   0.0188   0.5955   0.5987   0.0000   0.0000   |            0.5934   0.6167   0.6182   0.0000   0.0000   - 0.5858  \u001b[0m\n",
      "Training 116    - 100    -            0.3933   0.0187   1.0000   1.0000   0.8199   0.8220   |            0.5918   0.9255   0.9269   0.7580   0.7594   - 20.9509 \n",
      "\u001b[1;4mValidati 116    - 100    -            0.3950   0.0187   0.6199   0.6212   0.0000   0.0000   |            0.5931   0.6067   0.6115   0.0000   0.0000   - 0.5799  \u001b[0m\n",
      "Training 117    - 100    -            0.3920   0.0187   1.0000   1.0000   0.8239   0.8257   |            0.5905   0.9457   0.9477   0.7652   0.7683   - 21.0944 \n",
      "\u001b[1;4mValidati 117    - 100    -            0.3935   0.0187   0.6212   0.6210   0.0000   0.0000   |            0.5918   0.5913   0.5967   0.0000   0.0000   - 0.6011  \u001b[0m\n",
      "Training 118    - 100    -            0.3906   0.0187   0.9962   0.9962   0.8153   0.8183   |            0.5898   0.9369   0.9365   0.7667   0.7696   - 20.8007 \n",
      "\u001b[1;4mValidati 118    - 100    -            0.3921   0.0187   0.6210   0.6312   0.0000   0.0000   |            0.5910   0.6243   0.6310   0.0000   0.0000   - 0.5770  \u001b[0m\n",
      "Training 119    - 100    -            0.3891   0.0186   1.0000   1.0000   0.8233   0.8261   |            0.5887   0.9407   0.9390   0.7721   0.7752   - 21.0181 \n",
      "\u001b[1;4mValidati 119    - 100    -            0.3906   0.0186   0.6308   0.6390   0.0000   0.0000   |            0.5899   0.6364   0.6425   0.0000   0.0000   - 0.6097  \u001b[0m\n",
      "Training 120    - 100    -            0.3877   0.0186   1.0000   1.0000   0.8258   0.8287   |            0.5889   0.9179   0.9162   0.7634   0.7650   - 21.0017 \n",
      "\u001b[1;4mValidati 120    - 100    -            0.3891   0.0186   0.6252   0.6295   0.0000   0.0000   |            0.5901   0.6297   0.6322   0.0000   0.0000   - 0.5912  \u001b[0m\n",
      "Training 121    - 100    -            0.3862   0.0186   1.0000   1.0000   0.8301   0.8316   |            0.5886   0.9179   0.9218   0.7643   0.7660   - 21.1423 \n",
      "\u001b[1;4mValidati 121    - 100    -            0.3879   0.0185   0.6056   0.6061   0.0000   0.0000   |            0.5901   0.6000   0.6051   0.0000   0.0000   - 0.5793  \u001b[0m\n",
      "Training 122    - 100    -            0.3851   0.0185   1.0000   0.9993   0.8224   0.8245   |            0.5886   0.9205   0.9203   0.7440   0.7465   - 20.8683 \n",
      "\u001b[1;4mValidati 122    - 100    -            0.3864   0.0185   0.6252   0.6328   0.0000   0.0000   |            0.5897   0.6330   0.6390   0.0000   0.0000   - 0.5912  \u001b[0m\n",
      "Training 123    - 100    -            0.3836   0.0185   0.9987   0.9987   0.8189   0.8230   |            0.5878   0.9394   0.9421   0.7561   0.7576   - 20.8886 \n",
      "\u001b[1;4mValidati 123    - 100    -            0.3850   0.0185   0.6132   0.6183   0.0000   0.0000   |            0.5890   0.6199   0.6215   0.0000   0.0000   - 0.6235  \u001b[0m\n",
      "Training 124    - 100    -            0.3822   0.0185   1.0000   1.0000   0.8235   0.8258   |            0.5872   0.9242   0.9261   0.7534   0.7560   - 21.0312 \n",
      "\u001b[1;4mValidati 124    - 100    -            0.3836   0.0185   0.6330   0.6368   0.0000   0.0000   |            0.5885   0.6263   0.6292   0.0000   0.0000   - 0.5797  \u001b[0m\n",
      "Training 125    - 100    -            0.3808   0.0185   1.0000   1.0000   0.8279   0.8303   |            0.5869   0.9179   0.9214   0.7528   0.7554   - 21.0525 \n",
      "\u001b[1;4mValidati 125    - 100    -            0.3823   0.0185   0.6078   0.6094   0.0000   0.0000   |            0.5882   0.6078   0.6129   0.0000   0.0000   - 0.5805  \u001b[0m\n",
      "Training 126    - 100    -            0.3795   0.0185   1.0000   1.0000   0.8229   0.8268   |            0.5868   0.9230   0.9227   0.7424   0.7449   - 21.0360 \n",
      "\u001b[1;4mValidati 126    - 100    -            0.3809   0.0185   0.6056   0.6170   0.0000   0.0000   |            0.5880   0.6232   0.6258   0.0000   0.0000   - 0.5826  \u001b[0m\n",
      "Training 127    - 100    -            0.3782   0.0184   1.0000   1.0000   0.8273   0.8293   |            0.5863   0.9280   0.9279   0.7598   0.7633   - 20.9714 \n",
      "\u001b[1;4mValidati 127    - 100    -            0.3795   0.0184   0.6089   0.6128   0.0000   0.0000   |            0.5874   0.6551   0.6512   0.0000   0.0000   - 0.5688  \u001b[0m\n",
      "Training 128    - 100    -            0.3768   0.0184   1.0000   1.0000   0.8241   0.8270   |            0.5857   0.9306   0.9305   0.7508   0.7523   - 20.8820 \n",
      "\u001b[1;4mValidati 128    - 100    -            0.3781   0.0184   0.6176   0.6210   0.0000   0.0000   |            0.5868   0.6221   0.6267   0.0000   0.0000   - 0.5803  \u001b[0m\n",
      "Training 129    - 100    -            0.3755   0.0184   1.0000   1.0000   0.8247   0.8271   |            0.5854   0.9255   0.9258   0.7619   0.7642   - 20.8919 \n",
      "\u001b[1;4mValidati 129    - 100    -            0.3770   0.0183   0.5913   0.5962   0.0000   0.0000   |            0.5866   0.6078   0.6091   0.0000   0.0000   - 0.5970  \u001b[0m\n",
      "Training 130    - 100    -            0.3744   0.0183   1.0000   1.0000   0.8247   0.8271   |            0.5861   0.9091   0.9081   0.7422   0.7429   - 21.0570 \n",
      "\u001b[1;4mValidati 130    - 100    -            0.3756   0.0183   0.6210   0.6224   0.0000   0.0000   |            0.5873   0.6067   0.5988   0.0000   0.0000   - 0.5804  \u001b[0m\n",
      "Training 131    - 100    -            0.3731   0.0183   1.0000   1.0000   0.8240   0.8266   |            0.5855   0.9293   0.9303   0.7523   0.7527   - 20.9533 \n",
      "\u001b[1;4mValidati 131    - 100    -            0.3743   0.0183   0.6286   0.6335   0.0000   0.0000   |            0.5867   0.6221   0.6247   0.0000   0.0000   - 0.5804  \u001b[0m\n",
      "Training 132    - 100    -            0.3717   0.0183   1.0000   1.0000   0.8200   0.8219   |            0.5852   0.9381   0.9372   0.7210   0.7228   - 21.0742 \n",
      "\u001b[1;4mValidati 132    - 100    -            0.3729   0.0183   0.6629   0.6693   0.0000   0.0000   |            0.5863   0.6420   0.6416   0.0000   0.0000   - 0.5805  \u001b[0m\n",
      "Training 133    - 100    -            0.3704   0.0183   1.0000   1.0000   0.8217   0.8251   |            0.5842   0.9444   0.9455   0.7460   0.7485   - 20.8818 \n",
      "\u001b[1;4mValidati 133    - 100    -            0.3716   0.0183   0.6442   0.6470   0.0000   0.0000   |            0.5852   0.6353   0.6422   0.0000   0.0000   - 0.5882  \u001b[0m\n",
      "Training 134    - 100    -            0.3691   0.0183   1.0000   1.0000   0.8189   0.8204   |            0.5843   0.9293   0.9281   0.7537   0.7563   - 21.0255 \n",
      "\u001b[1;4mValidati 134    - 100    -            0.3704   0.0183   0.6288   0.6299   0.0000   0.0000   |            0.5854   0.6297   0.6368   0.0000   0.0000   - 0.5968  \u001b[0m\n",
      "Training 135    - 100    -            0.3679   0.0183   1.0000   1.0000   0.8158   0.8182   |            0.5836   0.9444   0.9449   0.7391   0.7402   - 20.9499 \n",
      "\u001b[1;4mValidati 135    - 100    -            0.3693   0.0183   0.6299   0.6313   0.0000   0.0000   |            0.5847   0.6154   0.6246   0.0000   0.0000   - 0.6077  \u001b[0m\n",
      "Training 136    - 100    -            0.3669   0.0183   0.9937   0.9941   0.8142   0.8153   |            0.5837   0.9217   0.9229   0.7551   0.7558   - 20.9177 \n",
      "\u001b[1;4mValidati 136    - 100    -            0.3686   0.0183   0.6065   0.6106   0.0000   0.0000   |            0.5849   0.6078   0.6140   0.0000   0.0000   - 0.5665  \u001b[0m\n",
      "Training 137    - 100    -            0.3662   0.0183   0.9962   0.9955   0.8053   0.8070   |            0.5830   0.9419   0.9413   0.7410   0.7418   - 20.9873 \n",
      "\u001b[1;4mValidati 137    - 100    -            0.3674   0.0183   0.6554   0.6564   0.0000   0.0000   |            0.5840   0.6442   0.6493   0.0000   0.0000   - 0.5803  \u001b[0m\n",
      "Training 138    - 100    -            0.3650   0.0183   0.9987   0.9987   0.8203   0.8229   |            0.5832   0.9217   0.9201   0.7480   0.7501   - 20.9094 \n",
      "\u001b[1;4mValidati 138    - 100    -            0.3661   0.0183   0.6600   0.6575   0.0000   0.0000   |            0.5842   0.6408   0.6400   0.0000   0.0000   - 0.5819  \u001b[0m\n",
      "Training 139    - 100    -            0.3638   0.0183   0.9987   0.9987   0.8141   0.8173   |            0.5825   0.9356   0.9373   0.7414   0.7428   - 21.2623 \n",
      "\u001b[1;4mValidati 139    - 100    -            0.3651   0.0183   0.6475   0.6476   0.0000   0.0000   |            0.5836   0.6464   0.6499   0.0000   0.0000   - 0.6037  \u001b[0m\n",
      "Training 140    - 100    -            0.3628   0.0183   1.0000   1.0000   0.8190   0.8223   |            0.5818   0.9444   0.9428   0.7564   0.7585   - 20.9521 \n",
      "\u001b[1;4mValidati 140    - 100    -            0.3637   0.0183   0.6808   0.6822   0.0000   0.0000   |            0.5827   0.6529   0.6551   0.0000   0.0000   - 0.5974  \u001b[0m\n",
      "Training 141    - 100    -            0.3614   0.0183   1.0000   1.0000   0.8201   0.8221   |            0.5814   0.9280   0.9303   0.7457   0.7469   - 21.3267 \n",
      "\u001b[1;4mValidati 141    - 100    -            0.3626   0.0183   0.6567   0.6595   0.0000   0.0000   |            0.5825   0.6321   0.6407   0.0000   0.0000   - 0.5816  \u001b[0m\n",
      "Training 142    - 100    -            0.3603   0.0183   1.0000   1.0000   0.8221   0.8238   |            0.5811   0.9432   0.9427   0.7470   0.7496   - 20.8980 \n",
      "\u001b[1;4mValidati 142    - 100    -            0.3613   0.0183   0.6752   0.6757   0.0000   0.0000   |            0.5819   0.6708   0.6700   0.0000   0.0000   - 0.5987  \u001b[0m\n",
      "Training 143    - 100    -            0.3591   0.0183   0.9962   0.9968   0.8180   0.8209   |            0.5799   0.9482   0.9486   0.7655   0.7681   - 20.9531 \n",
      "\u001b[1;4mValidati 143    - 100    -            0.3601   0.0183   0.6777   0.6811   0.0000   0.0000   |            0.5807   0.6607   0.6610   0.0000   0.0000   - 0.5810  \u001b[0m\n",
      "Training 144    - 100    -            0.3578   0.0183   0.9987   0.9987   0.8184   0.8215   |            0.5794   0.9356   0.9364   0.7358   0.7375   - 20.8588 \n",
      "\u001b[1;4mValidati 144    - 100    -            0.3588   0.0183   0.6821   0.6869   0.0000   0.0000   |            0.5802   0.6433   0.6588   0.0000   0.0000   - 0.5810  \u001b[0m\n",
      "Training 145    - 100    -            0.3566   0.0182   1.0000   1.0000   0.8209   0.8232   |            0.5782   0.9457   0.9452   0.7679   0.7694   - 21.0551 \n",
      "\u001b[1;4mValidati 145    - 100    -            0.3576   0.0182   0.6886   0.6956   0.0000   0.0000   |            0.5790   0.6652   0.6680   0.0000   0.0000   - 0.5961  \u001b[0m\n",
      "Training 146    - 100    -            0.3554   0.0182   1.0000   1.0000   0.8202   0.8232   |            0.5782   0.9192   0.9200   0.7573   0.7598   - 20.9518 \n",
      "\u001b[1;4mValidati 146    - 100    -            0.3565   0.0182   0.6368   0.6403   0.0000   0.0000   |            0.5791   0.6400   0.6504   0.0000   0.0000   - 0.5801  \u001b[0m\n",
      "Training 147    - 100    -            0.3543   0.0182   0.9975   0.9975   0.8213   0.8252   |            0.5775   0.9394   0.9395   0.7538   0.7553   - 20.9790 \n",
      "\u001b[1;4mValidati 147    - 100    -            0.3554   0.0182   0.6301   0.6370   0.0000   0.0000   |            0.5784   0.6576   0.6597   0.0000   0.0000   - 0.5819  \u001b[0m\n",
      "Training 148    - 100    -            0.3532   0.0182   1.0000   0.9993   0.8171   0.8175   |            0.5779   0.9230   0.9232   0.7567   0.7589   - 20.9537 \n",
      "\u001b[1;4mValidati 148    - 100    -            0.3542   0.0182   0.6475   0.6527   0.0000   0.0000   |            0.5787   0.6797   0.6828   0.0000   0.0000   - 0.6096  \u001b[0m\n",
      "Training 149    - 100    -            0.3521   0.0182   0.9987   0.9987   0.8174   0.8205   |            0.5773   0.9419   0.9426   0.7435   0.7451   - 21.1723 \n",
      "\u001b[1;4mValidati 149    - 100    -            0.3532   0.0182   0.6487   0.6521   0.0000   0.0000   |            0.5781   0.6366   0.6439   0.0000   0.0000   - 0.5972  \u001b[0m\n",
      "Training 150    - 100    -            0.3510   0.0181   1.0000   1.0000   0.8171   0.8205   |            0.5776   0.9179   0.9180   0.7598   0.7625   - 20.9690 \n",
      "\u001b[1;4mValidati 150    - 100    -            0.3521   0.0181   0.6234   0.6300   0.0000   0.0000   |            0.5785   0.6554   0.6592   0.0000   0.0000   - 0.5850  \u001b[0m\n",
      "Training 151    - 100    -            0.3500   0.0181   1.0000   1.0000   0.8202   0.8227   |            0.5775   0.9242   0.9219   0.7612   0.7647   - 21.0507 \n",
      "\u001b[1;4mValidati 151    - 100    -            0.3511   0.0181   0.6377   0.6440   0.0000   0.0000   |            0.5784   0.6411   0.6403   0.0000   0.0000   - 0.5986  \u001b[0m\n",
      "Training 152    - 100    -            0.3490   0.0181   1.0000   1.0000   0.8225   0.8244   |            0.5774   0.9268   0.9289   0.7598   0.7629   - 20.9857 \n",
      "\u001b[1;4mValidati 152    - 100    -            0.3501   0.0181   0.6453   0.6140   0.0000   0.0000   |            0.5784   0.6433   0.6412   0.0000   0.0000   - 0.5964  \u001b[0m\n",
      "Training 153    - 100    -            0.3481   0.0181   1.0000   1.0000   0.8238   0.8248   |            0.5773   0.9255   0.9275   0.7449   0.7471   - 21.1503 \n",
      "\u001b[1;4mValidati 153    - 100    -            0.3492   0.0181   0.6313   0.6334   0.0000   0.0000   |            0.5782   0.6422   0.6408   0.0000   0.0000   - 0.5732  \u001b[0m\n",
      "Training 154    - 100    -            0.3472   0.0181   1.0000   1.0000   0.8217   0.8243   |            0.5761   0.9508   0.9511   0.7412   0.7428   - 21.0062 \n",
      "\u001b[1;4mValidati 154    - 100    -            0.3482   0.0181   0.6212   0.6133   0.0000   0.0000   |            0.5770   0.6158   0.6170   0.0000   0.0000   - 0.5916  \u001b[0m\n",
      "Training 155    - 100    -            0.3462   0.0180   1.0000   1.0000   0.8184   0.8199   |            0.5758   0.9242   0.9238   0.7496   0.7512   - 21.0251 \n",
      "\u001b[1;4mValidati 155    - 100    -            0.3473   0.0180   0.6025   0.6104   0.0000   0.0000   |            0.5767   0.6246   0.6264   0.0000   0.0000   - 0.5838  \u001b[0m\n",
      "Training 156    - 100    -            0.3453   0.0180   1.0000   1.0000   0.8190   0.8214   |            0.5753   0.9394   0.9407   0.7510   0.7540   - 20.9882 \n",
      "\u001b[1;4mValidati 156    - 100    -            0.3464   0.0180   0.6420   0.6447   0.0000   0.0000   |            0.5762   0.6565   0.6493   0.0000   0.0000   - 0.5817  \u001b[0m\n",
      "Training 157    - 100    -            0.3444   0.0180   1.0000   1.0000   0.8184   0.8208   |            0.5746   0.9318   0.9322   0.7535   0.7551   - 20.9060 \n",
      "\u001b[1;4mValidati 157    - 100    -            0.3455   0.0180   0.6112   0.6146   0.0000   0.0000   |            0.5756   0.6223   0.6231   0.0000   0.0000   - 0.6122  \u001b[0m\n",
      "Training 158    - 100    -            0.3436   0.0180   0.9975   0.9975   0.8127   0.8142   |            0.5740   0.9508   0.9481   0.7398   0.7431   - 21.0622 \n",
      "\u001b[1;4mValidati 158    - 100    -            0.3446   0.0180   0.6167   0.6168   0.0000   0.0000   |            0.5749   0.6069   0.6070   0.0000   0.0000   - 0.5838  \u001b[0m\n",
      "Training 159    - 100    -            0.3426   0.0180   1.0000   1.0000   0.8146   0.8156   |            0.5742   0.9141   0.9155   0.7445   0.7461   - 20.9353 \n",
      "\u001b[1;4mValidati 159    - 100    -            0.3437   0.0180   0.6212   0.6258   0.0000   0.0000   |            0.5752   0.5982   0.5970   0.0000   0.0000   - 0.5819  \u001b[0m\n",
      "Training 160    - 100    -            0.3418   0.0180   1.0000   1.0000   0.8189   0.8210   |            0.5743   0.9217   0.9211   0.7393   0.7406   - 20.9035 \n",
      "\u001b[1;4mValidati 160    - 100    -            0.3428   0.0180   0.6386   0.6342   0.0000   0.0000   |            0.5751   0.6520   0.6563   0.0000   0.0000   - 0.6132  \u001b[0m\n",
      "Training 161    - 100    -            0.3409   0.0179   1.0000   1.0000   0.8166   0.8186   |            0.5739   0.9356   0.9356   0.7395   0.7422   - 21.1607 \n",
      "\u001b[1;4mValidati 161    - 100    -            0.3419   0.0179   0.6585   0.6608   0.0000   0.0000   |            0.5747   0.6487   0.6506   0.0000   0.0000   - 0.5840  \u001b[0m\n",
      "Training 162    - 100    -            0.3400   0.0179   1.0000   1.0000   0.8185   0.8211   |            0.5738   0.9179   0.9202   0.7380   0.7411   - 20.8953 \n",
      "\u001b[1;4mValidati 162    - 100    -            0.3410   0.0179   0.6386   0.6432   0.0000   0.0000   |            0.5747   0.6509   0.6501   0.0000   0.0000   - 0.5697  \u001b[0m\n",
      "Training 163    - 100    -            0.3391   0.0179   1.0000   1.0000   0.8158   0.8191   |            0.5732   0.9482   0.9465   0.7548   0.7561   - 20.9602 \n",
      "\u001b[1;4mValidati 163    - 100    -            0.3402   0.0179   0.6357   0.6377   0.0000   0.0000   |            0.5741   0.6366   0.6382   0.0000   0.0000   - 0.5898  \u001b[0m\n",
      "Training 164    - 100    -            0.3383   0.0179   1.0000   1.0000   0.8158   0.8195   |            0.5730   0.9205   0.9232   0.7473   0.7490   - 20.9639 \n",
      "\u001b[1;4mValidati 164    - 100    -            0.3395   0.0179   0.6301   0.6331   0.0000   0.0000   |            0.5740   0.6058   0.6130   0.0000   0.0000   - 0.5842  \u001b[0m\n",
      "Training 165    - 100    -            0.3377   0.0179   1.0000   1.0000   0.8204   0.8232   |            0.5729   0.9394   0.9381   0.7578   0.7590   - 20.9434 \n",
      "\u001b[1;4mValidati 165    - 100    -            0.3388   0.0179   0.6212   0.6247   0.0000   0.0000   |            0.5739   0.6201   0.6272   0.0000   0.0000   - 0.5824  \u001b[0m\n",
      "Training 166    - 100    -            0.3369   0.0178   1.0000   1.0000   0.8157   0.8176   |            0.5726   0.9331   0.9326   0.7414   0.7433   - 21.0849 \n",
      "\u001b[1;4mValidati 166    - 100    -            0.3380   0.0178   0.6313   0.6343   0.0000   0.0000   |            0.5735   0.6290   0.6296   0.0000   0.0000   - 0.6036  \u001b[0m\n",
      "Training 167    - 100    -            0.3362   0.0178   1.0000   1.0000   0.8175   0.8192   |            0.5728   0.9192   0.9199   0.7568   0.7589   - 21.3879 \n",
      "\u001b[1;4mValidati 167    - 100    -            0.3371   0.0178   0.6520   0.6559   0.0000   0.0000   |            0.5737   0.6498   0.6444   0.0000   0.0000   - 0.6035  \u001b[0m\n",
      "Training 168    - 100    -            0.3353   0.0178   0.9987   0.9993   0.8097   0.8132   |            0.5726   0.9280   0.9278   0.7538   0.7555   - 21.2689 \n",
      "\u001b[1;4mValidati 168    - 100    -            0.3362   0.0178   0.6400   0.6399   0.0000   0.0000   |            0.5734   0.6531   0.6503   0.0000   0.0000   - 0.5979  \u001b[0m\n",
      "Training 169    - 100    -            0.3344   0.0178   1.0000   1.0000   0.8133   0.8153   |            0.5724   0.9167   0.9184   0.7469   0.7499   - 21.1970 \n",
      "\u001b[1;4mValidati 169    - 100    -            0.3354   0.0178   0.6156   0.6195   0.0000   0.0000   |            0.5732   0.6400   0.6464   0.0000   0.0000   - 0.6165  \u001b[0m\n",
      "Training 170    - 100    -            0.3336   0.0178   1.0000   1.0000   0.8122   0.8159   |            0.5723   0.9242   0.9223   0.7388   0.7413   - 21.0651 \n",
      "\u001b[1;4mValidati 170    - 100    -            0.3348   0.0177   0.6299   0.6330   0.0000   0.0000   |            0.5733   0.6366   0.6380   0.0000   0.0000   - 0.5842  \u001b[0m\n",
      "Training 171    - 100    -            0.3330   0.0178   1.0000   0.9993   0.8138   0.8167   |            0.5715   0.9533   0.9542   0.7210   0.7240   - 21.0958 \n",
      "\u001b[1;4mValidati 171    - 100    -            0.3340   0.0177   0.6366   0.6403   0.0000   0.0000   |            0.5724   0.6234   0.6254   0.0000   0.0000   - 0.6110  \u001b[0m\n",
      "Training 172    - 100    -            0.3323   0.0177   1.0000   1.0000   0.8122   0.8152   |            0.5714   0.9280   0.9294   0.7442   0.7461   - 21.1758 \n",
      "\u001b[1;4mValidati 172    - 100    -            0.3332   0.0177   0.6663   0.6710   0.0000   0.0000   |            0.5722   0.6574   0.6675   0.0000   0.0000   - 0.5898  \u001b[0m\n",
      "Training 173    - 100    -            0.3314   0.0177   1.0000   1.0000   0.8154   0.8170   |            0.5705   0.9533   0.9518   0.7501   0.7509   - 21.0181 \n",
      "\u001b[1;4mValidati 173    - 100    -            0.3323   0.0177   0.6422   0.6404   0.0000   0.0000   |            0.5713   0.6522   0.6517   0.0000   0.0000   - 0.5934  \u001b[0m\n",
      "Training 174    - 100    -            0.3306   0.0177   1.0000   1.0000   0.8163   0.8191   |            0.5701   0.9432   0.9460   0.7402   0.7420   - 20.9012 \n",
      "\u001b[1;4mValidati 174    - 100    -            0.3316   0.0177   0.6444   0.6485   0.0000   0.0000   |            0.5709   0.6321   0.6501   0.0000   0.0000   - 0.5890  \u001b[0m\n",
      "Training 175    - 100    -            0.3299   0.0177   1.0000   1.0000   0.8119   0.8154   |            0.5701   0.9268   0.9292   0.7357   0.7385   - 21.0473 \n",
      "\u001b[1;4mValidati 175    - 100    -            0.3308   0.0177   0.6596   0.6638   0.0000   0.0000   |            0.5709   0.6730   0.6764   0.0000   0.0000   - 0.6279  \u001b[0m\n",
      "Training 176    - 100    -            0.3291   0.0177   1.0000   1.0000   0.8134   0.8167   |            0.5695   0.9407   0.9399   0.7252   0.7271   - 21.1111 \n",
      "\u001b[1;4mValidati 176    - 100    -            0.3300   0.0177   0.6487   0.6551   0.0000   0.0000   |            0.5703   0.6520   0.6626   0.0000   0.0000   - 0.5844  \u001b[0m\n",
      "Training 177    - 100    -            0.3284   0.0177   1.0000   1.0000   0.8128   0.8155   |            0.5694   0.9343   0.9348   0.7414   0.7444   - 21.1068 \n",
      "\u001b[1;4mValidati 177    - 100    -            0.3293   0.0176   0.6179   0.6212   0.0000   0.0000   |            0.5702   0.6422   0.6430   0.0000   0.0000   - 0.5942  \u001b[0m\n",
      "Training 178    - 100    -            0.3276   0.0176   1.0000   1.0000   0.8160   0.8170   |            0.5696   0.9318   0.9346   0.7491   0.7498   - 20.9686 \n",
      "\u001b[1;4mValidati 178    - 100    -            0.3285   0.0176   0.6730   0.6712   0.0000   0.0000   |            0.5703   0.6696   0.6716   0.0000   0.0000   - 0.5888  \u001b[0m\n",
      "Training 179    - 100    -            0.3268   0.0176   1.0000   1.0000   0.8170   0.8193   |            0.5687   0.9508   0.9516   0.7378   0.7396   - 21.5987 \n",
      "\u001b[1;4mValidati 179    - 100    -            0.3278   0.0176   0.6279   0.6315   0.0000   0.0000   |            0.5695   0.6246   0.6267   0.0000   0.0000   - 0.5856  \u001b[0m\n",
      "Training 180    - 100    -            0.3261   0.0176   1.0000   1.0000   0.8151   0.8164   |            0.5691   0.9141   0.9141   0.7418   0.7445   - 21.3706 \n",
      "\u001b[1;4mValidati 180    - 100    -            0.3270   0.0176   0.6475   0.6499   0.0000   0.0000   |            0.5698   0.6388   0.6438   0.0000   0.0000   - 0.5807  \u001b[0m\n",
      "Training 181    - 100    -            0.3254   0.0176   1.0000   1.0000   0.8137   0.8155   |            0.5690   0.9116   0.9124   0.7225   0.7255   - 20.9635 \n",
      "\u001b[1;4mValidati 181    - 100    -            0.3263   0.0176   0.6520   0.6522   0.0000   0.0000   |            0.5697   0.6576   0.6636   0.0000   0.0000   - 0.5836  \u001b[0m\n",
      "Training 182    - 100    -            0.3247   0.0176   1.0000   1.0000   0.8142   0.8181   |            0.5689   0.9318   0.9301   0.7252   0.7278   - 20.9722 \n",
      "\u001b[1;4mValidati 182    - 100    -            0.3256   0.0176   0.6411   0.6422   0.0000   0.0000   |            0.5697   0.6346   0.6351   0.0000   0.0000   - 0.5958  \u001b[0m\n",
      "Training 183    - 100    -            0.3240   0.0176   1.0000   1.0000   0.8139   0.8166   |            0.5691   0.9141   0.9121   0.7468   0.7494   - 21.0162 \n",
      "\u001b[1;4mValidati 183    - 100    -            0.3249   0.0176   0.6377   0.6399   0.0000   0.0000   |            0.5698   0.6478   0.6538   0.0000   0.0000   - 0.5855  \u001b[0m\n",
      "Training 184    - 100    -            0.3233   0.0176   1.0000   1.0000   0.8119   0.8152   |            0.5690   0.9293   0.9281   0.7375   0.7385   - 21.1660 \n",
      "\u001b[1;4mValidati 184    - 100    -            0.3242   0.0176   0.6542   0.6569   0.0000   0.0000   |            0.5697   0.6475   0.6603   0.0000   0.0000   - 0.5855  \u001b[0m\n",
      "Training 185    - 100    -            0.3226   0.0176   1.0000   1.0000   0.8130   0.8153   |            0.5679   0.9659   0.9648   0.7261   0.7267   - 20.9077 \n",
      "\u001b[1;4mValidati 185    - 100    -            0.3236   0.0176   0.6223   0.6261   0.0000   0.0000   |            0.5688   0.6246   0.6318   0.0000   0.0000   - 0.5927  \u001b[0m\n",
      "Training 186    - 100    -            0.3220   0.0175   1.0000   1.0000   0.8120   0.8145   |            0.5676   0.9280   0.9312   0.7442   0.7467   - 21.0598 \n",
      "\u001b[1;4mValidati 186    - 100    -            0.3228   0.0175   0.6652   0.6658   0.0000   0.0000   |            0.5683   0.6674   0.6722   0.0000   0.0000   - 0.6009  \u001b[0m\n",
      "Training 187    - 100    -            0.3213   0.0175   1.0000   1.0000   0.8123   0.8153   |            0.5666   0.9558   0.9554   0.7480   0.7496   - 20.9355 \n",
      "\u001b[1;4mValidati 187    - 100    -            0.3221   0.0175   0.6498   0.6533   0.0000   0.0000   |            0.5673   0.6520   0.6515   0.0000   0.0000   - 0.5891  \u001b[0m\n",
      "Training 188    - 100    -            0.3206   0.0175   1.0000   1.0000   0.8103   0.8113   |            0.5664   0.9356   0.9351   0.7375   0.7389   - 20.9804 \n",
      "\u001b[1;4mValidati 188    - 100    -            0.3214   0.0175   0.6321   0.6376   0.0000   0.0000   |            0.5672   0.6422   0.6431   0.0000   0.0000   - 0.5974  \u001b[0m\n",
      "Training 189    - 100    -            0.3199   0.0175   1.0000   1.0000   0.8120   0.8149   |            0.5667   0.9053   0.9053   0.7362   0.7384   - 21.2972 \n",
      "\u001b[1;4mValidati 189    - 100    -            0.3208   0.0175   0.6377   0.6381   0.0000   0.0000   |            0.5675   0.6498   0.6443   0.0000   0.0000   - 0.5775  \u001b[0m\n",
      "Training 190    - 100    -            0.3193   0.0175   0.9987   0.9987   0.8115   0.8135   |            0.5663   0.9419   0.9409   0.7200   0.7213   - 21.0742 \n",
      "\u001b[1;4mValidati 190    - 100    -            0.3202   0.0175   0.6377   0.6419   0.0000   0.0000   |            0.5671   0.6377   0.6427   0.0000   0.0000   - 0.6410  \u001b[0m\n",
      "Training 191    - 100    -            0.3187   0.0175   1.0000   1.0000   0.8119   0.8156   |            0.5656   0.9470   0.9483   0.7205   0.7223   - 21.3556 \n",
      "\u001b[1;4mValidati 191    - 100    -            0.3196   0.0175   0.6498   0.6532   0.0000   0.0000   |            0.5664   0.6257   0.6266   0.0000   0.0000   - 0.6425  \u001b[0m\n",
      "Training 192    - 100    -            0.3181   0.0175   1.0000   1.0000   0.8113   0.8140   |            0.5650   0.9470   0.9456   0.7328   0.7356   - 21.3445 \n",
      "\u001b[1;4mValidati 192    - 100    -            0.3189   0.0175   0.6464   0.6521   0.0000   0.0000   |            0.5659   0.6377   0.6436   0.0000   0.0000   - 0.5839  \u001b[0m\n",
      "Training 193    - 100    -            0.3175   0.0175   0.9987   0.9993   0.8105   0.8140   |            0.5646   0.9369   0.9360   0.7272   0.7291   - 21.3345 \n",
      "\u001b[1;4mValidati 193    - 100    -            0.3183   0.0175   0.6355   0.6402   0.0000   0.0000   |            0.5653   0.6475   0.6534   0.0000   0.0000   - 0.6137  \u001b[0m\n",
      "Training 194    - 100    -            0.3168   0.0175   1.0000   1.0000   0.8116   0.8146   |            0.5641   0.9432   0.9441   0.7247   0.7273   - 21.1595 \n",
      "\u001b[1;4mValidati 194    - 100    -            0.3177   0.0175   0.6257   0.6302   0.0000   0.0000   |            0.5650   0.6201   0.6251   0.0000   0.0000   - 0.5919  \u001b[0m\n",
      "Training 195    - 100    -            0.3163   0.0175   1.0000   1.0000   0.8145   0.8180   |            0.5636   0.9419   0.9439   0.7402   0.7438   - 21.0936 \n",
      "\u001b[1;4mValidati 195    - 100    -            0.3172   0.0174   0.6167   0.6241   0.0000   0.0000   |            0.5643   0.6355   0.6396   0.0000   0.0000   - 0.5851  \u001b[0m\n",
      "Training 196    - 100    -            0.3157   0.0174   1.0000   1.0000   0.8109   0.8143   |            0.5638   0.9179   0.9159   0.7433   0.7463   - 21.1915 \n",
      "\u001b[1;4mValidati 196    - 100    -            0.3167   0.0174   0.6366   0.6431   0.0000   0.0000   |            0.5645   0.6388   0.6458   0.0000   0.0000   - 0.5845  \u001b[0m\n",
      "Training 197    - 100    -            0.3152   0.0174   0.9975   0.9975   0.8107   0.8129   |            0.5634   0.9230   0.9248   0.7096   0.7117   - 21.0508 \n",
      "\u001b[1;4mValidati 197    - 100    -            0.3160   0.0174   0.6629   0.6622   0.0000   0.0000   |            0.5641   0.6464   0.6500   0.0000   0.0000   - 0.5863  \u001b[0m\n",
      "Training 198    - 100    -            0.3146   0.0174   1.0000   1.0000   0.8129   0.8158   |            0.5630   0.9482   0.9483   0.7399   0.7401   - 20.9775 \n",
      "\u001b[1;4mValidati 198    - 100    -            0.3155   0.0174   0.6388   0.6406   0.0000   0.0000   |            0.5637   0.6411   0.6420   0.0000   0.0000   - 0.5702  \u001b[0m\n",
      "Training 199    - 100    -            0.3140   0.0174   1.0000   1.0000   0.8119   0.8145   |            0.5634   0.9179   0.9198   0.7227   0.7241   - 20.9365 \n",
      "\u001b[1;4mValidati 199    - 100    -            0.3149   0.0174   0.6366   0.6390   0.0000   0.0000   |            0.5641   0.6234   0.6262   0.0000   0.0000   - 0.5990  \u001b[0m\n",
      "Training 200    - 100    -            0.3135   0.0174   1.0000   1.0000   0.8124   0.8151   |            0.5630   0.9369   0.9389   0.7391   0.7422   - 21.2052 \n",
      "\u001b[1;4mValidati 200    - 100    -            0.3143   0.0174   0.6268   0.6278   0.0000   0.0000   |            0.5637   0.6257   0.6313   0.0000   0.0000   - 0.5824  \u001b[0m\r"
     ]
    }
   ],
   "source": [
    "print(header)\n",
    "\n",
    "start_epoch = checkpoint.epoch_counter\n",
    "end_epoch = args.nb_epoch\n",
    "\n",
    "for e in range(start_epoch, args.nb_epoch):\n",
    "    train(e)\n",
    "    val(e)\n",
    "\n",
    "    tensorboard.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the hyper parameters and the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {}\n",
    "for key, value in args.__dict__.items():\n",
    "    hparams[key] = str(value)\n",
    "\n",
    "final_metrics = {\n",
    "    \"max_acc_student\": maximum_tracker.max[\"student_acc\"],\n",
    "    \"max_f1_student\": maximum_tracker.max[\"student_f1\"],\n",
    "    \"max_acc_teacher\": maximum_tracker.max[\"teacher_acc\"],\n",
    "    \"max_f1_teacher\": maximum_tracker.max[\"teacher_f1\"],\n",
    "}\n",
    "\n",
    "tensorboard.add_hparams(hparams, final_metrics)\n",
    "\n",
    "tensorboard.flush()\n",
    "tensorboard.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x1008 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = list(range(checkpoint.epoch_counter))\n",
    "sm = lambda y, w: np.convolve(y, np.ones(w) / w, mode=\"same\")\n",
    "pp = lambda k: plt.plot(\n",
    "    x, tensorboard.history[k], label=f\"{k} = {max(tensorboard.history[k])}\"\n",
    ")\n",
    "spp = lambda k: plt.plot(\n",
    "    x, sm(tensorboard.history[k], 5), label=f\"{k} = {max(tensorboard.history[k])}\"\n",
    ")\n",
    "\n",
    "\n",
    "plt.figure(0, figsize=(30, 14))\n",
    "plt.subplot(2, 3, 1)\n",
    "pp(\"max/student_acc\")\n",
    "pp(\"max/teacher_acc\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "pp(\"hyperparameters/learning_rate\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleAttributeError",
     "evalue": "'MixUpBatchShuffle' object has no attribute 'lambda_history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-27c4ea1a1ca9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixup_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambda_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/pytorch-dev/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    937\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m    940\u001b[0m             type(self).__name__, name))\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'MixUpBatchShuffle' object has no attribute 'lambda_history'"
     ]
    }
   ],
   "source": [
    "plt.hist(mixup_fn.lambda_history, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "320000 * 2 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "64 * 500 * 4 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "625 / 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-dev",
   "language": "python",
   "name": "pytorch-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
