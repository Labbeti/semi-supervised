{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.cuda import empty_cache\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.nn.parallel import DataParallel\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from advertorch.attacks import GradientSignAttack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from metric_utils.metrics import CategoricalAccuracy, FScore, ContinueAverage, Ratio\n",
    "from SSL.util.checkpoint import CheckPoint, mSummaryWriter\n",
    "from SSL.util.utils import reset_seed, get_datetime, ZipCycle, track_maximum, save_source_as_img, DotDict\n",
    "from SSL.util.model_loader import load_model\n",
    "from SSL.util.loaders import load_dataset, load_optimizer, load_callbacks, load_preprocesser\n",
    "\n",
    "from SSL.ramps import Warmup, sigmoid_rampup\n",
    "from SSL.losses import loss_cot, loss_diff, loss_sup, JensenShanon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--from_config\", default=\"\", type=str)\n",
    "parser.add_argument(\"-d\", \"--dataset_root\", default=\"../../datasets/\", type=str)\n",
    "parser.add_argument(\"-D\", \"--dataset\", default=\"esc10\", type=str)\n",
    "\n",
    "group_t = parser.add_argument_group(\"Commun parameters\")\n",
    "group_t.add_argument(\"--model\", default=\"wideresnet28_2\", type=str)\n",
    "group_t.add_argument(\"--supervised_ratio\", default=0.1, type=float)\n",
    "group_t.add_argument(\"--batch_size\", default=100, type=int)\n",
    "group_t.add_argument(\"--nb_epoch\", default=300, type=int)\n",
    "group_t.add_argument(\"--learning_rate\", default=5e-4, type=float)\n",
    "group_t.add_argument(\"--resume\", action=\"store_true\", default=False)\n",
    "group_t.add_argument(\"--seed\", default=1234, type=int)\n",
    "\n",
    "group_m = parser.add_argument_group(\"Model parameters\")\n",
    "group_m.add_argument(\"--num_classes\", default=10, type=int)\n",
    "\n",
    "group_u = parser.add_argument_group(\"ESC and UBS8K parameters\")\n",
    "group_u.add_argument(\"-t\", \"--train_folds\", nargs=\"+\", default=[1, 2, 3, 4], type=int)\n",
    "group_u.add_argument(\"-v\", \"--val_folds\", nargs=\"+\", default=[5], type=int)\n",
    "\n",
    "group_h = parser.add_argument_group('hyperparameters')\n",
    "group_h.add_argument(\"--lambda_cot_max\", default=1, type=float)\n",
    "group_h.add_argument(\"--lambda_diff_max\", default=0.5, type=float)\n",
    "group_h.add_argument(\"--warmup_length\", default=160, type=int)\n",
    "group_h.add_argument(\"--fusion_method\", default=\"m1\")\n",
    "group_h.add_argument(\"--epsilon\", default=0.02, type=float)\n",
    "\n",
    "group_h.add_argument(\"--ema_alpha\", default=0.999, type=float)\n",
    "group_h.add_argument(\"--teacher_noise\", default=0, type=float)\n",
    "group_h.add_argument(\"--lambda_ccost_max\", default=1, type=float)\n",
    "group_h.add_argument(\"--ccost_method\", default=\"js\", type=str)\n",
    "group_h.add_argument(\"--ccost_softmax\", action=\"store_false\", default=True)\n",
    "\n",
    "group_l = parser.add_argument_group(\"Logs\")\n",
    "group_l.add_argument(\"--checkpoint_root\", default=\"../model_save/\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_root\", default=\"../tensorboard/\", type=str)\n",
    "group_l.add_argument(\"--checkpoint_path\", default=\"deep-co-training\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_path\", default=\"deep-co-training\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_sufix\", default=\"\", type=str)\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "tensorboard_path = os.path.join(args.tensorboard_root, args.dataset, args.tensorboard_path)\n",
    "checkpoint_path = os.path.join(args.checkpoint_root, args.dataset, args.checkpoint_path)"
   ]
  },
  {
   "source": [
    "## Basic verification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_datasets = [\"esc10\", \"ubs8k\", \"speechcommand\"]\n",
    "available_models = [\"cnn03\", \"wideresnet28_2\", \"wideresnet28_4\", \"wideresnet28_8\"]\n",
    "available_ccost_method = [\"mse\", \"js\"]\n",
    "\n",
    "assert args.dataset in available_datasets\n",
    "assert args.model in available_models\n",
    "assert args.ccost_method in available_ccost_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SSL.trainers.trainers import Trainer\n",
    "\n",
    "class DeepCoTrainingTeacherTrainer(Trainer):\n",
    "    def __init__(self, model: str, dataset: str,\n",
    "                 ccost_method: str = \"mse\",\n",
    "                 fusion_method: str = \"m1\",\n",
    "                 ema_alpha: float = 0.999,\n",
    "                 teacher_noise: int = 0):\n",
    "        super().__init__(model, \"dct\", dataset)\n",
    "        self.ccost_method = ccost_method\n",
    "        self.fusion_method = fusion_method\n",
    "        self.ema_alpha = ema_alpha\n",
    "        self.teacher_noise = teacher_noise\n",
    "\n",
    "trainer = DeepCoTrainingTeacherTrainer(\n",
    "    args.model, args.dataset,\n",
    "    args.ccost_method, args.fusion_method, args.ema_alpha,\n",
    "    args.teacher_noise\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Load the transformation\n",
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.load_transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import MethodType\n",
    "\n",
    "def _load_dataset(self, parameters):\n",
    "    print(\"Load the dataset\")\n",
    "    outputs = load_dataset(\n",
    "        framework=self.framework,\n",
    "        train_transform=self.train_transform,\n",
    "        val_transform=self.val_transform,\n",
    "        **parameters\n",
    "    )\n",
    "\n",
    "    self.manager, self.train_loader, self.val_loader = outputs\n",
    "    self.input_shape = tuple(self.train_loader._iterables[0].dataset[0][0].shape)\n",
    "\n",
    "trainer.load_dataset = MethodType(_load_dataset, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Load the dataset\n[5]\nDataset already downloaded and verified.\nDataset already downloaded and verified.\n"
     ]
    }
   ],
   "source": [
    "parameters = dict(\n",
    "    dataset=args.dataset,\n",
    "    \n",
    "    dataset_root = args.dataset_root,\n",
    "    supervised_ratio = args.supervised_ratio,\n",
    "    batch_size = args.batch_size,\n",
    "    train_folds = args.train_folds,\n",
    "    val_folds = args.val_folds,\n",
    "    \n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "\n",
    "    verbose = 2\n",
    ")\n",
    "\n",
    "trainer.load_dataset(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Prep models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_model(self):\n",
    "    print(\"Create the model\")\n",
    "    empty_cache()\n",
    "\n",
    "    model_func = load_model(self.dataset, self.model_str)\n",
    "    commun_args = dict(\n",
    "        manager=self.manager,\n",
    "        num_classes=self.num_classes,\n",
    "        input_shape=list(self.input_shape),\n",
    "    )\n",
    "\n",
    "    # Create the three models\n",
    "    self.m1 = model_func(**commun_args)\n",
    "    self.m2 = model_func(**commun_args)\n",
    "    self.teacher = model_func(**commun_args)\n",
    "\n",
    "    # push to cuda\n",
    "    self.m1, self.m2, self.teacher = self.m1.cuda(), self.m2.cuda(), self.teacher.cuda()\n",
    "\n",
    "    # Remove teacher from the graph\n",
    "    for p in teacher.parameters():\n",
    "        p.detach()\n",
    "\n",
    "    summary(self.m1, self.input_shape)\n",
    "\n",
    "trainer.create_model = MethodType(_create_model, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Create the model\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'empty_cache' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3b2b2e2da19e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-fd3473883b1d>\u001b[0m in \u001b[0;36m_create_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Create the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'empty_cache' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training initialization"
   ]
  },
  {
   "source": [
    "## Losses"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_loss(self):\n",
    "    # see losses.py\n",
    "    if self.ccost_method == \"mse\":\n",
    "        self.cc = nn.MSELoss(reduction=\"mean\") # Unsupervised loss\n",
    "    elif self.ccost_method == \"js\":\n",
    "        self.cc = JensenShanon\n",
    "\n",
    "trainer.init_loss = MethodType(init_loss, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.init_loss()"
   ]
  },
  {
   "source": [
    "## Optiomizer & callbacks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters=DotDict(\n",
    "    learning_rate=args.learning_rate,\n",
    ")\n",
    "trainer.init_optimizer(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters=DotDict(\n",
    "    nb_epoch=args.nb_epoch,\n",
    "    optimizer=trainer.optimizer,\n",
    ")\n",
    "trainer.init_callbacks(parameters)"
   ]
  },
  {
   "source": [
    "## Logs and Checkpoint"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logs\n",
    "def init_logs(self, parameters: DotDict):\n",
    "    print(\"Prepare the log system\")\n",
    "\n",
    "    title_ele = (\n",
    "        args.model, args.supervised_ratio,\n",
    "        get_datetime(), self.model_str,\n",
    "        args.fusion_method, args.ema_alpha, args.teacher_noise\n",
    "    )\n",
    "\n",
    "    tensorboard_title = \"%s/%sS/%s_%s_teacher_%s-fusion_%sa_%sn\" % title_ele\n",
    "    self.tensorboard = SummaryWriter(log_dir=\"%s/%s\" % (self.tensorboard_path, tensorboard_title))\n",
    "\n",
    "trainer.init_logs = MethodType(init_logs, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.init_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "def init_checkpoint(self, parameters: DotDict):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "../tensorboard/esc10/deep-co-training/wideresnet28_2/0.1S/2020-11-15_16:52:07_wideresnet28_2_teacher_m1-fusion_0.999a_0n\n"
     ]
    }
   ],
   "source": [
    "# tensorboard\n",
    "tensorboard_title = f\"{args.model}/{args.supervised_ratio}S/\" \\\n",
    "                    f\"{get_datetime()}_{model_func.__name__}_teacher_\" \\\n",
    "                    f\"{args.fusion_method}-fusion_{args.ema_alpha}a_{args.teacher_noise}n\"\n",
    "checkpoint_title = f\"{args.model}/{args.supervised_ratio}S/\" \\\n",
    "                   f\"{args.model}_teacher_\" \\\n",
    "                   f\"{args.fusion_method}-fusion_{args.ema_alpha}a_{args.teacher_noise}n\"\n",
    "\n",
    "tensorboard = mSummaryWriter(log_dir=f\"{tensorboard_path}/{tensorboard_title}\",comment=model_func.__name__)\n",
    "print(os.path.join(tensorboard_path, tensorboard_title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer & callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optim_args = dict(\n",
    "    learning_rate=args.learning_rate,\n",
    ")\n",
    "\n",
    "optimizer = load_optimizer(args.dataset, \"dct\", model1=m1, model2=m2, **optim_args)\n",
    "callbacks = load_callbacks(args.dataset, \"dct\", optimizer=optimizer, nb_epoch=args.nb_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Adversarial generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "# adversarial generation\n",
    "adv_generator_1 = GradientSignAttack(\n",
    "    m1, loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"),\n",
    "    eps=args.epsilon, clip_min=-np.inf, clip_max=np.inf, targeted=False\n",
    ")\n",
    "\n",
    "adv_generator_2 = GradientSignAttack(\n",
    "    m2, loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"),\n",
    "    eps=args.epsilon, clip_min=-np.inf, clip_max=np.inf, targeted=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "# define the warmups & add them to the callbacks (for update)\n",
    "lambda_cot = Warmup(args.lambda_cot_max, args.warmup_length, sigmoid_rampup)\n",
    "lambda_diff = Warmup(args.lambda_diff_max, args.warmup_length, sigmoid_rampup)\n",
    "lambda_ccost = Warmup(args.lambda_ccost_max, args.warmup_length, sigmoid_rampup)\n",
    "callbacks += [lambda_cot, lambda_diff, lambda_ccost]\n",
    "\n",
    "# checkpoints\n",
    "checkpoint = CheckPoint([m1, m2, teacher], optimizer, mode=\"max\", name=\"%s/%s_m1.torch\" % (checkpoint_path, checkpoint_title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Metrics and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "metrics_fn = dict(\n",
    "    # ratio_s=[Ratio(), Ratio()],\n",
    "    # ratio_u=[Ratio(), Ratio()],\n",
    "    acc_s=[CategoricalAccuracy(), CategoricalAccuracy()],\n",
    "    acc_u=[CategoricalAccuracy(), CategoricalAccuracy()],\n",
    "    acc_t=[CategoricalAccuracy(), CategoricalAccuracy()],\n",
    "    acc_f=CategoricalAccuracy(),\n",
    "    f1_s=[FScore(), FScore()],\n",
    "    f1_u=[FScore(), FScore()],\n",
    "    \n",
    "    avg_total=ContinueAverage(),\n",
    "    avg_sup=ContinueAverage(),\n",
    "    avg_cot=ContinueAverage(),\n",
    "    avg_diff=ContinueAverage(),\n",
    "    avg_teacher=ContinueAverage(),\n",
    ")\n",
    "\n",
    "maximum_tracker = track_maximum()\n",
    "\n",
    "softmax_fn = lambda x: x\n",
    "if args.ccost_softmax:\n",
    "    softmax_fn = nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "def reset_metrics():\n",
    "    for item in metrics_fn.values():\n",
    "        if isinstance(item, list):\n",
    "            for f in item:\n",
    "                f.reset()\n",
    "        else:\n",
    "            item.reset()\n",
    "\n",
    "reset_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "         Epoch  - %      - Losses:  Lsup   | Lcot   | Ldiff  | Lteach | total  - metrics:  acc_s1    | acc_u1    | acc_ts    | acc_tu    - Time  \n"
     ]
    }
   ],
   "source": [
    "UNDERLINE_SEQ = \"\\033[1;4m\"\n",
    "RESET_SEQ = \"\\033[0m\"\n",
    "\n",
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<8.8} {:<6.6} | {:<6.6} | {:<6.6} | {:<6.6} | {:<6.6} - {:<9.9} {:<9.9} | {:<9.9} | {:<9.9} | {:<9.9} - {:<6.6}\"\n",
    "value_form  = \"{:<8.8} {:<6} - {:<6} - {:<8.8} {:<6.4f} | {:<6.4f} | {:<6.4f} | {:<6.4f} | {:<6.4f}- {:<9.9} {:<9.4f} | {:<9.4f} | {:<9.4f} | {:<9.4f} - {:<6.4f}\"\n",
    "\n",
    "header = header_form.format(\n",
    "    \"\", \"Epoch\", \"%\", \"Losses:\", \"Lsup\", \"Lcot\", \"Ldiff\", \"Lteacher\", \"total\", \"metrics: \", \"acc_s1\", \"acc_u1\", \"acc_ts\", \"acc_tu\", \"Time\"\n",
    ")\n",
    "\n",
    "train_form = value_form\n",
    "val_form = UNDERLINE_SEQ + value_form + RESET_SEQ\n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_teacher_model(student_model, teacher_model, alpha, epoch):\n",
    "    # Use the true average until the exponential average is more correct\n",
    "    alpha = min(1 - 1 / (epoch + 1), alpha)\n",
    "    \n",
    "    for param, ema_param in zip(student_model.parameters(), teacher_model.parameters()):\n",
    "        ema_param.data.mul_(alpha).add_(param.data,  alpha = 1-alpha)\n",
    "\n",
    "noise_fn = lambda x: x\n",
    "if args.teacher_noise != 0:\n",
    "    n_db = args.teacher_noise\n",
    "    noise_fn = transforms.Lambda(lambda x: x + (torch.rand(x.shape).cuda() * n_db + n_db))\n",
    "\n",
    "def arithmetric_avg(a, b):\n",
    "    return (a + b) / 2\n",
    "\n",
    "def geometric_avg(a, b):\n",
    "    # in log space to avoid overflow\n",
    "    return torch.exp(torch.log((a + b) / 2))\n",
    "\n",
    "def harmonic_avg(a, b):\n",
    "    return 2 / (1/a + 1/b)\n",
    "\n",
    "def get_fusion_method(method: str):\n",
    "    if method == \"m1\":\n",
    "        return lambda x, y: x\n",
    "    elif method == \"m2\":\n",
    "        return lambda x, y: y\n",
    "    elif method == \"arithmetic_mean\":\n",
    "        return arithmetric_avg\n",
    "    elif method == \"geometric_mean\":\n",
    "        return geometric_avg\n",
    "    elif method == \"harmonic_mean\":\n",
    "        return harmonic_avg\n",
    "    else:\n",
    "        raise ValueError(\"method %s doesn't exist for fusion\")\n",
    "\n",
    "fusion_fn = get_fusion_method(args.fusion_method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "nb_batch = len(train_loader)\n",
    "def train(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "\n",
    "    reset_metrics()\n",
    "    m1.train()\n",
    "    m2.train()\n",
    "\n",
    "    for batch, (S1, S2, U) in enumerate(train_loader):\n",
    "        x_s1, y_s1 = S1\n",
    "        x_s2, y_s2 = S2\n",
    "        x_u, y_u = U\n",
    "\n",
    "        x_s1, x_s2, x_u = x_s1.cuda(), x_s2.cuda(), x_u.cuda()\n",
    "        y_s1, y_s2, y_u = y_s1.cuda(), y_s2.cuda(), y_u.cuda()\n",
    "\n",
    "        with autocast():\n",
    "            logits_s1 = m1(x_s1)\n",
    "            logits_s2 = m2(x_s2)\n",
    "            logits_u1 = m1(x_u)\n",
    "            logits_u2 = m2(x_u)\n",
    "\n",
    "            logits_tu = teacher(noise_fn(x_u))\n",
    "\n",
    "        # pseudo labels of U\n",
    "        pred_u1 = torch.argmax(logits_u1, 1)\n",
    "        pred_u2 = torch.argmax(logits_u2, 1)\n",
    "\n",
    "        # ======== Generate adversarial examples ========\n",
    "        # fix batchnorm ----\n",
    "        m1.eval()\n",
    "        m2.eval()\n",
    "\n",
    "        #generate adversarial examples ----\n",
    "        adv_data_s1 = adv_generator_1.perturb(x_s1, y_s1)\n",
    "        adv_data_u1 = adv_generator_1.perturb(x_u, pred_u1)\n",
    "\n",
    "        adv_data_s2 = adv_generator_2.perturb(x_s2, y_s2)\n",
    "        adv_data_u2 = adv_generator_2.perturb(x_u, pred_u2)\n",
    "\n",
    "        m1.train()\n",
    "        m2.train()\n",
    "\n",
    "        # predict adversarial examples ----\n",
    "        with autocast():\n",
    "            adv_logits_s1 = m1(adv_data_s2)\n",
    "            adv_logits_s2 = m2(adv_data_s1)\n",
    "\n",
    "            adv_logits_u1 = m1(adv_data_u2)\n",
    "            adv_logits_u2 = m2(adv_data_u1)\n",
    "\n",
    "        # ======== calculate the differents loss ========\n",
    "        # zero the parameter gradients ----\n",
    "        for p in m1.parameters(): p.grad = None # zero grad\n",
    "        for p in m2.parameters(): p.grad = None\n",
    "\n",
    "        # losses ----\n",
    "        with autocast():\n",
    "            l_sup = loss_sup(logits_s1, logits_s2, y_s1, y_s2)\n",
    "\n",
    "            l_cot = loss_cot(logits_u1, logits_u2)\n",
    "\n",
    "            l_diff = loss_diff(\n",
    "                logits_s1, logits_s2, adv_logits_s1, adv_logits_s2,\n",
    "                logits_u1, logits_u2, adv_logits_u1, adv_logits_u2)\n",
    "\n",
    "            # Teacher consistency cost\n",
    "            # logits_student_u = (logits_u1 + logits_u2) / 2\n",
    "            logits_student_u = logits_u1\n",
    "            l_teacher = self.cc(softmax_fn(logits_student_u), softmax_fn(logits_tu))\n",
    "\n",
    "            total_loss = l_sup + lambda_cot() * l_cot + lambda_diff() * l_diff + lambda_ccost() * l_teacher\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ======== Calc the metrics ========\n",
    "        with torch.set_grad_enabled(False):\n",
    "            # predict logits teacher on S1, for monitoring purpose\n",
    "            logits_ts = teacher(x_s1)\n",
    "\n",
    "            # Update teacher\n",
    "            update_teacher_model(m1, teacher, args.ema_alpha, epoch*nb_batch + batch)\n",
    "\n",
    "            # accuracies ----\n",
    "            pred_s1 = torch.argmax(logits_s1, dim=1)\n",
    "            pred_s2 = torch.argmax(logits_s2, dim=1)\n",
    "            pred_t1 = torch.argmax(logits_ts, dim=1)\n",
    "            pred_tu = torch.argmax(logits_tu, dim=1)\n",
    "            pred_ts = torch.argmax(logits_ts, dim=1)\n",
    "\n",
    "            acc_s1 = metrics_fn[\"acc_s\"][0](pred_s1, y_s1)\n",
    "            acc_s2 = metrics_fn[\"acc_s\"][1](pred_s2, y_s2)\n",
    "            acc_u1 = metrics_fn[\"acc_u\"][0](pred_u1, y_u)\n",
    "            acc_u2 = metrics_fn[\"acc_u\"][1](pred_u2, y_u)\n",
    "            acc_t1 = metrics_fn[\"acc_t\"][0](pred_ts, y_s1)\n",
    "            acc_tu = metrics_fn[\"acc_t\"][1](pred_tu, y_u)\n",
    "\n",
    "            # ratios  ----\n",
    "            # adv_pred_s1 = torch.argmax(adv_logits_s1, 1)\n",
    "            # adv_pred_s2 = torch.argmax(adv_logits_s2, 1)\n",
    "            # adv_pred_u1 = torch.argmax(adv_logits_u1, 1)\n",
    "            # adv_pred_u2 = torch.argmax(adv_logits_u2, 1)\n",
    "\n",
    "            # ratio_s1 = metrics_fn[\"ratio_s\"][0](adv_pred_s1, y_s1)\n",
    "            # ratio_s2 = metrics_fn[\"ratio_s\"][0](adv_pred_s2, y_s2)\n",
    "            # ratio_u1 = metrics_fn[\"ratio_s\"][0](adv_pred_u1, y_u)\n",
    "            # ratio_u2 = metrics_fn[\"ratio_s\"][0](adv_pred_u2, y_u)\n",
    "            # ========\n",
    "\n",
    "            avg_total = metrics_fn[\"avg_total\"](total_loss.item())\n",
    "            avg_sup = metrics_fn[\"avg_sup\"](l_sup.item())\n",
    "            avg_diff = metrics_fn[\"avg_diff\"](l_diff.item())\n",
    "            avg_cot = metrics_fn[\"avg_cot\"](l_cot.item())\n",
    "            avg_teacher = metrics_fn[\"avg_teacher\"](l_teacher.item())\n",
    "\n",
    "            # logs\n",
    "            print(train_form.format(\n",
    "                \"Training: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (batch + 1) / len(train_loader)),\n",
    "                \"\", avg_sup.mean, avg_cot.mean, avg_diff.mean, avg_teacher.mean, avg_total.mean,\n",
    "                \"\", acc_s1.mean, acc_u1.mean, acc_t1.mean, acc_tu.mean,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "\n",
    "    # using tensorboard to monitor loss and acc\\n\",\n",
    "    tensorboard.add_scalar('train/total_loss', avg_total.mean, epoch)\n",
    "    tensorboard.add_scalar('train/Lsup', avg_sup.mean, epoch )\n",
    "    tensorboard.add_scalar('train/Lcot', avg_cot.mean, epoch )\n",
    "    tensorboard.add_scalar('train/Ldiff', avg_diff.mean, epoch )\n",
    "    tensorboard.add_scalar('train/Lteacher', avg_teacher.mean, epoch )\n",
    "    tensorboard.add_scalar(\"train/acc_1\", acc_s1.mean, epoch )\n",
    "    tensorboard.add_scalar(\"train/acc_2\", acc_s2.mean, epoch )\n",
    "\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_s1\", acc_s1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_s2\", acc_s2.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_u1\", acc_u1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_u2\", acc_u2.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_t1\", acc_t1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_tu\", acc_tu.mean, epoch)\n",
    "\n",
    "    # tensorboard.add_scalar(\"detail_ratio/ratio_s1\", ratio_s1.mean, epoch)\n",
    "    # tensorboard.add_scalar(\"detail_ratio/ratio_s2\", ratio_s2.mean, epoch)\n",
    "    # tensorboard.add_scalar(\"detail_ratio/ratio_u1\", ratio_u1.mean, epoch)\n",
    "    # tensorboard.add_scalar(\"detail_ratio/ratio_u2\", ratio_u2.mean, epoch)\n",
    "\n",
    "    # Return the total loss to check for NaN\n",
    "    return total_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def test(epoch, msg = \"\"):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "\n",
    "    reset_metrics()\n",
    "    m1.eval()\n",
    "    m2.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for batch, (X, y) in enumerate(val_loader):\n",
    "            x = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            with autocast():\n",
    "                logits_1 = m1(x)\n",
    "                logits_2 = m2(x)\n",
    "                logits_t = teacher(x)\n",
    "\n",
    "                # losses ----\n",
    "                l_sup = loss_sup(logits_1, logits_2, y, y)\n",
    "\n",
    "            # ======== Calc the metrics ========\n",
    "            # accuracies ----\n",
    "            pred_1 = torch.argmax(logits_1, dim=1)\n",
    "            pred_2 = torch.argmax(logits_2, dim=1)\n",
    "            pred_t = torch.argmax(logits_t, dim=1)\n",
    "            pred_f12 = torch.argmax(fusion_fn(logits_1, logits_2), dim=1)\n",
    "\n",
    "            acc_1 = metrics_fn[\"acc_s\"][0](pred_1, y)\n",
    "            acc_2 = metrics_fn[\"acc_s\"][1](pred_2, y)\n",
    "            acc_t = metrics_fn[\"acc_t\"][0](pred_t, y)\n",
    "            acc_f12 = metrics_fn[\"acc_f\"](pred_f12, y)\n",
    "\n",
    "            avg_sup = metrics_fn[\"avg_sup\"](l_sup.item())\n",
    "\n",
    "            # logs\n",
    "            print(val_form.format(\n",
    "                \"Validation: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (batch + 1) / len(train_loader)),\n",
    "                \"\", avg_sup.mean, 0.0, 0.0, 0.0, avg_sup.mean,\n",
    "                \"\", acc_1.mean, 0.0, acc_t.mean, 0.0,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"val/acc_1\", acc_1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"val/acc_2\", acc_2.mean, epoch)\n",
    "    tensorboard.add_scalar(\"val/acc_t\", acc_t.mean, epoch)\n",
    "    tensorboard.add_scalar(\"val/acc_f12\", acc_f12.mean, epoch)\n",
    "        \n",
    "    tensorboard.add_scalar(\"max/acc_1\", maximum_tracker(\"acc_1\", acc_1.mean), epoch )\n",
    "    tensorboard.add_scalar(\"max/acc_2\", maximum_tracker(\"acc_2\", acc_2.mean), epoch )\n",
    "    tensorboard.add_scalar(\"max/acc_t\", maximum_tracker(\"acc_t\", acc_t.mean), epoch )\n",
    "    tensorboard.add_scalar(\"max/acc_f12\", maximum_tracker(\"acc_f12\", acc_f12.mean), epoch )\n",
    "    \n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/lambda_cot\", lambda_cot(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/lambda_diff\", lambda_diff(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/lambda_ccost\", lambda_ccost(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/learning_rate\", get_lr(optimizer), epoch)\n",
    "\n",
    "    # Apply callbacks\n",
    "    for c in callbacks:\n",
    "        c.step()\n",
    "\n",
    "    # call checkpoint\n",
    "    checkpoint.step(acc_1.mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "         Epoch  - %      - Losses:  Lsup   | Lcot   | Ldiff  | Lteach | total  - metrics:  acc_s1    | acc_u1    | acc_ts    | acc_tu    - Time  \n",
      "\n",
      "Training 1      - 100    -          3.3878 | 0.0641 | 4.1340 | 0.0001 | 3.4161-           0.2500    | 0.3222    | 0.2500    | 0.3056    - 5.3139\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'acc_f'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-7aa034ed55fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-9845585e4068>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(epoch, msg)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0macc_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics_fn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"acc_s\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0macc_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics_fn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"acc_t\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0macc_f12\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics_fn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"acc_f\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_f12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mavg_sup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics_fn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"avg_sup\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_sup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'acc_f'"
     ]
    }
   ],
   "source": [
    "# can resume training\n",
    "if args.resume:\n",
    "    checkpoint.load_last()\n",
    "start_epoch = checkpoint.epoch_counter\n",
    "\n",
    "print(header)\n",
    "for epoch in range(start_epoch, args.nb_epoch):\n",
    "    total_loss = train(epoch)\n",
    "    \n",
    "    if np.isnan(total_loss):\n",
    "        print(\"Losses are NaN, stoping the training here\")\n",
    "        break\n",
    "        \n",
    "    test(epoch)\n",
    "\n",
    "    tensorboard.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {}\n",
    "for key, value in args.__dict__.items():\n",
    "    hparams[key] = str(value)\n",
    "\n",
    "final_metrics = {\n",
    "    \"max_acc_1\": maximum_tracker.max[\"acc_1\"],\n",
    "    \"max_acc_2\": maximum_tracker.max[\"acc_2\"],\n",
    "    \"max_acc_t\": maximum_tracker.max[\"acc_t\"],\n",
    "    \"max_acc_t\": maximum_tracker.max[\"acc_t\"],\n",
    "}\n",
    "tensorboard.add_hparams(hparams, final_metrics)\n",
    "tensorboard.flush()\n",
    "\n",
    "source_code_path = \"co-training_teacher.ipynb\"\n",
    "source_code_img, padding_size = save_source_as_img(source_code_path)\n",
    "tensorboard.add_image(\"source_code_path___%s\" % padding_size, source_code_img , 0, dataformats=\"HW\")\n",
    "\n",
    "tensorboard.flush()\n",
    "tensorboard.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = list(range(checkpoint.epoch_counter))\n",
    "sm = lambda y, w: np.convolve(y, np.ones(w)/w, mode='same')\n",
    "pp = lambda k: plt.plot(x, tensorboard.history[k], label=f\"{k} = {max(tensorboard.history[k])}\")\n",
    "spp = lambda k: plt.plot(x, sm(tensorboard.history[k], 5), label=f\"{k} = {max(tensorboard.history[k])}\")\n",
    "\n",
    "\n",
    "plt.figure(0, figsize=(30, 14))\n",
    "plt.subplot(2, 3, 1)\n",
    "pp(\"val/acc_1\")\n",
    "pp(\"val/acc_2\")\n",
    "pp(\"val/acc_t\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "pp(\"detail_hyperparameters/lambda_cot\")\n",
    "pp(\"detail_hyperparameters/lambda_diff\")\n",
    "pp(\"detail_hyperparameters/lambda_ccost\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "pp(\"detail_hyperparameters/learning_rate\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "pp(\"max/acc_1\")\n",
    "pp(\"max/acc_2\")\n",
    "pp(\"max/acc_t\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit ('dct': conda)",
   "metadata": {
    "interpreter": {
     "hash": "4213439f5b613b420c7569d9a47a12bf897408d7580af00afce4972bcc867ad9"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}