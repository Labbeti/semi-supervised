{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.nn.parallel import DataParallel\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from advertorch.attacks import GradientSignAttack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from metric_utils.metrics import CategoricalAccuracy, FScore, ContinueAverage, Ratio\n",
    "from SSL.util.checkpoint import CheckPoint, mSummaryWriter\n",
    "from SSL.util.utils import reset_seed, get_datetime, ZipCycle, track_maximum\n",
    "from SSL.util.model_loader import load_model\n",
    "from SSL.util.loaders import load_dataset, load_optimizer, load_callbacks, load_preprocesser\n",
    "\n",
    "from SSL.ramps import Warmup, sigmoid_rampup\n",
    "from SSL.losses import loss_cot, loss_diff, loss_sup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--from_config\", default=\"\", type=str)\n",
    "parser.add_argument(\"-d\", \"--dataset_root\", default=\"../../datasets/\", type=str)\n",
    "parser.add_argument(\"-D\", \"--dataset\", default=\"esc10\", type=str)\n",
    "\n",
    "group_t = parser.add_argument_group(\"Commun parameters\")\n",
    "group_t.add_argument(\"--model\", default=\"wideresnet28_2\", type=str)\n",
    "group_t.add_argument(\"--supervised_ratio\", default=0.1, type=float)\n",
    "group_t.add_argument(\"--batch_size\", default=100, type=int)\n",
    "group_t.add_argument(\"--nb_epoch\", default=300, type=int)\n",
    "group_t.add_argument(\"--learning_rate\", default=5e-4, type=float)\n",
    "group_t.add_argument(\"--resume\", action=\"store_true\", default=False)\n",
    "group_t.add_argument(\"--seed\", default=1234, type=int)\n",
    "\n",
    "group_m = parser.add_argument_group(\"Model parameters\")\n",
    "group_m.add_argument(\"--num_classes\", default=10, type=int)\n",
    "\n",
    "group_u = parser.add_argument_group(\"ESC and UBS8K parameters\")\n",
    "group_u.add_argument(\"-t\", \"--train_folds\", nargs=\"+\", default=[1, 2, 3, 4], type=int)\n",
    "group_u.add_argument(\"-v\", \"--val_folds\", nargs=\"+\", default=[5], type=int)\n",
    "\n",
    "group_h = parser.add_argument_group('hyperparameters')\n",
    "group_h.add_argument(\"--lambda_cot_max\", default=1, type=float)\n",
    "group_h.add_argument(\"--lambda_diff_max\", default=0.5, type=float)\n",
    "group_h.add_argument(\"--warmup_length\", default=160, type=int)\n",
    "group_h.add_argument(\"--epsilon\", default=0.02, type=float)\n",
    "\n",
    "group_l = parser.add_argument_group(\"Logs\")\n",
    "group_l.add_argument(\"--checkpoint_root\", default=\"../model_save/\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_root\", default=\"../tensorboard/\", type=str)\n",
    "group_l.add_argument(\"--checkpoint_path\", default=\"deep-co-training\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_path\", default=\"deep-co-training\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_sufix\", default=\"\", type=str)\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "tensorboard_path = os.path.join(args.tensorboard_root, args.dataset, args.tensorboard_path)\n",
    "checkpoint_path = os.path.join(args.checkpoint_root, args.dataset, args.checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "reset_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "################################################################################\n### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n################################################################################\n\n"
     ]
    }
   ],
   "source": [
    "train_transform, val_transform = load_preprocesser(args.dataset, \"dct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): MelSpectrogram(\n",
       "    (spectrogram): Spectrogram()\n",
       "    (mel_scale): MelScale()\n",
       "  )\n",
       "  (1): AmplitudeToDB()\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "train_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[5]\nDataset already downloaded and verified.\nDataset already downloaded and verified.\n"
     ]
    }
   ],
   "source": [
    "manager, train_loader, val_loader = load_dataset(\n",
    "    args.dataset,\n",
    "    \"dct\",\n",
    "    \n",
    "    dataset_root = args.dataset_root,\n",
    "    supervised_ratio = args.supervised_ratio,\n",
    "    batch_size = args.batch_size,\n",
    "    train_folds = args.train_folds,\n",
    "    val_folds = args.val_folds,\n",
    "\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    "    \n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([64, 431])"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "input_shape = train_loader._iterables[0].dataset[0][0].shape\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model_func = load_model(args.dataset, args.model)\n",
    "\n",
    "commun_args = dict(\n",
    "    manager=manager,\n",
    "    num_classes=args.num_classes,\n",
    "    input_shape=list(input_shape),\n",
    ")\n",
    "\n",
    "m1, m2 = model_func(**commun_args), model_func(**commun_args)\n",
    "\n",
    "# m1 = DataParallel(m1)\n",
    "# m2 = DataParallel(m2)\n",
    "\n",
    "m1 = m1.cuda()\n",
    "m2 = m2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1          [-1, 32, 64, 431]             864\n       BatchNorm2d-2          [-1, 32, 64, 431]              64\n              ReLU-3          [-1, 32, 64, 431]               0\n         MaxPool2d-4          [-1, 32, 32, 216]               0\n            Conv2d-5          [-1, 32, 32, 216]           9,216\n       BatchNorm2d-6          [-1, 32, 32, 216]              64\n              ReLU-7          [-1, 32, 32, 216]               0\n            Conv2d-8          [-1, 32, 32, 216]           9,216\n       BatchNorm2d-9          [-1, 32, 32, 216]              64\n             ReLU-10          [-1, 32, 32, 216]               0\n       BasicBlock-11          [-1, 32, 32, 216]               0\n           Conv2d-12          [-1, 32, 32, 216]           9,216\n      BatchNorm2d-13          [-1, 32, 32, 216]              64\n             ReLU-14          [-1, 32, 32, 216]               0\n           Conv2d-15          [-1, 32, 32, 216]           9,216\n      BatchNorm2d-16          [-1, 32, 32, 216]              64\n             ReLU-17          [-1, 32, 32, 216]               0\n       BasicBlock-18          [-1, 32, 32, 216]               0\n           Conv2d-19          [-1, 32, 32, 216]           9,216\n      BatchNorm2d-20          [-1, 32, 32, 216]              64\n             ReLU-21          [-1, 32, 32, 216]               0\n           Conv2d-22          [-1, 32, 32, 216]           9,216\n      BatchNorm2d-23          [-1, 32, 32, 216]              64\n             ReLU-24          [-1, 32, 32, 216]               0\n       BasicBlock-25          [-1, 32, 32, 216]               0\n           Conv2d-26          [-1, 32, 32, 216]           9,216\n      BatchNorm2d-27          [-1, 32, 32, 216]              64\n             ReLU-28          [-1, 32, 32, 216]               0\n           Conv2d-29          [-1, 32, 32, 216]           9,216\n      BatchNorm2d-30          [-1, 32, 32, 216]              64\n             ReLU-31          [-1, 32, 32, 216]               0\n       BasicBlock-32          [-1, 32, 32, 216]               0\n           Conv2d-33          [-1, 64, 16, 108]          18,432\n      BatchNorm2d-34          [-1, 64, 16, 108]             128\n             ReLU-35          [-1, 64, 16, 108]               0\n           Conv2d-36          [-1, 64, 16, 108]          36,864\n      BatchNorm2d-37          [-1, 64, 16, 108]             128\n           Conv2d-38          [-1, 64, 16, 108]           2,048\n      BatchNorm2d-39          [-1, 64, 16, 108]             128\n             ReLU-40          [-1, 64, 16, 108]               0\n       BasicBlock-41          [-1, 64, 16, 108]               0\n           Conv2d-42          [-1, 64, 16, 108]          36,864\n      BatchNorm2d-43          [-1, 64, 16, 108]             128\n             ReLU-44          [-1, 64, 16, 108]               0\n           Conv2d-45          [-1, 64, 16, 108]          36,864\n      BatchNorm2d-46          [-1, 64, 16, 108]             128\n             ReLU-47          [-1, 64, 16, 108]               0\n       BasicBlock-48          [-1, 64, 16, 108]               0\n           Conv2d-49          [-1, 64, 16, 108]          36,864\n      BatchNorm2d-50          [-1, 64, 16, 108]             128\n             ReLU-51          [-1, 64, 16, 108]               0\n           Conv2d-52          [-1, 64, 16, 108]          36,864\n      BatchNorm2d-53          [-1, 64, 16, 108]             128\n             ReLU-54          [-1, 64, 16, 108]               0\n       BasicBlock-55          [-1, 64, 16, 108]               0\n           Conv2d-56          [-1, 64, 16, 108]          36,864\n      BatchNorm2d-57          [-1, 64, 16, 108]             128\n             ReLU-58          [-1, 64, 16, 108]               0\n           Conv2d-59          [-1, 64, 16, 108]          36,864\n      BatchNorm2d-60          [-1, 64, 16, 108]             128\n             ReLU-61          [-1, 64, 16, 108]               0\n       BasicBlock-62          [-1, 64, 16, 108]               0\n           Conv2d-63           [-1, 128, 8, 54]          73,728\n      BatchNorm2d-64           [-1, 128, 8, 54]             256\n             ReLU-65           [-1, 128, 8, 54]               0\n           Conv2d-66           [-1, 128, 8, 54]         147,456\n      BatchNorm2d-67           [-1, 128, 8, 54]             256\n           Conv2d-68           [-1, 128, 8, 54]           8,192\n      BatchNorm2d-69           [-1, 128, 8, 54]             256\n             ReLU-70           [-1, 128, 8, 54]               0\n       BasicBlock-71           [-1, 128, 8, 54]               0\n           Conv2d-72           [-1, 128, 8, 54]         147,456\n      BatchNorm2d-73           [-1, 128, 8, 54]             256\n             ReLU-74           [-1, 128, 8, 54]               0\n           Conv2d-75           [-1, 128, 8, 54]         147,456\n      BatchNorm2d-76           [-1, 128, 8, 54]             256\n             ReLU-77           [-1, 128, 8, 54]               0\n       BasicBlock-78           [-1, 128, 8, 54]               0\n           Conv2d-79           [-1, 128, 8, 54]         147,456\n      BatchNorm2d-80           [-1, 128, 8, 54]             256\n             ReLU-81           [-1, 128, 8, 54]               0\n           Conv2d-82           [-1, 128, 8, 54]         147,456\n      BatchNorm2d-83           [-1, 128, 8, 54]             256\n             ReLU-84           [-1, 128, 8, 54]               0\n       BasicBlock-85           [-1, 128, 8, 54]               0\n           Conv2d-86           [-1, 128, 8, 54]         147,456\n      BatchNorm2d-87           [-1, 128, 8, 54]             256\n             ReLU-88           [-1, 128, 8, 54]               0\n           Conv2d-89           [-1, 128, 8, 54]         147,456\n      BatchNorm2d-90           [-1, 128, 8, 54]             256\n             ReLU-91           [-1, 128, 8, 54]               0\n       BasicBlock-92           [-1, 128, 8, 54]               0\nAdaptiveAvgPool2d-93            [-1, 128, 1, 1]               0\n           Linear-94                   [-1, 10]           1,290\n================================================================\nTotal params: 1,472,554\nTrainable params: 1,472,554\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.11\nForward/backward pass size (MB): 107.11\nParams size (MB): 5.62\nEstimated Total Size (MB): 112.83\n----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "s = summary(m1, tuple(input_shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "../tensorboard/esc10/deep-co-training/wideresnet28_2/0.1S/2020-10-28_15:31:32_wideresnet28_2_0.1S\n"
     ]
    }
   ],
   "source": [
    "# tensorboard\n",
    "tensorboard_title = f\"{args.model}/{args.supervised_ratio}S/\" \\\n",
    "                    f\"{get_datetime()}_{model_func.__name__}_{args.supervised_ratio}S\"\n",
    "checkpoint_title = f\"{args.model}/{args.supervised_ratio}S/\" \\\n",
    "                   f\"{args.model}_{args.supervised_ratio}S\"\n",
    "\n",
    "tensorboard = mSummaryWriter(log_dir=f\"{tensorboard_path}/{tensorboard_title}\",comment=model_func.__name__)\n",
    "print(os.path.join(tensorboard_path, tensorboard_title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer & callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optim_args = dict(\n",
    "    learning_rate=args.learning_rate,\n",
    ")\n",
    "\n",
    "optimizer = load_optimizer(args.dataset, \"dct\", model1=m1, model2=m2, **optim_args)\n",
    "callbacks = load_callbacks(args.dataset, \"dct\", optimizer=optimizer, nb_epoch=args.nb_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Adversarial generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "# adversarial generation\n",
    "adv_generator_1 = GradientSignAttack(\n",
    "    m1, loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"),\n",
    "    eps=args.epsilon, clip_min=-np.inf, clip_max=np.inf, targeted=False\n",
    ")\n",
    "\n",
    "adv_generator_2 = GradientSignAttack(\n",
    "    m2, loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"),\n",
    "    eps=args.epsilon, clip_min=-np.inf, clip_max=np.inf, targeted=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "# Losses\n",
    "# see losses.py\n",
    "\n",
    "# define the warmups & add them to the callbacks (for update)\n",
    "lambda_cot = Warmup(args.lambda_cot_max, args.warmup_length, sigmoid_rampup)\n",
    "lambda_diff = Warmup(args.lambda_diff_max, args.warmup_length, sigmoid_rampup)\n",
    "callbacks += [lambda_cot, lambda_diff]\n",
    "\n",
    "# checkpoints\n",
    "checkpoint = CheckPoint([m1, m2], optimizer, mode=\"max\", name=\"%s/%s_m1.torch\" % (checkpoint_path, checkpoint_title))\n",
    "\n",
    "# metrics\n",
    "metrics_fn = dict(\n",
    "    ratio_s=[Ratio(), Ratio()],\n",
    "    ratio_u=[Ratio(), Ratio()],\n",
    "    acc_s=[CategoricalAccuracy(), CategoricalAccuracy()],\n",
    "    acc_u=[CategoricalAccuracy(), CategoricalAccuracy()],\n",
    "    f1_s=[FScore(), FScore()],\n",
    "    f1_u=[FScore(), FScore()],\n",
    "    \n",
    "    avg_total=ContinueAverage(),\n",
    "    avg_sup=ContinueAverage(),\n",
    "    avg_cot=ContinueAverage(),\n",
    "    avg_diff=ContinueAverage(),\n",
    ")\n",
    "maximum_tracker = track_maximum()\n",
    "\n",
    "\n",
    "def reset_metrics():\n",
    "    for item in metrics_fn.values():\n",
    "        if isinstance(item, list):\n",
    "            for f in item:\n",
    "                f.reset()\n",
    "        else:\n",
    "            item.reset()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "reset_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Can resume previous training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "if args.resume:\n",
    "    checkpoint.load_last()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Metrics and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "         Epoch  - %      - Losses:  Lsup   | Lcot   | Ldiff  | total  - metrics:  acc_s1    | acc_u1   - Time  \n"
     ]
    }
   ],
   "source": [
    "UNDERLINE_SEQ = \"\\033[1;4m\"\n",
    "RESET_SEQ = \"\\033[0m\"\n",
    "\n",
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<8.8} {:<6.6} | {:<6.6} | {:<6.6} | {:<6.6} - {:<9.9} {:<9.9} | {:<9.9}- {:<6.6}\"\n",
    "value_form  = \"{:<8.8} {:<6} - {:<6} - {:<8.8} {:<6.4f} | {:<6.4f} | {:<6.4f} | {:<6.4f} - {:<9.9} {:<9.4f} | {:<9.4f}- {:<6.4f}\"\n",
    "\n",
    "header = header_form.format(\n",
    "    \"\", \"Epoch\", \"%\", \"Losses:\", \"Lsup\", \"Lcot\", \"Ldiff\", \"total\", \"metrics: \", \"acc_s1\", \"acc_u1\",\"Time\"\n",
    ")\n",
    "\n",
    "train_form = value_form\n",
    "val_form = UNDERLINE_SEQ + value_form + RESET_SEQ\n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "\n",
    "    reset_metrics()\n",
    "    m1.train()\n",
    "    m2.train()\n",
    "\n",
    "    for batch, (S1, S2, U) in enumerate(train_loader):\n",
    "        x_s1, y_s1 = S1\n",
    "        x_s2, y_s2 = S2\n",
    "        x_u, y_u = U\n",
    "\n",
    "        x_s1, x_s2, x_u = x_s1.cuda(), x_s2.cuda(), x_u.cuda()\n",
    "        y_s1, y_s2, y_u = y_s1.cuda(), y_s2.cuda(), y_u.cuda()\n",
    "\n",
    "        with autocast():\n",
    "            logits_s1 = m1(x_s1)\n",
    "            logits_s2 = m2(x_s2)\n",
    "            logits_u1 = m1(x_u)\n",
    "            logits_u2 = m2(x_u)\n",
    "\n",
    "        # pseudo labels of U\n",
    "        pred_u1 = torch.argmax(logits_u1, 1)\n",
    "        pred_u2 = torch.argmax(logits_u2, 1)\n",
    "\n",
    "        # ======== Generate adversarial examples ========\n",
    "        # fix batchnorm ----\n",
    "        m1.eval()\n",
    "        m2.eval()\n",
    "\n",
    "        #generate adversarial examples ----\n",
    "        adv_data_s1 = adv_generator_1.perturb(x_s1, y_s1)\n",
    "        adv_data_u1 = adv_generator_1.perturb(x_u, pred_u1)\n",
    "\n",
    "        adv_data_s2 = adv_generator_2.perturb(x_s2, y_s2)\n",
    "        adv_data_u2 = adv_generator_2.perturb(x_u, pred_u2)\n",
    "\n",
    "        m1.train()\n",
    "        m2.train()\n",
    "\n",
    "        # predict adversarial examples ----\n",
    "        with autocast():\n",
    "            adv_logits_s1 = m1(adv_data_s2)\n",
    "            adv_logits_s2 = m2(adv_data_s1)\n",
    "\n",
    "            adv_logits_u1 = m1(adv_data_u2)\n",
    "            adv_logits_u2 = m2(adv_data_u1)\n",
    "\n",
    "        # ======== calculate the differents loss ========\n",
    "        # zero the parameter gradients ----\n",
    "        for p in m1.parameters(): p.grad = None # zero grad\n",
    "        for p in m2.parameters(): p.grad = None\n",
    "\n",
    "        # losses ----\n",
    "        with autocast():\n",
    "            l_sup = loss_sup(logits_s1, logits_s2, y_s1, y_s2)\n",
    "\n",
    "            l_cot = loss_cot(logits_u1, logits_u2)\n",
    "\n",
    "            l_diff = loss_diff(\n",
    "                logits_s1, logits_s2, adv_logits_s1, adv_logits_s2,\n",
    "                logits_u1, logits_u2, adv_logits_u1, adv_logits_u2\n",
    "            )\n",
    "\n",
    "            total_loss = l_sup + lambda_cot() * l_cot + lambda_diff() * l_diff\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ======== Calc the metrics ========\n",
    "        with torch.set_grad_enabled(False):\n",
    "            # accuracies ----\n",
    "            pred_s1 = torch.argmax(logits_s1, dim=1)\n",
    "            pred_s2 = torch.argmax(logits_s2, dim=1)\n",
    "\n",
    "            acc_s1 = metrics_fn[\"acc_s\"][0](pred_s1, y_s1)\n",
    "            acc_s2 = metrics_fn[\"acc_s\"][1](pred_s2, y_s2)\n",
    "            acc_u1 = metrics_fn[\"acc_u\"][0](pred_u1, y_u)\n",
    "            acc_u2 = metrics_fn[\"acc_u\"][1](pred_u2, y_u)\n",
    "\n",
    "            # ratios  ----\n",
    "            adv_pred_s1 = torch.argmax(adv_logits_s1, 1)\n",
    "            adv_pred_s2 = torch.argmax(adv_logits_s2, 1)\n",
    "            adv_pred_u1 = torch.argmax(adv_logits_u1, 1)\n",
    "            adv_pred_u2 = torch.argmax(adv_logits_u2, 1)\n",
    "\n",
    "            ratio_s1 = metrics_fn[\"ratio_s\"][0](adv_pred_s1, y_s1)\n",
    "            ratio_s2 = metrics_fn[\"ratio_s\"][0](adv_pred_s2, y_s2)\n",
    "            ratio_u1 = metrics_fn[\"ratio_s\"][0](adv_pred_u1, y_u)\n",
    "            ratio_u2 = metrics_fn[\"ratio_s\"][0](adv_pred_u2, y_u)\n",
    "            # ========\n",
    "\n",
    "            avg_total = metrics_fn[\"avg_total\"](total_loss.item())\n",
    "            avg_sup = metrics_fn[\"avg_sup\"](l_sup.item())\n",
    "            avg_diff = metrics_fn[\"avg_diff\"](l_diff.item())\n",
    "            avg_cot = metrics_fn[\"avg_cot\"](l_cot.item())\n",
    "\n",
    "            # logs\n",
    "            print(train_form.format(\n",
    "                \"Training: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (batch + 1) / len(train_loader)),\n",
    "                \"\", avg_sup.mean, avg_cot.mean, avg_diff.mean, avg_total.mean,\n",
    "                \"\", acc_s1.mean, acc_u1.mean,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "\n",
    "    # using tensorboard to monitor loss and acc\\n\",\n",
    "    tensorboard.add_scalar('train/total_loss', avg_total.mean, epoch)\n",
    "    tensorboard.add_scalar('train/Lsup', avg_sup.mean, epoch )\n",
    "    tensorboard.add_scalar('train/Lcot', avg_cot.mean, epoch )\n",
    "    tensorboard.add_scalar('train/Ldiff', avg_diff.mean, epoch )\n",
    "    tensorboard.add_scalar(\"train/acc_1\", acc_s1.mean, epoch )\n",
    "    tensorboard.add_scalar(\"train/acc_2\", acc_s2.mean, epoch )\n",
    "\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_s1\", acc_s1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_s2\", acc_s2.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_u1\", acc_u1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_u2\", acc_u2.mean, epoch)\n",
    "\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio_s1\", ratio_s1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio_s2\", ratio_s2.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio_u1\", ratio_u1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio_u2\", ratio_u2.mean, epoch)\n",
    "\n",
    "    # Return the total loss to check for NaN\n",
    "    return total_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def test(epoch, msg = \"\"):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "\n",
    "    reset_metrics()\n",
    "    m1.eval()\n",
    "    m2.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for batch, (X, y) in enumerate(val_loader):\n",
    "            x = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            with autocast():\n",
    "                logits_1 = m1(x)\n",
    "                logits_2 = m2(x)\n",
    "\n",
    "                # losses ----\n",
    "                l_sup = loss_sup(logits_1, logits_2, y, y)\n",
    "\n",
    "            # ======== Calc the metrics ========\n",
    "            # accuracies ----\n",
    "            pred_1 = torch.argmax(logits_1, dim=1)\n",
    "            pred_2 = torch.argmax(logits_2, dim=1)\n",
    "\n",
    "            acc_1 = metrics_fn[\"acc_s\"][0](pred_1, y)\n",
    "            acc_2 = metrics_fn[\"acc_s\"][1](pred_2, y)\n",
    "\n",
    "            avg_sup = metrics_fn[\"avg_sup\"](l_sup.item())\n",
    "\n",
    "            # logs\n",
    "            print(val_form.format(\n",
    "                \"Validation: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (batch + 1) / len(train_loader)),\n",
    "                \"\", avg_sup.mean, 0.0, 0.0, avg_sup.mean,\n",
    "                \"\", acc_1.mean, 0.0,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"val/acc_1\", acc_1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"val/acc_2\", acc_2.mean, epoch)\n",
    "        \n",
    "    tensorboard.add_scalar(\"max/acc_1\", maximum_tracker(\"acc_1\", acc_1.mean), epoch )\n",
    "    tensorboard.add_scalar(\"max/acc_2\", maximum_tracker(\"acc_2\", acc_2.mean), epoch )\n",
    "    \n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/lambda_cot\", lambda_cot(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/lambda_diff\", lambda_diff(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/learning_rate\", get_lr(optimizer), epoch)\n",
    "\n",
    "    # Apply callbacks\n",
    "    for c in callbacks:\n",
    "        c.step()\n",
    "\n",
    "    # call checkpoint\n",
    "    checkpoint.step(acc_1.mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "         Epoch  - %      - Losses:  Lsup   | Lcot   | Ldiff  | total  - metrics:  acc_s1    | acc_u1   - Time  \n",
      "\n",
      "Training 1      - 100    -          4.7289 | 0.1494 | 5.3813 | 4.7661 -           0.1250    | 0.1181   - 6.4726\n",
      "\u001b[1;4mValidati 1      - 25     -          10.1545 | 0.0000 | 0.0000 | 10.1545 -           0.2000    | 0.0000   - 0.8135\u001b[0m\n",
      "Training 2      - 100    -          3.2159 | 0.0723 | 4.3175 | 3.2319 -           0.4750    | 0.3167   - 4.0799\n",
      "\n",
      "Training 3      - 100    -          2.3210 | 0.0693 | 3.8062 | 2.3361 -           0.6500    | 0.4431   - 4.1118\n",
      "\n",
      "Training 4      - 100    -          1.9889 | 0.0720 | 3.5281 | 2.0038 -           0.7750    | 0.4944   - 4.0884\n",
      "\n",
      "Training 5      - 100    -          1.7189 | 0.0950 | 3.5132 | 1.7349 -           0.8250    | 0.5972   - 4.0901\n",
      "\n",
      "Training 6      - 100    -          1.5732 | 0.0766 | 3.0524 | 1.5879 -           0.8750    | 0.6000   - 4.0997\n",
      "\n",
      "Training 7      - 100    -          1.6113 | 0.0780 | 3.0909 | 1.6271 -           0.7750    | 0.5986   - 4.1008\n",
      "\n",
      "Training 8      - 100    -          0.9491 | 0.1064 | 3.2333 | 0.9669 -           0.9750    | 0.5889   - 4.1115\n",
      "\n",
      "Training 9      - 100    -          0.8419 | 0.1112 | 3.0583 | 0.8599 -           0.9750    | 0.5972   - 4.1282\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-00f403dd6319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-b0b380c1348b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml_sup\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambda_cot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ml_cot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambda_diff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ml_diff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/dct/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/dct/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(header)\n",
    "\n",
    "start_epoch = checkpoint.epoch_counter\n",
    "\n",
    "for epoch in range(0, args.nb_epoch):\n",
    "    total_loss = train(epoch)\n",
    "    \n",
    "    if np.isnan(total_loss):\n",
    "        print(\"Losses are NaN, stoping the training here\")\n",
    "        break\n",
    "        \n",
    "    test(epoch)\n",
    "\n",
    "    tensorboard.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {}\n",
    "for key, value in args.__dict__.items():\n",
    "    hparams[key] = str(value)\n",
    "\n",
    "final_metrics = {\n",
    "    \"max_acc_1\": maximum_tracker.max[\"acc_1\"],\n",
    "    \"max_acc_2\": maximum_tracker.max[\"acc_2\"],\n",
    "}\n",
    "\n",
    "tensorboard.flush()\n",
    "tensorboard.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = list(range(checkpoint.epoch_counter))\n",
    "sm = lambda y, w: np.convolve(y, np.ones(w)/w, mode='same')\n",
    "pp = lambda k: plt.plot(x, tensorboard.history[k], label=f\"{k} = {max(tensorboard.history[k])}\")\n",
    "spp = lambda k: plt.plot(x, sm(tensorboard.history[k], 5), label=f\"{k} = {max(tensorboard.history[k])}\")\n",
    "\n",
    "\n",
    "plt.figure(0, figsize=(30, 14))\n",
    "plt.subplot(2, 3, 1)\n",
    "pp(\"val/acc_1\")\n",
    "pp(\"val/acc_2\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "pp(\"detail_hyperparameters/lambda_cot\")\n",
    "pp(\"detail_hyperparameters/lambda_diff\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "pp(\"detail_hyperparameters/learning_rate\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.8.0 64-bit ('dct': conda)",
   "display_name": "Python 3.8.0 64-bit ('dct': conda)",
   "metadata": {
    "interpreter": {
     "hash": "4213439f5b613b420c7569d9a47a12bf897408d7580af00afce4972bcc867ad9"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}