{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.nn.parallel import DataParallel\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from advertorch.attacks import GradientSignAttack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from metric_utils.metrics import CategoricalAccuracy, FScore, ContinueAverage, Ratio\n",
    "from SSL.util.checkpoint import CheckPoint, mSummaryWriter\n",
    "from SSL.util.utils import reset_seed, get_datetime, ZipCycle, track_maximum\n",
    "from SSL.util.model_loader import load_model\n",
    "from SSL.util.loaders import (\n",
    "    load_dataset,\n",
    "    load_optimizer,\n",
    "    load_callbacks,\n",
    "    load_preprocesser,\n",
    ")\n",
    "from SSL.util.mixup import MixUpBatchShuffle\n",
    "\n",
    "from SSL.ramps import Warmup, sigmoid_rampup\n",
    "from SSL.losses import loss_cot, loss_diff, loss_sup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--from_config\", default=\"\", type=str)\n",
    "parser.add_argument(\"-d\", \"--dataset_root\", default=\"../../datasets/\", type=str)\n",
    "parser.add_argument(\"-D\", \"--dataset\", default=\"ubs8k\", type=str)\n",
    "parser.add_argument(\"-N\", \"--nb_gpu\", default=1, type=int)\n",
    "\n",
    "group_t = parser.add_argument_group(\"Commun parameters\")\n",
    "group_t.add_argument(\"--model\", default=\"wideresnet28_2\", type=str)\n",
    "group_t.add_argument(\"--supervised_ratio\", default=0.1, type=float)\n",
    "group_t.add_argument(\"--batch_size\", default=100, type=int)\n",
    "group_t.add_argument(\"--nb_epoch\", default=300, type=int)\n",
    "group_t.add_argument(\"--learning_rate\", default=5e-4, type=float)\n",
    "group_t.add_argument(\"--resume\", action=\"store_true\", default=False)\n",
    "group_t.add_argument(\"--seed\", default=1234, type=int)\n",
    "\n",
    "group_m = parser.add_argument_group(\"Model parameters\")\n",
    "group_m.add_argument(\"--num_classes\", default=10, type=int)\n",
    "\n",
    "group_u = parser.add_argument_group(\"ESC and UBS8K parameters\")\n",
    "group_u.add_argument(\n",
    "    \"-t\", \"--train_folds\", nargs=\"+\", default=[1, 2, 3, 4, 5, 6, 7, 8, 9], type=int\n",
    ")\n",
    "group_u.add_argument(\"-v\", \"--val_folds\", nargs=\"+\", default=[10], type=int)\n",
    "\n",
    "group_h = parser.add_argument_group(\"hyperparameters\")\n",
    "group_h.add_argument(\"--lambda_cot_max\", default=1, type=float)\n",
    "group_h.add_argument(\"--lambda_diff_max\", default=0.5, type=float)\n",
    "group_h.add_argument(\"--warmup_length\", default=160, type=int)\n",
    "group_h.add_argument(\"--epsilon\", default=0.02, type=float)\n",
    "\n",
    "group_mixup = parser.add_argument_group(\"Mixup parameters\")\n",
    "group_mixup.add_argument(\"--mixup\", action=\"store_true\", default=False)\n",
    "group_mixup.add_argument(\"--mixup_alpha\", type=float, default=0.4)\n",
    "group_mixup.add_argument(\"--mixup_max\", action=\"store_true\", default=False)\n",
    "group_mixup.add_argument(\"--mixup_label\", action=\"store_true\", default=False)\n",
    "\n",
    "group_l = parser.add_argument_group(\"Logs\")\n",
    "group_l.add_argument(\"--checkpoint_root\", default=\"../../model_save/\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_root\", default=\"../../tensorboard/\", type=str)\n",
    "group_l.add_argument(\"--checkpoint_path\", default=\"deep-co-training_mixup\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_path\", default=\"deep-co-training_mixup\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_sufix\", default=\"\", type=str)\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "tensorboard_path = os.path.join(\n",
    "    args.tensorboard_root, args.dataset, args.tensorboard_path\n",
    ")\n",
    "checkpoint_path = os.path.join(args.checkpoint_root, args.dataset, args.checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "reset_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/PyTorch/audio/torchaudio/extension/extension.py:14: UserWarning: torchaudio C++ extension is not available.\n",
      "  warnings.warn('torchaudio C++ extension is not available.')\n"
     ]
    }
   ],
   "source": [
    "train_transform, val_transform = load_preprocesser(args.dataset, \"dct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f546a1990964197aafbf072229e5802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/samova/lcances/Datasets/UrbanSound8K/ubs8k/datasets.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.y[\"idx\"] = list(range(len(self.y)))\n"
     ]
    }
   ],
   "source": [
    "manager, train_loader, val_loader = load_dataset(\n",
    "    args.dataset,\n",
    "    \"dct\",\n",
    "    dataset_root=args.dataset_root,\n",
    "    supervised_ratio=args.supervised_ratio,\n",
    "    batch_size=args.batch_size,\n",
    "    train_folds=args.train_folds,\n",
    "    val_folds=args.val_folds,\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 173)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = train_loader._iterables[0].dataset[0][0].shape\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model_func = load_model(args.dataset, args.model)\n",
    "\n",
    "commun_args = dict(\n",
    "    manager=manager,\n",
    "    num_classes=args.num_classes,\n",
    "    input_shape=list(input_shape),\n",
    ")\n",
    "\n",
    "m1, m2 = model_func(**commun_args), model_func(**commun_args)\n",
    "\n",
    "if args.nb_gpu > 1:\n",
    "    m1 = DataParallel(m1)\n",
    "    m2 = DataParallel(m2)\n",
    "\n",
    "m1 = m1.cuda()\n",
    "m2 = m2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 32, 64, 173]             288\n",
      "       BatchNorm2d-2          [-1, 32, 64, 173]              64\n",
      "              ReLU-3          [-1, 32, 64, 173]               0\n",
      "         MaxPool2d-4           [-1, 32, 32, 87]               0\n",
      "            Conv2d-5           [-1, 32, 32, 87]           9,216\n",
      "       BatchNorm2d-6           [-1, 32, 32, 87]              64\n",
      "              ReLU-7           [-1, 32, 32, 87]               0\n",
      "            Conv2d-8           [-1, 32, 32, 87]           9,216\n",
      "       BatchNorm2d-9           [-1, 32, 32, 87]              64\n",
      "             ReLU-10           [-1, 32, 32, 87]               0\n",
      "       BasicBlock-11           [-1, 32, 32, 87]               0\n",
      "           Conv2d-12           [-1, 32, 32, 87]           9,216\n",
      "      BatchNorm2d-13           [-1, 32, 32, 87]              64\n",
      "             ReLU-14           [-1, 32, 32, 87]               0\n",
      "           Conv2d-15           [-1, 32, 32, 87]           9,216\n",
      "      BatchNorm2d-16           [-1, 32, 32, 87]              64\n",
      "             ReLU-17           [-1, 32, 32, 87]               0\n",
      "       BasicBlock-18           [-1, 32, 32, 87]               0\n",
      "           Conv2d-19           [-1, 32, 32, 87]           9,216\n",
      "      BatchNorm2d-20           [-1, 32, 32, 87]              64\n",
      "             ReLU-21           [-1, 32, 32, 87]               0\n",
      "           Conv2d-22           [-1, 32, 32, 87]           9,216\n",
      "      BatchNorm2d-23           [-1, 32, 32, 87]              64\n",
      "             ReLU-24           [-1, 32, 32, 87]               0\n",
      "       BasicBlock-25           [-1, 32, 32, 87]               0\n",
      "           Conv2d-26           [-1, 32, 32, 87]           9,216\n",
      "      BatchNorm2d-27           [-1, 32, 32, 87]              64\n",
      "             ReLU-28           [-1, 32, 32, 87]               0\n",
      "           Conv2d-29           [-1, 32, 32, 87]           9,216\n",
      "      BatchNorm2d-30           [-1, 32, 32, 87]              64\n",
      "             ReLU-31           [-1, 32, 32, 87]               0\n",
      "       BasicBlock-32           [-1, 32, 32, 87]               0\n",
      "           Conv2d-33           [-1, 64, 16, 44]          18,432\n",
      "      BatchNorm2d-34           [-1, 64, 16, 44]             128\n",
      "             ReLU-35           [-1, 64, 16, 44]               0\n",
      "           Conv2d-36           [-1, 64, 16, 44]          36,864\n",
      "      BatchNorm2d-37           [-1, 64, 16, 44]             128\n",
      "           Conv2d-38           [-1, 64, 16, 44]           2,048\n",
      "      BatchNorm2d-39           [-1, 64, 16, 44]             128\n",
      "             ReLU-40           [-1, 64, 16, 44]               0\n",
      "       BasicBlock-41           [-1, 64, 16, 44]               0\n",
      "           Conv2d-42           [-1, 64, 16, 44]          36,864\n",
      "      BatchNorm2d-43           [-1, 64, 16, 44]             128\n",
      "             ReLU-44           [-1, 64, 16, 44]               0\n",
      "           Conv2d-45           [-1, 64, 16, 44]          36,864\n",
      "      BatchNorm2d-46           [-1, 64, 16, 44]             128\n",
      "             ReLU-47           [-1, 64, 16, 44]               0\n",
      "       BasicBlock-48           [-1, 64, 16, 44]               0\n",
      "           Conv2d-49           [-1, 64, 16, 44]          36,864\n",
      "      BatchNorm2d-50           [-1, 64, 16, 44]             128\n",
      "             ReLU-51           [-1, 64, 16, 44]               0\n",
      "           Conv2d-52           [-1, 64, 16, 44]          36,864\n",
      "      BatchNorm2d-53           [-1, 64, 16, 44]             128\n",
      "             ReLU-54           [-1, 64, 16, 44]               0\n",
      "       BasicBlock-55           [-1, 64, 16, 44]               0\n",
      "           Conv2d-56           [-1, 64, 16, 44]          36,864\n",
      "      BatchNorm2d-57           [-1, 64, 16, 44]             128\n",
      "             ReLU-58           [-1, 64, 16, 44]               0\n",
      "           Conv2d-59           [-1, 64, 16, 44]          36,864\n",
      "      BatchNorm2d-60           [-1, 64, 16, 44]             128\n",
      "             ReLU-61           [-1, 64, 16, 44]               0\n",
      "       BasicBlock-62           [-1, 64, 16, 44]               0\n",
      "           Conv2d-63           [-1, 128, 8, 22]          73,728\n",
      "      BatchNorm2d-64           [-1, 128, 8, 22]             256\n",
      "             ReLU-65           [-1, 128, 8, 22]               0\n",
      "           Conv2d-66           [-1, 128, 8, 22]         147,456\n",
      "      BatchNorm2d-67           [-1, 128, 8, 22]             256\n",
      "           Conv2d-68           [-1, 128, 8, 22]           8,192\n",
      "      BatchNorm2d-69           [-1, 128, 8, 22]             256\n",
      "             ReLU-70           [-1, 128, 8, 22]               0\n",
      "       BasicBlock-71           [-1, 128, 8, 22]               0\n",
      "           Conv2d-72           [-1, 128, 8, 22]         147,456\n",
      "      BatchNorm2d-73           [-1, 128, 8, 22]             256\n",
      "             ReLU-74           [-1, 128, 8, 22]               0\n",
      "           Conv2d-75           [-1, 128, 8, 22]         147,456\n",
      "      BatchNorm2d-76           [-1, 128, 8, 22]             256\n",
      "             ReLU-77           [-1, 128, 8, 22]               0\n",
      "       BasicBlock-78           [-1, 128, 8, 22]               0\n",
      "           Conv2d-79           [-1, 128, 8, 22]         147,456\n",
      "      BatchNorm2d-80           [-1, 128, 8, 22]             256\n",
      "             ReLU-81           [-1, 128, 8, 22]               0\n",
      "           Conv2d-82           [-1, 128, 8, 22]         147,456\n",
      "      BatchNorm2d-83           [-1, 128, 8, 22]             256\n",
      "             ReLU-84           [-1, 128, 8, 22]               0\n",
      "       BasicBlock-85           [-1, 128, 8, 22]               0\n",
      "           Conv2d-86           [-1, 128, 8, 22]         147,456\n",
      "      BatchNorm2d-87           [-1, 128, 8, 22]             256\n",
      "             ReLU-88           [-1, 128, 8, 22]               0\n",
      "           Conv2d-89           [-1, 128, 8, 22]         147,456\n",
      "      BatchNorm2d-90           [-1, 128, 8, 22]             256\n",
      "             ReLU-91           [-1, 128, 8, 22]               0\n",
      "       BasicBlock-92           [-1, 128, 8, 22]               0\n",
      "AdaptiveAvgPool2d-93            [-1, 128, 1, 1]               0\n",
      "           Linear-94                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 1,471,978\n",
      "Trainable params: 1,471,978\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 43.29\n",
      "Params size (MB): 5.62\n",
      "Estimated Total Size (MB): 48.95\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "s = summary(m1, tuple(input_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensorboard_root = (\n",
    "    f\"{args.model}/{args.supervised_ratio}/{get_datetime()}_{model_func.__name__}\"\n",
    ")\n",
    "checkpoint_root = f\"{args.model}/{args.supervised_ratio}/{model_func.__name__}\"\n",
    "\n",
    "# dct parameters\n",
    "sufix_title = f\"_{args.lambda_cot_max}-lcm\"\n",
    "sufix_title = f\"_{args.lambda_diff_max}-ldm\"\n",
    "sufix_title = f\"_{args.warmup_length}-wl\"\n",
    "sufix_title = f\"_{args.epsilon}-eps\"\n",
    "\n",
    "# mixup parameters\n",
    "if args.mixup:\n",
    "    sufix_title += \"_mixup\"\n",
    "    if args.mixup_max:\n",
    "        sufix_title += \"-max\"\n",
    "    if args.mixup_label:\n",
    "        sufix_title += \"-label\"\n",
    "    sufix_title += f\"-{args.mixup_alpha}-a\"\n",
    "\n",
    "# normale training parameters\n",
    "sufix_title += f\"_{args.learning_rate}-lr\"\n",
    "sufix_title += f\"_{args.supervised_ratio}-sr\"\n",
    "sufix_title += f\"_{args.nb_epoch}-e\"\n",
    "sufix_title += f\"_{args.batch_size}-bs\"\n",
    "sufix_title += f\"_{args.seed}-seed\"\n",
    "\n",
    "tensorboard_title = tensorboard_root + sufix_title\n",
    "checkpoint_title = checkpoint_root + sufix_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../tensorboard/ubs8k/deep-co-training_mixup/wideresnet28_2/0.1/2021-01-18_20:22:59_wideresnet28_2_0.02-eps_0.0005-lr_0.1-sr_300-e_100-bs_1234-seed\n"
     ]
    }
   ],
   "source": [
    "tensorboard = mSummaryWriter(\n",
    "    log_dir=\"%s/%s\" % (tensorboard_path, tensorboard_title), comment=model_func.__name__\n",
    ")\n",
    "print(os.path.join(tensorboard_path, tensorboard_title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer & callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optim_args = dict(\n",
    "    learning_rate=args.learning_rate,\n",
    ")\n",
    "\n",
    "optimizer = load_optimizer(args.dataset, \"dct\", model1=m1, model2=m2, **optim_args)\n",
    "callbacks = load_callbacks(\n",
    "    args.dataset, \"dct\", optimizer=optimizer, nb_epoch=args.nb_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Adversarial generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "# adversarial generation\n",
    "adv_generator_1 = GradientSignAttack(\n",
    "    m1,\n",
    "    loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"),\n",
    "    eps=args.epsilon,\n",
    "    clip_min=-np.inf,\n",
    "    clip_max=np.inf,\n",
    "    targeted=False,\n",
    ")\n",
    "\n",
    "adv_generator_2 = GradientSignAttack(\n",
    "    m2,\n",
    "    loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"),\n",
    "    eps=args.epsilon,\n",
    "    clip_min=-np.inf,\n",
    "    clip_max=np.inf,\n",
    "    targeted=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint initialise at:  /users/samova/lcances/semi-supervised/model_save/ubs8k/deep-co-training_mixup/wideresnet28_2/0.1/wideresnet28_2_0.02-eps_0.0005-lr_0.1-sr_300-e_100-bs_1234-seed_m1.torch\n",
      "name:  wideresnet28_2_0.02-eps_0.0005-lr_0.1-sr_300-e_100-bs_1234-seed_m1.torch\n",
      "mode:  max\n"
     ]
    }
   ],
   "source": [
    "# Losses\n",
    "# see losses.py\n",
    "\n",
    "# define the warmups & add them to the callbacks (for update)\n",
    "lambda_cot = Warmup(args.lambda_cot_max, args.warmup_length, sigmoid_rampup)\n",
    "lambda_diff = Warmup(args.lambda_diff_max, args.warmup_length, sigmoid_rampup)\n",
    "callbacks += [lambda_cot, lambda_diff]\n",
    "\n",
    "# checkpoints\n",
    "checkpoint = CheckPoint(\n",
    "    [m1, m2],\n",
    "    optimizer,\n",
    "    mode=\"max\",\n",
    "    name=\"%s/%s_m1.torch\" % (checkpoint_path, checkpoint_title),\n",
    ")\n",
    "\n",
    "# metrics\n",
    "metrics_fn = dict(\n",
    "    ratio_s=[Ratio(), Ratio()],\n",
    "    ratio_u=[Ratio(), Ratio()],\n",
    "    acc_s=[CategoricalAccuracy(), CategoricalAccuracy()],\n",
    "    acc_u=[CategoricalAccuracy(), CategoricalAccuracy()],\n",
    "    f1_s=[FScore(), FScore()],\n",
    "    f1_u=[FScore(), FScore()],\n",
    "    avg_total=ContinueAverage(),\n",
    "    avg_sup=ContinueAverage(),\n",
    "    avg_cot=ContinueAverage(),\n",
    "    avg_diff=ContinueAverage(),\n",
    ")\n",
    "maximum_tracker = track_maximum()\n",
    "\n",
    "\n",
    "def reset_metrics():\n",
    "    for item in metrics_fn.values():\n",
    "        if isinstance(item, list):\n",
    "            for f in item:\n",
    "                f.reset()\n",
    "        else:\n",
    "            item.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "reset_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Metrics and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  Lsup   | Lcot   | Ldiff  | total  - metrics:  acc_s1    | acc_u1   - Time  \n"
     ]
    }
   ],
   "source": [
    "UNDERLINE_SEQ = \"\\033[1;4m\"\n",
    "RESET_SEQ = \"\\033[0m\"\n",
    "\n",
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<8.8} {:<6.6} | {:<6.6} | {:<6.6} | {:<6.6} - {:<9.9} {:<9.9} | {:<9.9}- {:<6.6}\"\n",
    "value_form = \"{:<8.8} {:<6} - {:<6} - {:<8.8} {:<6.4f} | {:<6.4f} | {:<6.4f} | {:<6.4f} - {:<9.9} {:<9.4f} | {:<9.4f}- {:<6.4f}\"\n",
    "\n",
    "header = header_form.format(\n",
    "    \"\",\n",
    "    \"Epoch\",\n",
    "    \"%\",\n",
    "    \"Losses:\",\n",
    "    \"Lsup\",\n",
    "    \"Lcot\",\n",
    "    \"Ldiff\",\n",
    "    \"total\",\n",
    "    \"metrics: \",\n",
    "    \"acc_s1\",\n",
    "    \"acc_u1\",\n",
    "    \"Time\",\n",
    ")\n",
    "\n",
    "train_form = value_form\n",
    "val_form = UNDERLINE_SEQ + value_form + RESET_SEQ\n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup_fn = MixUpBatchShuffle(\n",
    "    alpha=args.mixup_alpha, apply_max=args.mixup_max, mix_labels=args.mixup_label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "\n",
    "    reset_metrics()\n",
    "    m1.train()\n",
    "    m2.train()\n",
    "\n",
    "    for batch, (S1, S2, U) in enumerate(train_loader):\n",
    "        x_s1, y_s1 = S1\n",
    "        x_s2, y_s2 = S2\n",
    "        x_u, y_u = U\n",
    "\n",
    "        # Apply mixup if needed, otherwise no mixup.\n",
    "        if args.mixup:\n",
    "            x_s, y_s = mixup_fn(x_s, y_s)\n",
    "            x_u, y_u = mixup_fn(x_u, y_u)\n",
    "\n",
    "        x_s1, x_s2, x_u = x_s1.cuda(), x_s2.cuda(), x_u.cuda()\n",
    "        y_s1, y_s2, y_u = y_s1.cuda(), y_s2.cuda(), y_u.cuda()\n",
    "\n",
    "        with autocast():\n",
    "            logits_s1 = m1(x_s1)\n",
    "            logits_s2 = m2(x_s2)\n",
    "            logits_u1 = m1(x_u)\n",
    "            logits_u2 = m2(x_u)\n",
    "\n",
    "        # pseudo labels of U\n",
    "        pred_u1 = torch.argmax(logits_u1, 1)\n",
    "        pred_u2 = torch.argmax(logits_u2, 1)\n",
    "\n",
    "        # ======== Generate adversarial examples ========\n",
    "        # fix batchnorm ----\n",
    "        m1.eval()\n",
    "        m2.eval()\n",
    "\n",
    "        # generate adversarial examples ----\n",
    "        adv_data_s1 = adv_generator_1.perturb(x_s1, y_s1)\n",
    "        adv_data_u1 = adv_generator_1.perturb(x_u, pred_u1)\n",
    "\n",
    "        adv_data_s2 = adv_generator_2.perturb(x_s2, y_s2)\n",
    "        adv_data_u2 = adv_generator_2.perturb(x_u, pred_u2)\n",
    "\n",
    "        m1.train()\n",
    "        m2.train()\n",
    "\n",
    "        # predict adversarial examples ----\n",
    "        with autocast():\n",
    "            adv_logits_s1 = m1(adv_data_s2)\n",
    "            adv_logits_s2 = m2(adv_data_s1)\n",
    "\n",
    "            adv_logits_u1 = m1(adv_data_u2)\n",
    "            adv_logits_u2 = m2(adv_data_u1)\n",
    "\n",
    "        # ======== calculate the differents loss ========\n",
    "        # zero the parameter gradients ----\n",
    "        for p in m1.parameters():\n",
    "            p.grad = None  # zero grad\n",
    "        for p in m2.parameters():\n",
    "            p.grad = None\n",
    "\n",
    "        # losses ----\n",
    "        with autocast():\n",
    "            l_sup = loss_sup(logits_s1, logits_s2, y_s1, y_s2)\n",
    "\n",
    "            l_cot = loss_cot(logits_u1, logits_u2)\n",
    "\n",
    "            l_diff = loss_diff(\n",
    "                logits_s1,\n",
    "                logits_s2,\n",
    "                adv_logits_s1,\n",
    "                adv_logits_s2,\n",
    "                logits_u1,\n",
    "                logits_u2,\n",
    "                adv_logits_u1,\n",
    "                adv_logits_u2,\n",
    "            )\n",
    "\n",
    "            total_loss = l_sup + lambda_cot() * l_cot + lambda_diff() * l_diff\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ======== Calc the metrics ========\n",
    "        with torch.set_grad_enabled(False):\n",
    "            # accuracies ----\n",
    "            pred_s1 = torch.argmax(logits_s1, dim=1)\n",
    "            pred_s2 = torch.argmax(logits_s2, dim=1)\n",
    "\n",
    "            acc_s1 = metrics_fn[\"acc_s\"][0](pred_s1, y_s1)\n",
    "            acc_s2 = metrics_fn[\"acc_s\"][1](pred_s2, y_s2)\n",
    "            acc_u1 = metrics_fn[\"acc_u\"][0](pred_u1, y_u)\n",
    "            acc_u2 = metrics_fn[\"acc_u\"][1](pred_u2, y_u)\n",
    "\n",
    "            # ratios  ----\n",
    "            adv_pred_s1 = torch.argmax(adv_logits_s1, 1)\n",
    "            adv_pred_s2 = torch.argmax(adv_logits_s2, 1)\n",
    "            adv_pred_u1 = torch.argmax(adv_logits_u1, 1)\n",
    "            adv_pred_u2 = torch.argmax(adv_logits_u2, 1)\n",
    "\n",
    "            ratio_s1 = metrics_fn[\"ratio_s\"][0](adv_pred_s1, y_s1)\n",
    "            ratio_s2 = metrics_fn[\"ratio_s\"][0](adv_pred_s2, y_s2)\n",
    "            ratio_u1 = metrics_fn[\"ratio_s\"][0](adv_pred_u1, y_u)\n",
    "            ratio_u2 = metrics_fn[\"ratio_s\"][0](adv_pred_u2, y_u)\n",
    "            # ========\n",
    "\n",
    "            avg_total = metrics_fn[\"avg_total\"](total_loss.item())\n",
    "            avg_sup = metrics_fn[\"avg_sup\"](l_sup.item())\n",
    "            avg_diff = metrics_fn[\"avg_diff\"](l_diff.item())\n",
    "            avg_cot = metrics_fn[\"avg_cot\"](l_cot.item())\n",
    "\n",
    "            # logs\n",
    "            print(\n",
    "                train_form.format(\n",
    "                    \"Training: \",\n",
    "                    epoch + 1,\n",
    "                    int(100 * (batch + 1) / len(train_loader)),\n",
    "                    \"\",\n",
    "                    avg_sup.mean,\n",
    "                    avg_cot.mean,\n",
    "                    avg_diff.mean,\n",
    "                    avg_total.mean,\n",
    "                    \"\",\n",
    "                    acc_s1.mean,\n",
    "                    acc_u1.mean,\n",
    "                    time.time() - start_time,\n",
    "                ),\n",
    "                end=\"\\r\",\n",
    "            )\n",
    "\n",
    "    # using tensorboard to monitor loss and acc\\n\",\n",
    "    tensorboard.add_scalar(\"train/total_loss\", avg_total.mean, epoch)\n",
    "    tensorboard.add_scalar(\"train/Lsup\", avg_sup.mean, epoch)\n",
    "    tensorboard.add_scalar(\"train/Lcot\", avg_cot.mean, epoch)\n",
    "    tensorboard.add_scalar(\"train/Ldiff\", avg_diff.mean, epoch)\n",
    "    tensorboard.add_scalar(\"train/acc_1\", acc_s1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"train/acc_2\", acc_s2.mean, epoch)\n",
    "\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_s1\", acc_s1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_s2\", acc_s2.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_u1\", acc_u1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_u2\", acc_u2.mean, epoch)\n",
    "\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio_s1\", ratio_s1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio_s2\", ratio_s2.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio_u1\", ratio_u1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio_u2\", ratio_u2.mean, epoch)\n",
    "\n",
    "    # Return the total loss to check for NaN\n",
    "    return total_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def test(epoch, msg=\"\"):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "\n",
    "    reset_metrics()\n",
    "    m1.eval()\n",
    "    m2.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for batch, (X, y) in enumerate(val_loader):\n",
    "            x = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            with autocast():\n",
    "                logits_1 = m1(x)\n",
    "                logits_2 = m2(x)\n",
    "\n",
    "                # losses ----\n",
    "                l_sup = loss_sup(logits_1, logits_2, y, y)\n",
    "\n",
    "            # ======== Calc the metrics ========\n",
    "            # accuracies ----\n",
    "            pred_1 = torch.argmax(logits_1, dim=1)\n",
    "            pred_2 = torch.argmax(logits_2, dim=1)\n",
    "\n",
    "            acc_1 = metrics_fn[\"acc_s\"][0](pred_1, y)\n",
    "            acc_2 = metrics_fn[\"acc_s\"][1](pred_2, y)\n",
    "\n",
    "            avg_sup = metrics_fn[\"avg_sup\"](l_sup.item())\n",
    "\n",
    "            # logs\n",
    "            print(\n",
    "                val_form.format(\n",
    "                    \"Validation: \",\n",
    "                    epoch + 1,\n",
    "                    int(100 * (batch + 1) / len(train_loader)),\n",
    "                    \"\",\n",
    "                    avg_sup.mean,\n",
    "                    0.0,\n",
    "                    0.0,\n",
    "                    avg_sup.mean,\n",
    "                    \"\",\n",
    "                    acc_1.mean,\n",
    "                    0.0,\n",
    "                    time.time() - start_time,\n",
    "                ),\n",
    "                end=\"\\r\",\n",
    "            )\n",
    "\n",
    "    tensorboard.add_scalar(\"val/acc_1\", acc_1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"val/acc_2\", acc_2.mean, epoch)\n",
    "\n",
    "    tensorboard.add_scalar(\"max/acc_1\", maximum_tracker(\"acc_1\", acc_1.mean), epoch)\n",
    "    tensorboard.add_scalar(\"max/acc_2\", maximum_tracker(\"acc_2\", acc_2.mean), epoch)\n",
    "\n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/lambda_cot\", lambda_cot(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/lambda_diff\", lambda_diff(), epoch)\n",
    "    tensorboard.add_scalar(\n",
    "        \"detail_hyperparameters/learning_rate\", get_lr(optimizer), epoch\n",
    "    )\n",
    "\n",
    "    # Apply callbacks\n",
    "    for c in callbacks:\n",
    "        c.step()\n",
    "\n",
    "    # call checkpoint\n",
    "    checkpoint.step(acc_1.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  Lsup   | Lcot   | Ldiff  | total  - metrics:  acc_s1    | acc_u1   - Time  \n",
      "\n",
      "Training 1      - 100    -          3.7813 | 0.0992 | 4.3770 | 3.8114 -           0.3175    | 0.3330   - 109.9176\n",
      "\u001b[1;4mValidati 1      - 11     -          3.4327 | 0.0000 | 0.0000 | 3.4327 -           0.4345    | 0.0000   - 7.2972\u001b[0m\n",
      "Training 2      - 100    -          3.1095 | 0.0908 | 3.7994 | 3.1238 -           0.4025    | 0.4565   - 45.9481\n",
      "\u001b[1;4mValidati 2      - 11     -          3.3035 | 0.0000 | 0.0000 | 3.3035 -           0.4741    | 0.0000   - 0.4079\u001b[0m\n",
      "Training 3      - 100    -          2.6920 | 0.0957 | 3.5921 | 2.7064 -           0.5213    | 0.5462   - 46.7079\n",
      "\u001b[1;4mValidati 3      - 11     -          2.4631 | 0.0000 | 0.0000 | 2.4631 -           0.5297    | 0.0000   - 0.4186\u001b[0m\n",
      "Training 4      - 100    -          2.4142 | 0.0925 | 3.3323 | 2.4284 -           0.5713    | 0.6010   - 46.6390\n",
      "\u001b[1;4mValidati 4      - 11     -          2.5173 | 0.0000 | 0.0000 | 2.5173 -           0.5957    | 0.0000   - 0.4471\u001b[0m\n",
      "Training 5      - 100    -          2.0923 | 0.0862 | 3.0140 | 2.1061 -           0.6212    | 0.6498   - 47.1855\n",
      "\u001b[1;4mValidati 5      - 11     -          2.1449 | 0.0000 | 0.0000 | 2.1449 -           0.6543    | 0.0000   - 0.4255\u001b[0m\n",
      "Training 6      - 100    -          1.7729 | 0.0938 | 2.9068 | 1.7871 -           0.7000    | 0.6823   - 46.7910\n",
      "\u001b[1;4mValidati 6      - 11     -          2.2445 | 0.0000 | 0.0000 | 2.2445 -           0.6381    | 0.0000   - 0.4356\u001b[0m\n",
      "Training 7      - 100    -          1.6707 | 0.1061 | 2.9133 | 1.6859 -           0.7438    | 0.7069   - 47.1291\n",
      "\u001b[1;4mValidati 7      - 11     -          2.2553 | 0.0000 | 0.0000 | 2.2553 -           0.6474    | 0.0000   - 0.4224\u001b[0m\n",
      "Training 8      - 100    -          1.4948 | 0.1007 | 2.7212 | 1.5099 -           0.7675    | 0.7143   - 47.4147\n",
      "\u001b[1;4mValidati 8      - 11     -          2.2072 | 0.0000 | 0.0000 | 2.2072 -           0.6695    | 0.0000   - 0.4259\u001b[0m\n",
      "Training 9      - 100    -          1.4572 | 0.0995 | 2.6423 | 1.4728 -           0.7488    | 0.7270   - 47.2756\n",
      "\u001b[1;4mValidati 9      - 11     -          2.2600 | 0.0000 | 0.0000 | 2.2600 -           0.6878    | 0.0000   - 0.4218\u001b[0m\n",
      "Training 10     - 100    -          1.2450 | 0.0999 | 2.5770 | 1.2611 -           0.8212    | 0.7587   - 47.0476\n",
      "\u001b[1;4mValidati 10     - 11     -          2.2055 | 0.0000 | 0.0000 | 2.2055 -           0.6447    | 0.0000   - 0.4266\u001b[0m\n",
      "Training 11     - 100    -          1.1180 | 0.1041 | 2.5038 | 1.1347 -           0.7962    | 0.7490   - 47.9982\n",
      "\u001b[1;4mValidati 11     - 11     -          2.0476 | 0.0000 | 0.0000 | 2.0476 -           0.6913    | 0.0000   - 0.4273\u001b[0m\n",
      "Training 12     - 100    -          0.9672 | 0.0954 | 2.3323 | 0.9837 -           0.8588    | 0.7645   - 46.7263\n",
      "\u001b[1;4mValidati 12     - 11     -          2.1979 | 0.0000 | 0.0000 | 2.1979 -           0.7300    | 0.0000   - 0.4638\u001b[0m\n",
      "Training 13     - 100    -          0.8666 | 0.0998 | 2.3310 | 0.8841 -           0.8863    | 0.7891   - 47.3026\n",
      "\u001b[1;4mValidati 13     - 11     -          2.0167 | 0.0000 | 0.0000 | 2.0167 -           0.6526    | 0.0000   - 0.4263\u001b[0m\n",
      "Training 14     - 100    -          0.9233 | 0.1035 | 2.3790 | 0.9423 -           0.8713    | 0.7822   - 46.6627\n",
      "\u001b[1;4mValidati 14     - 11     -          2.3273 | 0.0000 | 0.0000 | 2.3273 -           0.6477    | 0.0000   - 0.4230\u001b[0m\n",
      "Training 15     - 100    -          0.9525 | 0.1145 | 2.5617 | 0.9742 -           0.8100    | 0.7432   - 47.3505\n",
      "\u001b[1;4mValidati 15     - 11     -          2.3214 | 0.0000 | 0.0000 | 2.3214 -           0.5546    | 0.0000   - 0.4227\u001b[0m\n",
      "Training 16     - 100    -          0.7859 | 0.1058 | 2.3634 | 0.8071 -           0.8538    | 0.7692   - 46.8184\n",
      "\u001b[1;4mValidati 16     - 11     -          2.2125 | 0.0000 | 0.0000 | 2.2125 -           0.6832    | 0.0000   - 0.4223\u001b[0m\n",
      "Training 17     - 100    -          0.6270 | 0.0866 | 1.9923 | 0.6459 -           0.9013    | 0.8070   - 47.3457\n",
      "\u001b[1;4mValidati 17     - 11     -          2.0392 | 0.0000 | 0.0000 | 2.0392 -           0.7174    | 0.0000   - 0.4236\u001b[0m\n",
      "Training 18     - 100    -          0.5135 | 0.0893 | 1.9781 | 0.5334 -           0.9225    | 0.8141   - 47.1701\n",
      "\u001b[1;4mValidati 18     - 11     -          2.1524 | 0.0000 | 0.0000 | 2.1524 -           0.6838    | 0.0000   - 0.4212\u001b[0m\n",
      "Training 19     - 100    -          0.5016 | 0.0934 | 2.0311 | 0.5232 -           0.9275    | 0.8161   - 46.8231\n",
      "\u001b[1;4mValidati 19     - 11     -          2.3866 | 0.0000 | 0.0000 | 2.3866 -           0.6164    | 0.0000   - 0.4231\u001b[0m\n",
      "Training 20     - 100    -          0.3816 | 0.0955 | 2.0070 | 0.4042 -           0.9388    | 0.8112   - 46.6309\n",
      "\u001b[1;4mValidati 20     - 11     -          2.4479 | 0.0000 | 0.0000 | 2.4479 -           0.6631    | 0.0000   - 0.4236\u001b[0m\n",
      "Training 21     - 100    -          0.4316 | 0.0896 | 1.9114 | 0.4543 -           0.9225    | 0.8168   - 47.3975\n",
      "\u001b[1;4mValidati 21     - 11     -          2.5989 | 0.0000 | 0.0000 | 2.5989 -           0.6668    | 0.0000   - 0.4242\u001b[0m\n",
      "Training 22     - 100    -          0.4925 | 0.0977 | 2.0703 | 0.5185 -           0.9300    | 0.8208   - 46.8027\n",
      "\u001b[1;4mValidati 22     - 11     -          2.5122 | 0.0000 | 0.0000 | 2.5122 -           0.7066    | 0.0000   - 0.4302\u001b[0m\n",
      "Training 23     - 100    -          0.3926 | 0.0945 | 1.9998 | 0.4192 -           0.9550    | 0.8215   - 47.9333\n",
      "\u001b[1;4mValidati 23     - 11     -          2.3140 | 0.0000 | 0.0000 | 2.3140 -           0.6945    | 0.0000   - 0.4278\u001b[0m\n",
      "Training 24     - 100    -          0.3584 | 0.0874 | 1.8241 | 0.3840 -           0.9525    | 0.8128   - 46.6554\n",
      "\u001b[1;4mValidati 24     - 11     -          2.3145 | 0.0000 | 0.0000 | 2.3145 -           0.6797    | 0.0000   - 0.4244\u001b[0m\n",
      "Training 25     - 100    -          0.3163 | 0.0877 | 1.8237 | 0.3433 -           0.9425    | 0.8148   - 46.5636\n",
      "\u001b[1;4mValidati 25     - 11     -          2.4396 | 0.0000 | 0.0000 | 2.4396 -           0.6641    | 0.0000   - 0.4247\u001b[0m\n",
      "Training 26     - 100    -          0.3005 | 0.0878 | 1.8099 | 0.3287 -           0.9638    | 0.8244   - 47.0600\n",
      "\u001b[1;4mValidati 26     - 11     -          2.7527 | 0.0000 | 0.0000 | 2.7527 -           0.5872    | 0.0000   - 0.4286\u001b[0m\n",
      "Training 27     - 100    -          0.3046 | 0.0924 | 1.9255 | 0.3362 -           0.9625    | 0.8075   - 46.9500\n",
      "\u001b[1;4mValidati 27     - 11     -          2.4685 | 0.0000 | 0.0000 | 2.4685 -           0.6729    | 0.0000   - 0.4237\u001b[0m\n",
      "Training 28     - 100    -          0.3082 | 0.0893 | 1.8101 | 0.3396 -           0.9750    | 0.8205   - 48.3151\n",
      "\u001b[1;4mValidati 28     - 11     -          2.5498 | 0.0000 | 0.0000 | 2.5498 -           0.6642    | 0.0000   - 0.4265\u001b[0m\n",
      "Training 29     - 100    -          0.2662 | 0.0913 | 1.8530 | 0.3001 -           0.9588    | 0.8194   - 47.8838\n",
      "\u001b[1;4mValidati 29     - 11     -          2.5146 | 0.0000 | 0.0000 | 2.5146 -           0.6533    | 0.0000   - 0.4291\u001b[0m\n",
      "Training 30     - 100    -          0.1980 | 0.0864 | 1.7519 | 0.2317 -           0.9625    | 0.8163   - 48.1680\n",
      "\u001b[1;4mValidati 30     - 11     -          2.1435 | 0.0000 | 0.0000 | 2.1435 -           0.6832    | 0.0000   - 0.4292\u001b[0m\n",
      "Training 31     - 100    -          0.2963 | 0.0813 | 1.6372 | 0.3295 -           0.9388    | 0.8098   - 46.9395\n",
      "\u001b[1;4mValidati 31     - 11     -          2.6731 | 0.0000 | 0.0000 | 2.6731 -           0.6625    | 0.0000   - 0.4253\u001b[0m\n",
      "Training 32     - 100    -          0.2942 | 0.1021 | 2.0685 | 0.3382 -           0.9275    | 0.7766   - 48.1562\n",
      "\u001b[1;4mValidati 32     - 11     -          2.5101 | 0.0000 | 0.0000 | 2.5101 -           0.7006    | 0.0000   - 0.4302\u001b[0m\n",
      "Training 33     - 97     -          0.2144 | 0.0847 | 1.7260 | 0.2531 -           0.9679    | 0.8246   - 46.8433\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-3e6fecc56342>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-45787887308c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml_sup\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambda_cot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ml_cot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambda_diff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ml_diff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/pytorch-dev/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/pytorch-dev/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if args.resume:\n",
    "    checkpoint.load_last()\n",
    "start_epoch = checkpoint.epoch_counter\n",
    "\n",
    "print(header)\n",
    "for epoch in range(0, args.nb_epoch):\n",
    "    total_loss = train(epoch)\n",
    "\n",
    "    if np.isnan(total_loss):\n",
    "        print(\"Losses are NaN, stoping the training here\")\n",
    "        break\n",
    "\n",
    "    test(epoch)\n",
    "\n",
    "    tensorboard.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {}\n",
    "for key, value in args.__dict__.items():\n",
    "    hparams[key] = str(value)\n",
    "\n",
    "final_metrics = {\n",
    "    \"max_acc_1\": maximum_tracker.max[\"acc_1\"],\n",
    "    \"max_acc_2\": maximum_tracker.max[\"acc_2\"],\n",
    "}\n",
    "\n",
    "tensorboard.add_hparams(hparams, final_metrics)\n",
    "\n",
    "tensorboard.flush()\n",
    "tensorboard.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-dev",
   "language": "python",
   "name": "pytorch-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
